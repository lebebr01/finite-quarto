[
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "R Packages",
    "section": "",
    "text": "simglm:  This package attempts to create a general syntax structure to simulate regression models, both single and multilevel models. The package also facilitates the power by simulation paradigm, including the ability to cross factors similar to a Monte Carlo simulation design.\npdfsearch   Adds the ability to search for keywords from pdf files rendered to text by the pdftools R package.\nmars  This package estimates meta-analytic models to help with research syntheses. The estimation is written directly via the log-likelihood with further extensions planend soon.\nhighlightHTML   This package post-processes an HTML file to add text/cell formatting to portions of the HTML using CSS.\nmetaRmat  Implements methods for the synthesis of correlation matrices and multivariate meta-analysis functions.\nstatthink  A companion package to the open-source text book Statistical Reasoning through Computation and R\nSPSStoR:  This package converts SPSS syntax to R syntax. See the GitHub repository for updated SPSS routines that the package supports."
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Linking with the Bayesian Item Response Theory Model\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nBrandon LeBeau & Xiaoting Zhong\n\n\n\n\n\n\n\n\n\n\n\n\nUsing IDAS in an Intro Stats Course\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Meta-Analytic Data\n\n\n\n\n\n\n\n\nOct 29, 2021\n\n\nBrandon LeBeau & Ariel M. Aloe\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of Statistical Software and Quantitative Methods\n\n\n\n\n\n\n\n\nApr 24, 2020\n\n\nBrandon LeBeau & Ariel M. Aloe\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Psychological Constructs over a Developmental Span: Methodological Approaches\n\n\n\n\n\n\n\n\nFeb 14, 2020\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiating Among Gifted Learners: A Comparison of Classical Test Theory and Item Response Theory on Above-Level Testing\n\n\n\n\n\n\n\n\nNov 8, 2019\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of Statistical Software and Quantitative Methods\n\n\n\n\n\n\n\n\nJul 31, 2018\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation and Power Analysis of Generalized Linear Mixed Models\n\n\n\n\n\n\n\n\nAug 2, 2017\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nMake Power Fun (Again?)\n\n\n\n\n\n\n\n\nFeb 24, 2017\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nExtending accessibility of open-source statistical software to the masses A shiny case study\n\n\n\n\n\n\n\n\nOct 7, 2016\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating NCAA Football Coaches’ Abilities An Application of Item Response Theory\n\n\n\n\n\n\n\n\nJul 31, 2016\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation and power analysis of generalized linear mixed models\n\n\n\n\n\n\n\n\nJun 29, 2016\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nAssessing the validity of item response theory models when calibrating field test items\n\n\n\n\n\n\n\n\nApr 9, 2016\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nInformative vs uninformative prior distributions with characteristic curve linking methods\n\n\n\n\n\n\n\n\nApr 8, 2016\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nInteractively building test forms from an IRT perspective An application of R and Shiny\n\n\n\n\n\n\n\n\nFeb 18, 2016\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping to Item Response Theory - A College Football Adventure\n\n\n\n\n\n\n\n\nDec 4, 2015\n\n\nBrandon LeBeau\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-educate-r-posts",
    "href": "index.html#recent-educate-r-posts",
    "title": "Brandon LeBeau",
    "section": "Recent Educate-R Posts",
    "text": "Recent Educate-R Posts\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nOct 7, 2025\n\n\nMy First Tidy Tuesday!\n\n\n\n\n\n\nOct 6, 2025\n\n\nsimglm v0.9.23: Propensity Score Modeling\n\n\n\n\n\n\nSep 24, 2025\n\n\nsimglm v0.9.22: Propensity Score Simulation\n\n\n\n\n\n\nNo matching items\n\nAll Educate-R Posts"
  },
  {
    "objectID": "index.html#recent-publications",
    "href": "index.html#recent-publications",
    "title": "Brandon LeBeau",
    "section": "Recent Publications",
    "text": "Recent Publications\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\n\n\n\n\n\nOct 7, 2021\n\n\nStatistical Reasoning through Computation and R\n\n\n\n\n\n\nApr 25, 2021\n\n\nReproducible Analyses in Educational Research\n\n\n\n\n\n\nJun 1, 2020\n\n\nThe Advanced Placement Program in Rural Schools: Equalizing Opportunity\n\n\n\n\n\n\nNo matching items\n\nAll Publications"
  },
  {
    "objectID": "publications/ses.html",
    "href": "publications/ses.html",
    "title": "Student eligibility for a free lunch as an SES measure in education research",
    "section": "",
    "text": "The use of eligibility for a free lunch as a measure of a student’s socioeconomic status continues to be a fixture of quantitative education research. Despite its popularity, it is unclear that education researchers are familiar with what student eligibility for a free lunch does (and does not) represent. The authors examine the National School Lunch Program, which is responsible for certifying students as eligible for a free lunch, and conclude that free lunch eligibility is a poor measure of socioeconomic status, which suffers from important deficiencies that can bias inferences. A table characterizing key strengths and weaknesses of variables used as measures of socioeconomic status is provided to facilitate comparisons."
  },
  {
    "objectID": "publications/ses.html#abstract",
    "href": "publications/ses.html#abstract",
    "title": "Student eligibility for a free lunch as an SES measure in education research",
    "section": "",
    "text": "The use of eligibility for a free lunch as a measure of a student’s socioeconomic status continues to be a fixture of quantitative education research. Despite its popularity, it is unclear that education researchers are familiar with what student eligibility for a free lunch does (and does not) represent. The authors examine the National School Lunch Program, which is responsible for certifying students as eligible for a free lunch, and conclude that free lunch eligibility is a poor measure of socioeconomic status, which suffers from important deficiencies that can bias inferences. A table characterizing key strengths and weaknesses of variables used as measures of socioeconomic status is provided to facilitate comparisons."
  },
  {
    "objectID": "publications/ses.html#citation",
    "href": "publications/ses.html#citation",
    "title": "Student eligibility for a free lunch as an SES measure in education research",
    "section": "Citation",
    "text": "Citation\nHarwell, Michael R, LeBeau, Brandon (2010). Student eligibility for a free lunch as an SES measure in education research. **Educational Researcher, 39(2), 120 - 131."
  },
  {
    "objectID": "publications/ses.html#links",
    "href": "publications/ses.html#links",
    "title": "Student eligibility for a free lunch as an SES measure in education research",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Educational Researcher, 39(2), 120 - 131 Authors: Michael R Harwell, Brandon LeBeau Date: March 01, 2010"
  },
  {
    "objectID": "publications/meta-campbell.html",
    "href": "publications/meta-campbell.html",
    "title": "Research Synthesis and Meta-Analysis of Monte Carlo Studies: The Best of Two Worlds",
    "section": "",
    "text": "Research syntheses and meta-analyses have been widely used to combine primary applied research studies. To date, there has been little work applying these methods to combine Monte Carlo studies, however narrative reviews are common to evaluate the Monte Carlo literature. This manuscript briefly describes the benefits and challenges of combining outcomes from Monte Carlo studies using research synthesis and meta-analysis methods. The particular focus is to discuss and extend ideas first described by Harwell (1992)."
  },
  {
    "objectID": "publications/meta-campbell.html#abstract",
    "href": "publications/meta-campbell.html#abstract",
    "title": "Research Synthesis and Meta-Analysis of Monte Carlo Studies: The Best of Two Worlds",
    "section": "",
    "text": "Research syntheses and meta-analyses have been widely used to combine primary applied research studies. To date, there has been little work applying these methods to combine Monte Carlo studies, however narrative reviews are common to evaluate the Monte Carlo literature. This manuscript briefly describes the benefits and challenges of combining outcomes from Monte Carlo studies using research synthesis and meta-analysis methods. The particular focus is to discuss and extend ideas first described by Harwell (1992)."
  },
  {
    "objectID": "publications/meta-campbell.html#citation",
    "href": "publications/meta-campbell.html#citation",
    "title": "Research Synthesis and Meta-Analysis of Monte Carlo Studies: The Best of Two Worlds",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2017). Research Synthesis and Meta-Analysis of Monte Carlo Studies: The Best of Two Worlds. The Campbell Collaboration."
  },
  {
    "objectID": "publications/meta-campbell.html#links",
    "href": "publications/meta-campbell.html#links",
    "title": "Research Synthesis and Meta-Analysis of Monte Carlo Studies: The Best of Two Worlds",
    "section": "Links",
    "text": "Links\nPDF\n\nPublication: The Campbell Collaboration Authors: Brandon LeBeau Date: October 09, 2017"
  },
  {
    "objectID": "publications/simglm-power.html",
    "href": "publications/simglm-power.html",
    "title": "Power Analysis by Simulation using R and simglm",
    "section": "",
    "text": "Power is a task that is commonly done prior to collecting data for a primary study. In most cases closed-form solutions are used to estimate power which may statistical assumptions to be able to perform the computations, for example assume residuals are normally distributed. In real-world data, these statistical assumptions may not hold, therefore estimates of power when these assumptions are assumed will likely be inflated. Power by simulation is another way to compute power estimates and offers significant flexibility to the user to explore the impact of various statistical assumption violations may have on power. This tutorial uses the simglm R package to perform the power by simulation. The simglm package provides a framework to simulate data from generalized linear mixed models which includes a wide variety of models. In addition, functions to perform replications and to compute power estimate summaries are available for users to take advantage of. Two worked examples are shown, one for a two-sample t-test and another within a repeated measures or longitudinal framework."
  },
  {
    "objectID": "publications/simglm-power.html#abstract",
    "href": "publications/simglm-power.html#abstract",
    "title": "Power Analysis by Simulation using R and simglm",
    "section": "",
    "text": "Power is a task that is commonly done prior to collecting data for a primary study. In most cases closed-form solutions are used to estimate power which may statistical assumptions to be able to perform the computations, for example assume residuals are normally distributed. In real-world data, these statistical assumptions may not hold, therefore estimates of power when these assumptions are assumed will likely be inflated. Power by simulation is another way to compute power estimates and offers significant flexibility to the user to explore the impact of various statistical assumption violations may have on power. This tutorial uses the simglm R package to perform the power by simulation. The simglm package provides a framework to simulate data from generalized linear mixed models which includes a wide variety of models. In addition, functions to perform replications and to compute power estimate summaries are available for users to take advantage of. Two worked examples are shown, one for a two-sample t-test and another within a repeated measures or longitudinal framework."
  },
  {
    "objectID": "publications/simglm-power.html#citation",
    "href": "publications/simglm-power.html#citation",
    "title": "Power Analysis by Simulation using R and simglm",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2019). Power Analysis by Simulation using R and simglm. **."
  },
  {
    "objectID": "publications/simglm-power.html#links",
    "href": "publications/simglm-power.html#links",
    "title": "Power Analysis by Simulation using R and simglm",
    "section": "Links",
    "text": "Links\nPDF Code\n\nPublication: Authors: Brandon LeBeau Date: June 11, 2019"
  },
  {
    "objectID": "publications/esj-19.html",
    "href": "publications/esj-19.html",
    "title": "Reading Comprehension Assessment: The Effect of Reading the Items Aloud Before or After Reading the Passage",
    "section": "",
    "text": "This study investigated the effects of imposing task- or process-oriented reading behaviors on reading comprehension assessment performance. Students in Grades 5-8 (N = 275) were randomly assigned to hear multiple-choice items read aloud before or after reading a test passage and when they were and were not allowed access to the passage while answering items. A confirmatory factor analysis found a one-factor model with all test items loading on the comprehension latent variable fit better than treating literal, inferential, and evaluative items as three separate variables. Subsequently, a structural equation model explored whether testing condition or other covariates (state assessment score and student demographics) explained variance in reading comprehension assessment performance. In Grades 5-6, significant positive effects were found for students who kept the text while answering items, regardless of whether or not they previewed the items. In Grades 7-8, no significant differences were found for the four testing conditions."
  },
  {
    "objectID": "publications/esj-19.html#abstract",
    "href": "publications/esj-19.html#abstract",
    "title": "Reading Comprehension Assessment: The Effect of Reading the Items Aloud Before or After Reading the Passage",
    "section": "",
    "text": "This study investigated the effects of imposing task- or process-oriented reading behaviors on reading comprehension assessment performance. Students in Grades 5-8 (N = 275) were randomly assigned to hear multiple-choice items read aloud before or after reading a test passage and when they were and were not allowed access to the passage while answering items. A confirmatory factor analysis found a one-factor model with all test items loading on the comprehension latent variable fit better than treating literal, inferential, and evaluative items as three separate variables. Subsequently, a structural equation model explored whether testing condition or other covariates (state assessment score and student demographics) explained variance in reading comprehension assessment performance. In Grades 5-6, significant positive effects were found for students who kept the text while answering items, regardless of whether or not they previewed the items. In Grades 7-8, no significant differences were found for the four testing conditions."
  },
  {
    "objectID": "publications/esj-19.html#citation",
    "href": "publications/esj-19.html#citation",
    "title": "Reading Comprehension Assessment: The Effect of Reading the Items Aloud Before or After Reading the Passage",
    "section": "Citation",
    "text": "Citation\nReed, Deborah K., Stevenson, Nathan, LeBeau, Brandon (2019). Reading Comprehension Assessment: The Effect of Reading the Items Aloud Before or After Reading the Passage. **The Elementary School Journal, (in press)."
  },
  {
    "objectID": "publications/esj-19.html#links",
    "href": "publications/esj-19.html#links",
    "title": "Reading Comprehension Assessment: The Effect of Reading the Items Aloud Before or After Reading the Passage",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: The Elementary School Journal, (in press) Authors: Deborah K. Reed, Nathan Stevenson, Brandon LeBeau Date: May 20, 2019 DOI: 10.1086/705784"
  },
  {
    "objectID": "publications/diss1.html",
    "href": "publications/diss1.html",
    "title": "Impact of Serial Correlation Misspecification with the Linear Mixed Model",
    "section": "",
    "text": "Linear mixed models are popular models for use with clustered and longitudinal data due to their ability to model variation at different levels of clustering. A Monte Carlo study was used to explore the impact of assumption violations on the bias of parameter estimates and the empirical type I error rates. Simulated conditions included in this study are: simulated serial correlation structure, fitted serial correlation structure, random effect distribution, cluster sample size, and number of measurement occasions. Results showed that the fixed effects are unbiased, but the random components tend to be overestimated and the empirical Type I error rates tend to be inflated. Implications for applied researchers were discussed."
  },
  {
    "objectID": "publications/diss1.html#abstract",
    "href": "publications/diss1.html#abstract",
    "title": "Impact of Serial Correlation Misspecification with the Linear Mixed Model",
    "section": "",
    "text": "Linear mixed models are popular models for use with clustered and longitudinal data due to their ability to model variation at different levels of clustering. A Monte Carlo study was used to explore the impact of assumption violations on the bias of parameter estimates and the empirical type I error rates. Simulated conditions included in this study are: simulated serial correlation structure, fitted serial correlation structure, random effect distribution, cluster sample size, and number of measurement occasions. Results showed that the fixed effects are unbiased, but the random components tend to be overestimated and the empirical Type I error rates tend to be inflated. Implications for applied researchers were discussed."
  },
  {
    "objectID": "publications/diss1.html#citation",
    "href": "publications/diss1.html#citation",
    "title": "Impact of Serial Correlation Misspecification with the Linear Mixed Model",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2016). Impact of Serial Correlation Misspecification with the Linear Mixed Model. **Journal of Modern Applied Statistical Methods, 15 (1), 389-416."
  },
  {
    "objectID": "publications/diss1.html#links",
    "href": "publications/diss1.html#links",
    "title": "Impact of Serial Correlation Misspecification with the Linear Mixed Model",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Journal of Modern Applied Statistical Methods, 15 (1), 389-416 Authors: Brandon LeBeau Date: May 01, 2016"
  },
  {
    "objectID": "publications/belin-irt.html",
    "href": "publications/belin-irt.html",
    "title": "Differentiating Among Gifted Learners: A Comparison of Classical Test Theory and Item Response Theory on Above-Level Testing",
    "section": "",
    "text": "This study investigated the application of item response theory (IRT) to expand the range of ability estimates for gifted (hereinafter referred to as high-achieving) students’ performance on an above-level test. Using a sample of 4th – 6th grade high-achieving students (n = 1,893), we conducted a case study to compare estimates from classical test theory (CTT) and IRT. First, we estimated means, standard deviations, and percentiles of the students using CTT statistics. Next, we fitted a two- parameter IRT model using a multi-group framework to represent the different grade levels. We compared CTT and IRT results to determine the value-added of using an IRT approach in identifying high-achieving students through the Talent Search Model of above-level testing. Benefits of IRT, compared to CTT and with respect to the Talent Search Model, include invariance of item parameters across groups of individuals and invariance of an individual’s ability across tests of the same construct. IRT also uses the response string of individuals’ scores; therefore, test items can differentially inform the latent aptitude construct. Implications for academic talent identification with the Talent Search Model and development of academic talent are discussed."
  },
  {
    "objectID": "publications/belin-irt.html#abstract",
    "href": "publications/belin-irt.html#abstract",
    "title": "Differentiating Among Gifted Learners: A Comparison of Classical Test Theory and Item Response Theory on Above-Level Testing",
    "section": "",
    "text": "This study investigated the application of item response theory (IRT) to expand the range of ability estimates for gifted (hereinafter referred to as high-achieving) students’ performance on an above-level test. Using a sample of 4th – 6th grade high-achieving students (n = 1,893), we conducted a case study to compare estimates from classical test theory (CTT) and IRT. First, we estimated means, standard deviations, and percentiles of the students using CTT statistics. Next, we fitted a two- parameter IRT model using a multi-group framework to represent the different grade levels. We compared CTT and IRT results to determine the value-added of using an IRT approach in identifying high-achieving students through the Talent Search Model of above-level testing. Benefits of IRT, compared to CTT and with respect to the Talent Search Model, include invariance of item parameters across groups of individuals and invariance of an individual’s ability across tests of the same construct. IRT also uses the response string of individuals’ scores; therefore, test items can differentially inform the latent aptitude construct. Implications for academic talent identification with the Talent Search Model and development of academic talent are discussed."
  },
  {
    "objectID": "publications/belin-irt.html#citation",
    "href": "publications/belin-irt.html#citation",
    "title": "Differentiating Among Gifted Learners: A Comparison of Classical Test Theory and Item Response Theory on Above-Level Testing",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon, Assouline, Susan G., Mahatmya, Duhita, Lupkowski-Shoplik, Ann (2020). Differentiating Among Gifted Learners: A Comparison of Classical Test Theory and Item Response Theory on Above-Level Testing. **Gifted Child Quarterly, 64 (3), 219-237."
  },
  {
    "objectID": "publications/belin-irt.html#links",
    "href": "publications/belin-irt.html#links",
    "title": "Differentiating Among Gifted Learners: A Comparison of Classical Test Theory and Item Response Theory on Above-Level Testing",
    "section": "Links",
    "text": "Links\nLink to Journal PDF\n\nPublication: Gifted Child Quarterly, 64 (3), 219-237 Authors: Brandon LeBeau, Susan G. Assouline, Duhita Mahatmya, Ann Lupkowski-Shoplik Date: June 01, 2020 DOI: 10.1177/0016986220924050"
  },
  {
    "objectID": "publications/monte-carlo-design.html",
    "href": "publications/monte-carlo-design.html",
    "title": "Design and Analysis of Monte Carlo Studies: Improving External Validity",
    "section": "",
    "text": "As models have become more complex, mathematical derivations have become increasingly difficult or impossible. As a result, monte carlo methods have become popular to evaluate model assumption violations and their implications for unbiased estimates. However, external validity of monte carlo studies can be a significant weakness. This paper looks to explore that issue in more detail and suggests a simple design and analysis adjustment to increase the coverage of the simulation conditions which, it is argued, in turn improves the external validity of the monte carlo study. This is explored with a motivating monte carlo example based on a one variable analysis of variance."
  },
  {
    "objectID": "publications/monte-carlo-design.html#abstract",
    "href": "publications/monte-carlo-design.html#abstract",
    "title": "Design and Analysis of Monte Carlo Studies: Improving External Validity",
    "section": "",
    "text": "As models have become more complex, mathematical derivations have become increasingly difficult or impossible. As a result, monte carlo methods have become popular to evaluate model assumption violations and their implications for unbiased estimates. However, external validity of monte carlo studies can be a significant weakness. This paper looks to explore that issue in more detail and suggests a simple design and analysis adjustment to increase the coverage of the simulation conditions which, it is argued, in turn improves the external validity of the monte carlo study. This is explored with a motivating monte carlo example based on a one variable analysis of variance."
  },
  {
    "objectID": "publications/monte-carlo-design.html#citation",
    "href": "publications/monte-carlo-design.html#citation",
    "title": "Design and Analysis of Monte Carlo Studies: Improving External Validity",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2020). Design and Analysis of Monte Carlo Studies: Improving External Validity. **.\n\nPublication: Authors: Brandon LeBeau Date: June 01, 2020"
  },
  {
    "objectID": "publications/prior-sim.html",
    "href": "publications/prior-sim.html",
    "title": "Ability and Prior Distribution Mismatch: An Exploration of Common-Item Linking Methods",
    "section": "",
    "text": "Linking of two forms is an important task when using item response theory, particularly when two forms are administered to nonequivalent groups. When linking with characteristic curve methods, the ability distribution and weights associated with that distribution can be used to weight observations differently. These are commonly specified as equally spaced intervals from −4 to 4, but other options or distributional forms can be specified. The use of these different distributions and weights of the ability distributions will be explored with a Monte Carlo simulation. Primary simulation conditions will include sample size, number of items, number of common items, ability distribution, and randomly varying population transformation constants. Study results show that the linking weights have little impact on the estimation of the linking constants; however, the underlying ability distribution of examinees does have significant impact. Implications for applied researchers will be discussed."
  },
  {
    "objectID": "publications/prior-sim.html#abstract",
    "href": "publications/prior-sim.html#abstract",
    "title": "Ability and Prior Distribution Mismatch: An Exploration of Common-Item Linking Methods",
    "section": "",
    "text": "Linking of two forms is an important task when using item response theory, particularly when two forms are administered to nonequivalent groups. When linking with characteristic curve methods, the ability distribution and weights associated with that distribution can be used to weight observations differently. These are commonly specified as equally spaced intervals from −4 to 4, but other options or distributional forms can be specified. The use of these different distributions and weights of the ability distributions will be explored with a Monte Carlo simulation. Primary simulation conditions will include sample size, number of items, number of common items, ability distribution, and randomly varying population transformation constants. Study results show that the linking weights have little impact on the estimation of the linking constants; however, the underlying ability distribution of examinees does have significant impact. Implications for applied researchers will be discussed."
  },
  {
    "objectID": "publications/prior-sim.html#citation",
    "href": "publications/prior-sim.html#citation",
    "title": "Ability and Prior Distribution Mismatch: An Exploration of Common-Item Linking Methods",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2017). Ability and Prior Distribution Mismatch: An Exploration of Common-Item Linking Methods. **Applied Psychological Methods, 41 (7), 545-560."
  },
  {
    "objectID": "publications/prior-sim.html#links",
    "href": "publications/prior-sim.html#links",
    "title": "Ability and Prior Distribution Mismatch: An Exploration of Common-Item Linking Methods",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Applied Psychological Methods, 41 (7), 545-560 Authors: Brandon LeBeau Date: May 18, 2017 DOI: 10.1177/0146621617707508"
  },
  {
    "objectID": "publications/heterotypic-review.html",
    "href": "publications/heterotypic-review.html",
    "title": "Studying a Moving Target in Development: The Challenge and Opportunity of Heterotypic Continuity",
    "section": "",
    "text": "Many psychological constructs show heterotypic continuity—their behavioral manifestations change with development but their meaning remains the same (e.g., externalizing problems). However, research has paid little attention to how to account for heterotypic continuity. Conceptual and methodological challenges of heterotypic continuity may prevent researchers from examining lengthy developmental spans. Developmental theory requires that measurement accommodate changes in manifestation of constructs. Simulation and empirical work demonstrate that failure to account for heterotypic continuity when collecting or analyzing longitudinal data results in faulty developmental inferences. Accounting for heterotypic continuity may require using different measures across time with approaches that link measures on a comparable scale. Creating a developmental scale (i.e., developmental scaling) is recommended to link measures across time and account for heterotypic continuity, which is crucial in understanding development across the lifespan. The current synthesized review defines heterotypic continuity, describes how to identify it, and presents solutions to account for it. We note challenges of addressing heterotypic continuity, and propose steps in leveraging opportunities it creates to advance empirical study of development."
  },
  {
    "objectID": "publications/heterotypic-review.html#abstract",
    "href": "publications/heterotypic-review.html#abstract",
    "title": "Studying a Moving Target in Development: The Challenge and Opportunity of Heterotypic Continuity",
    "section": "",
    "text": "Many psychological constructs show heterotypic continuity—their behavioral manifestations change with development but their meaning remains the same (e.g., externalizing problems). However, research has paid little attention to how to account for heterotypic continuity. Conceptual and methodological challenges of heterotypic continuity may prevent researchers from examining lengthy developmental spans. Developmental theory requires that measurement accommodate changes in manifestation of constructs. Simulation and empirical work demonstrate that failure to account for heterotypic continuity when collecting or analyzing longitudinal data results in faulty developmental inferences. Accounting for heterotypic continuity may require using different measures across time with approaches that link measures on a comparable scale. Creating a developmental scale (i.e., developmental scaling) is recommended to link measures across time and account for heterotypic continuity, which is crucial in understanding development across the lifespan. The current synthesized review defines heterotypic continuity, describes how to identify it, and presents solutions to account for it. We note challenges of addressing heterotypic continuity, and propose steps in leveraging opportunities it creates to advance empirical study of development."
  },
  {
    "objectID": "publications/heterotypic-review.html#citation",
    "href": "publications/heterotypic-review.html#citation",
    "title": "Studying a Moving Target in Development: The Challenge and Opportunity of Heterotypic Continuity",
    "section": "Citation",
    "text": "Citation\nPetersen, Isaac T., Choe, Daniel E., LeBeau, Brandon (2020). Studying a Moving Target in Development: The Challenge and Opportunity of Heterotypic Continuity. Developmental Review."
  },
  {
    "objectID": "publications/heterotypic-review.html#links",
    "href": "publications/heterotypic-review.html#links",
    "title": "Studying a Moving Target in Development: The Challenge and Opportunity of Heterotypic Continuity",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Developmental Review Authors: Isaac T. Petersen, Daniel E. Choe, Brandon LeBeau Date: June 01, 2020 DOI: 10.1016/j.dr.2020.100935"
  },
  {
    "objectID": "publications/belin-ioapa-19.html",
    "href": "publications/belin-ioapa-19.html",
    "title": "The Advanced Placement Program in Rural Schools: Equalizing Opportunity",
    "section": "",
    "text": "The current study explores the effect an online Advanced Placement (AP) program has had on AP participation and passing rates in a single state. This state has a sizeable rural population where access to AP coursework is not always possible for students in rural communities. The online AP program aims to equalize access to rural students who would otherwise not have the opportunity to take advanced coursework. Study results show that although AP exam participation has not significantly increased due to the availability of the online AP program, passing rates have been higher for schools who do participate. In addition, the online AP coursework has increased access to AP coursework in small to middle sized schools."
  },
  {
    "objectID": "publications/belin-ioapa-19.html#abstract",
    "href": "publications/belin-ioapa-19.html#abstract",
    "title": "The Advanced Placement Program in Rural Schools: Equalizing Opportunity",
    "section": "",
    "text": "The current study explores the effect an online Advanced Placement (AP) program has had on AP participation and passing rates in a single state. This state has a sizeable rural population where access to AP coursework is not always possible for students in rural communities. The online AP program aims to equalize access to rural students who would otherwise not have the opportunity to take advanced coursework. Study results show that although AP exam participation has not significantly increased due to the availability of the online AP program, passing rates have been higher for schools who do participate. In addition, the online AP coursework has increased access to AP coursework in small to middle sized schools."
  },
  {
    "objectID": "publications/belin-ioapa-19.html#citation",
    "href": "publications/belin-ioapa-19.html#citation",
    "title": "The Advanced Placement Program in Rural Schools: Equalizing Opportunity",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon, Assouline, Susan G., Lupkowski-Shoplik, Ann, Mahatmya, Duhita (2020). The Advanced Placement Program in Rural Schools: Equalizing Opportunity. Roeper Review."
  },
  {
    "objectID": "publications/belin-ioapa-19.html#links",
    "href": "publications/belin-ioapa-19.html#links",
    "title": "The Advanced Placement Program in Rural Schools: Equalizing Opportunity",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Roeper Review Authors: Brandon LeBeau, Susan G. Assouline, Ann Lupkowski-Shoplik, Duhita Mahatmya Date: June 01, 2020 DOI: 10.1080/02783193.2020.1765923"
  },
  {
    "objectID": "publications/child-development-2020.html",
    "href": "publications/child-development-2020.html",
    "title": "Creating a developmental scale to account for heterotypic continuity in development: A simulation study",
    "section": "",
    "text": "Many psychological constructs show heterotypic continuity—their behavioral manifestations change with development (e.g., externalizing problems). However, research has paid little attention to how to account for heterotypic continuity. A promising approach to account for heterotypic continuity is creating a developmental scale using vertical scaling. We conducted a simulation to compare creating a developmental scale using vertical scaling to traditional approaches of assessing a construct over time. Traditional approaches that failed to account for heterotypic continuity resulted in less accurate growth estimates, at the person- and group-level. Findings suggest that ignoring heterotypic continuity may result in faulty developmental inferences. Creating a developmental scale with vertical scaling is recommended to link different measures across time and account for heterotypic continuity."
  },
  {
    "objectID": "publications/child-development-2020.html#abstract",
    "href": "publications/child-development-2020.html#abstract",
    "title": "Creating a developmental scale to account for heterotypic continuity in development: A simulation study",
    "section": "",
    "text": "Many psychological constructs show heterotypic continuity—their behavioral manifestations change with development (e.g., externalizing problems). However, research has paid little attention to how to account for heterotypic continuity. A promising approach to account for heterotypic continuity is creating a developmental scale using vertical scaling. We conducted a simulation to compare creating a developmental scale using vertical scaling to traditional approaches of assessing a construct over time. Traditional approaches that failed to account for heterotypic continuity resulted in less accurate growth estimates, at the person- and group-level. Findings suggest that ignoring heterotypic continuity may result in faulty developmental inferences. Creating a developmental scale with vertical scaling is recommended to link different measures across time and account for heterotypic continuity."
  },
  {
    "objectID": "publications/child-development-2020.html#citation",
    "href": "publications/child-development-2020.html#citation",
    "title": "Creating a developmental scale to account for heterotypic continuity in development: A simulation study",
    "section": "Citation",
    "text": "Citation\nPeterson, Isaac T., LeBeau, Brandon, Choe, Daniel (2020). Creating a developmental scale to account for heterotypic continuity in development: A simulation study. Child Development."
  },
  {
    "objectID": "publications/child-development-2020.html#links",
    "href": "publications/child-development-2020.html#links",
    "title": "Creating a developmental scale to account for heterotypic continuity in development: A simulation study",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Child Development Authors: Isaac T. Peterson, Brandon LeBeau, Daniel Choe Date: June 01, 2020 DOI: 10.1111/cdev.13433"
  },
  {
    "objectID": "publications/growth-different-measures.html",
    "href": "publications/growth-different-measures.html",
    "title": "Creating a Developmental Scale to Chart the Development of Psychopathology with Different Informants and Measures across Time",
    "section": "",
    "text": "RDoC aims to advance a dimensional, multilevel understanding of psychopathology across the lifespan. Two key challenges exist in applying a developmental perspective to RDoC: First, the most accurate informants for assessing a person’s psychopathology often differ across development (e.g., parents and teachers may be better informants of a person’s externalizing problems in early childhood, whereas peer- and self-report may also be important to assess in adolescence). Second, many constructs change in their behavioral manifestation across development (i.e., heterotypic continuity). Thus, different informants and measures across time may be necessary to account for the construct’s changing manifestation. The challenge of using different informants and measures of a construct across time is ensuring that the same construct is assessed in a comparable way across development. Vertical scaling creates a developmental scale to link scores from changing informants and measures to account for heterotypic continuity and study people’s development of psychopathology across the lifespan. This is the first study that created a developmental scale to assess people’s development by putting different informants and measures on the same scale. We examined the development of externalizing problems from ages 2–15 years (N=1,364) using annual ratings by mothers, fathers, teachers, other caregivers, and self-report. The developmental scale linked different informants and measures on the same scale. This allowed us to chart people’s growth trajectories and to identify multilevel risk factors, including poor verbal comprehension. Creating a developmental scale may be crucial to advance RDoC’s goal of studying the development of psychopathology across the lifespan."
  },
  {
    "objectID": "publications/growth-different-measures.html#abstract",
    "href": "publications/growth-different-measures.html#abstract",
    "title": "Creating a Developmental Scale to Chart the Development of Psychopathology with Different Informants and Measures across Time",
    "section": "",
    "text": "RDoC aims to advance a dimensional, multilevel understanding of psychopathology across the lifespan. Two key challenges exist in applying a developmental perspective to RDoC: First, the most accurate informants for assessing a person’s psychopathology often differ across development (e.g., parents and teachers may be better informants of a person’s externalizing problems in early childhood, whereas peer- and self-report may also be important to assess in adolescence). Second, many constructs change in their behavioral manifestation across development (i.e., heterotypic continuity). Thus, different informants and measures across time may be necessary to account for the construct’s changing manifestation. The challenge of using different informants and measures of a construct across time is ensuring that the same construct is assessed in a comparable way across development. Vertical scaling creates a developmental scale to link scores from changing informants and measures to account for heterotypic continuity and study people’s development of psychopathology across the lifespan. This is the first study that created a developmental scale to assess people’s development by putting different informants and measures on the same scale. We examined the development of externalizing problems from ages 2–15 years (N=1,364) using annual ratings by mothers, fathers, teachers, other caregivers, and self-report. The developmental scale linked different informants and measures on the same scale. This allowed us to chart people’s growth trajectories and to identify multilevel risk factors, including poor verbal comprehension. Creating a developmental scale may be crucial to advance RDoC’s goal of studying the development of psychopathology across the lifespan."
  },
  {
    "objectID": "publications/growth-different-measures.html#citation",
    "href": "publications/growth-different-measures.html#citation",
    "title": "Creating a Developmental Scale to Chart the Development of Psychopathology with Different Informants and Measures across Time",
    "section": "Citation",
    "text": "Citation\nPetersen, Isaac T., LeBeau, Brandon (2020). Creating a Developmental Scale to Chart the Development of Psychopathology with Different Informants and Measures across Time. Journal of Abnormal Psychology.\n\nPublication: Journal of Abnormal Psychology Authors: Isaac T. Petersen, Brandon LeBeau Date: June 01, 2020"
  },
  {
    "objectID": "publications/mnmap5.html",
    "href": "publications/mnmap5.html",
    "title": "A Multi-Institutional Study of High School Mathematics Curricula and College Mathematics Achievement and Course Taking",
    "section": "",
    "text": "This study examined the relationship between high school mathematics curricula and student achievement and course-taking patterns over 4 years of college course taking for a sample of over 10,000 students from 32 postsecondary 4-year institutions. Three types of curricula were studied: National Science Foundation (NSF) funded curricula, the University of Chicago School Mathematics Project curriculum, and commercially developed curricula. The major result was that high school mathematics curricula were unrelated to college mathematics achievement or students’ course-taking patterns for students who began college with precalculus (college algebra) or a more difficult course. However, students of the NSF-funded curricula were statistically more likely to begin their college mathematics at the developmental level. Implications of these results for research and practice are discussed."
  },
  {
    "objectID": "publications/mnmap5.html#abstract",
    "href": "publications/mnmap5.html#abstract",
    "title": "A Multi-Institutional Study of High School Mathematics Curricula and College Mathematics Achievement and Course Taking",
    "section": "",
    "text": "This study examined the relationship between high school mathematics curricula and student achievement and course-taking patterns over 4 years of college course taking for a sample of over 10,000 students from 32 postsecondary 4-year institutions. Three types of curricula were studied: National Science Foundation (NSF) funded curricula, the University of Chicago School Mathematics Project curriculum, and commercially developed curricula. The major result was that high school mathematics curricula were unrelated to college mathematics achievement or students’ course-taking patterns for students who began college with precalculus (college algebra) or a more difficult course. However, students of the NSF-funded curricula were statistically more likely to begin their college mathematics at the developmental level. Implications of these results for research and practice are discussed."
  },
  {
    "objectID": "publications/mnmap5.html#citation",
    "href": "publications/mnmap5.html#citation",
    "title": "A Multi-Institutional Study of High School Mathematics Curricula and College Mathematics Achievement and Course Taking",
    "section": "Citation",
    "text": "Citation\nHarwell, Michael R., Post, Thomas R., Medhanie, Amanuel, Dupuis, Danielle, LeBeau, Brandon (2013). A Multi-Institutional Study of High School Mathematics Curricula and College Mathematics Achievement and Course Taking. **Journal for Research in Mathematics Education, 44.5, 742 - 774."
  },
  {
    "objectID": "publications/mnmap5.html#links",
    "href": "publications/mnmap5.html#links",
    "title": "A Multi-Institutional Study of High School Mathematics Curricula and College Mathematics Achievement and Course Taking",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Journal for Research in Mathematics Education, 44.5, 742 - 774 Authors: Michael R. Harwell, Thomas R. Post, Amanuel Medhanie, Danielle Dupuis, Brandon LeBeau Date: June 01, 2013 DOI: 10.5951/jresematheduc.44.5.0742"
  },
  {
    "objectID": "publications/mnmap1.html",
    "href": "publications/mnmap1.html",
    "title": "The role of the accuplacer mathematics placement test on a student’s first college mathematics course",
    "section": "",
    "text": "The first college mathematics course a student enrolls in is often affected by performance on a college mathematics placement test. Yet validity evidence of mathematics placement tests remains limited, even for nationally standardized placement tests, and when it is available usually consists of examining a student’s subsequent performance in mathematics courses. This study expands on existing literature by considering whether a nationally standardized college mathematics placement test (ACCUPLACER) contributes to the prediction of enrollment and success in developmental and nondevelopmental mathematics courses above and beyond prediction associated with the ACT mathematics test. Results for a sample of more than 1,300 students from 20 postsecondary institutions suggest that ACCUPLACER does not contribute to either the prediction of enrollment or subsequent success in such courses, and that comparable information is provided by using the ACT mathematics score alone. Implications of these findings are discussed."
  },
  {
    "objectID": "publications/mnmap1.html#abstract",
    "href": "publications/mnmap1.html#abstract",
    "title": "The role of the accuplacer mathematics placement test on a student’s first college mathematics course",
    "section": "",
    "text": "The first college mathematics course a student enrolls in is often affected by performance on a college mathematics placement test. Yet validity evidence of mathematics placement tests remains limited, even for nationally standardized placement tests, and when it is available usually consists of examining a student’s subsequent performance in mathematics courses. This study expands on existing literature by considering whether a nationally standardized college mathematics placement test (ACCUPLACER) contributes to the prediction of enrollment and success in developmental and nondevelopmental mathematics courses above and beyond prediction associated with the ACT mathematics test. Results for a sample of more than 1,300 students from 20 postsecondary institutions suggest that ACCUPLACER does not contribute to either the prediction of enrollment or subsequent success in such courses, and that comparable information is provided by using the ACT mathematics score alone. Implications of these findings are discussed."
  },
  {
    "objectID": "publications/mnmap1.html#citation",
    "href": "publications/mnmap1.html#citation",
    "title": "The role of the accuplacer mathematics placement test on a student’s first college mathematics course",
    "section": "Citation",
    "text": "Citation\nMedhanie, Amanuel, Dupuis, Danielle, LeBeau, Brandon, Harwell, Michael R., Post, Thomas R. (2012). The role of the accuplacer mathematics placement test on a student’s first college mathematics course. **Educational and Psychological Measurement, 72(2), 332 - 351."
  },
  {
    "objectID": "publications/mnmap1.html#links",
    "href": "publications/mnmap1.html#links",
    "title": "The role of the accuplacer mathematics placement test on a student’s first college mathematics course",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Educational and Psychological Measurement, 72(2), 332 - 351 Authors: Amanuel Medhanie, Danielle Dupuis, Brandon LeBeau, Michael R. Harwell, Thomas R. Post Date: March 27, 2012"
  },
  {
    "objectID": "publications/mnmap3.html",
    "href": "publications/mnmap3.html",
    "title": "Student and High-School Characteristics related to completing a science, technology, engineering or mathematics (STEM) major in college",
    "section": "",
    "text": "Background: The importance of increasing the number of US college students completing degrees in science, technology, engineering or mathematics (STEM) has prompted calls for research to provide a better understanding of factors related to student participation in these majors, including the impact of a stu- dent’s high-school mathematics curriculum. Purpose: This study examines the relationship between various student and high-school characteristics and completion of a STEM major in college. Of spe- cific interest is the influence of a student’s high-school mathematics curriculum on the completion of a STEM major in college. Sample: The sample consisted of approximately 3500 students from 229 high schools. Students were predominantly Caucasian (80%), with slightly more males than females (52% vs 48%). Design and method: A quasi-experimental design with archival data was used for students who enrolled in, and graduated from, a post-secondary institution in the upper Midwest. To be included in the sample, students needed to have com- pleted at least three years of high-school mathematics. A generalized linear mixed model was used with students nested within high schools. The data were cross-sectional. Results: High-school predictors were not found to have a significant impact on the completion of a STEM major. Significant student-level predictors included ACT mathematics score, gender and high-school mathematics GPA. Conclusions: The results provide evidence that on average students are equally prepared for the rigorous mathematics coursework regardless of the high-school mathematics curriculum they completed."
  },
  {
    "objectID": "publications/mnmap3.html#abstract",
    "href": "publications/mnmap3.html#abstract",
    "title": "Student and High-School Characteristics related to completing a science, technology, engineering or mathematics (STEM) major in college",
    "section": "",
    "text": "Background: The importance of increasing the number of US college students completing degrees in science, technology, engineering or mathematics (STEM) has prompted calls for research to provide a better understanding of factors related to student participation in these majors, including the impact of a stu- dent’s high-school mathematics curriculum. Purpose: This study examines the relationship between various student and high-school characteristics and completion of a STEM major in college. Of spe- cific interest is the influence of a student’s high-school mathematics curriculum on the completion of a STEM major in college. Sample: The sample consisted of approximately 3500 students from 229 high schools. Students were predominantly Caucasian (80%), with slightly more males than females (52% vs 48%). Design and method: A quasi-experimental design with archival data was used for students who enrolled in, and graduated from, a post-secondary institution in the upper Midwest. To be included in the sample, students needed to have com- pleted at least three years of high-school mathematics. A generalized linear mixed model was used with students nested within high schools. The data were cross-sectional. Results: High-school predictors were not found to have a significant impact on the completion of a STEM major. Significant student-level predictors included ACT mathematics score, gender and high-school mathematics GPA. Conclusions: The results provide evidence that on average students are equally prepared for the rigorous mathematics coursework regardless of the high-school mathematics curriculum they completed."
  },
  {
    "objectID": "publications/mnmap3.html#citation",
    "href": "publications/mnmap3.html#citation",
    "title": "Student and High-School Characteristics related to completing a science, technology, engineering or mathematics (STEM) major in college",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon, Harwell, Michael R., Monson, Debra, Dupuis, Danielle, Medhanie, Amanuel, Post, Thomas R. (2012). Student and High-School Characteristics related to completing a science, technology, engineering or mathematics (STEM) major in college. **Research in Science & Technological Education, 30:1, 17 - 28."
  },
  {
    "objectID": "publications/mnmap3.html#links",
    "href": "publications/mnmap3.html#links",
    "title": "Student and High-School Characteristics related to completing a science, technology, engineering or mathematics (STEM) major in college",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Research in Science & Technological Education, 30:1, 17 - 28 Authors: Brandon LeBeau, Michael R. Harwell, Debra Monson, Danielle Dupuis, Amanuel Medhanie, Thomas R. Post Date: March 19, 2012 DOI: 10.1080/02635143.2012.659178"
  },
  {
    "objectID": "publications/sageopen-lmm.html",
    "href": "publications/sageopen-lmm.html",
    "title": "Model misspecification and assumption violations with the linear mixed model: A meta-analysis",
    "section": "",
    "text": "This meta-analysis attempts to synthesize the Monte Carlo literature for the linear mixed model under a longitudinal framework. The meta-analysis aims to inform researchers about conditions that are important to consider when evaluating model assumptions and adequacy. In addition, the meta-analysis may be helpful to those wishing to design future Monte Carlo simulations in identifying simulation conditions. The current meta-analysis will use the empirical type I error rate as the effect size and Monte Carlo simulation conditions will be coded to serve as moderator variables. The type I error rate for the fixed and random effects will be explored as the primary dependent variable. Effect sizes were coded from 13 studies, resulting in a total of 4,002 and 621 effect sizes for fixed and random effects respectively. Meta-regression and proportional odds models were used to explore variation in the empirical type I error rate effect sizes. Implications for applied researchers and researchers planning new Monte Carlo studies will be explored."
  },
  {
    "objectID": "publications/sageopen-lmm.html#abstract",
    "href": "publications/sageopen-lmm.html#abstract",
    "title": "Model misspecification and assumption violations with the linear mixed model: A meta-analysis",
    "section": "",
    "text": "This meta-analysis attempts to synthesize the Monte Carlo literature for the linear mixed model under a longitudinal framework. The meta-analysis aims to inform researchers about conditions that are important to consider when evaluating model assumptions and adequacy. In addition, the meta-analysis may be helpful to those wishing to design future Monte Carlo simulations in identifying simulation conditions. The current meta-analysis will use the empirical type I error rate as the effect size and Monte Carlo simulation conditions will be coded to serve as moderator variables. The type I error rate for the fixed and random effects will be explored as the primary dependent variable. Effect sizes were coded from 13 studies, resulting in a total of 4,002 and 621 effect sizes for fixed and random effects respectively. Meta-regression and proportional odds models were used to explore variation in the empirical type I error rate effect sizes. Implications for applied researchers and researchers planning new Monte Carlo studies will be explored."
  },
  {
    "objectID": "publications/sageopen-lmm.html#citation",
    "href": "publications/sageopen-lmm.html#citation",
    "title": "Model misspecification and assumption violations with the linear mixed model: A meta-analysis",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon, Song, Yoon Ah, Liu, Wei Cheng (2018). Model misspecification and assumption violations with the linear mixed model: A meta-analysis. **Sage Open, 8 (4)."
  },
  {
    "objectID": "publications/sageopen-lmm.html#links",
    "href": "publications/sageopen-lmm.html#links",
    "title": "Model misspecification and assumption violations with the linear mixed model: A meta-analysis",
    "section": "Links",
    "text": "Links\nLink to Sage Open PDF\n\nPublication: Sage Open, 8 (4) Authors: Brandon LeBeau, Yoon Ah Song, Wei Cheng Liu Date: December 01, 2018 DOI: 10.1177/2158244018820380"
  },
  {
    "objectID": "posts/2014-07-30-format-markdown-documents-in-r.html",
    "href": "posts/2014-07-30-format-markdown-documents-in-r.html",
    "title": "Format Markdown Documents in R",
    "section": "",
    "text": "Have you ever used a markdown file to create an html file? Have you ever wanted to quickly format the subsequent html file to add some color or other aspects? If your answer is yes to both of those questions, this package may be of interest to you.\nThe highlightHTML package aims to develop a flexible approach to add formatting to an html document by injecting CSS into the file. To do this, tags are created within the markdown document telling the R routine where to look for these tags. If you are familiar with the Twitterverse, this package will be equally comfortable. The tags take the form of the hashtags on Twitter. As an example, #bgblue, would be a command to change the background to blue.\nThe next thing needed by the package is to tell how much of the word, sentence, or header that should be affected by the tag. To do this, add braces before the tag and include all the content you want to be affected by the tag. For example, {#bggold this example will have a blue background}.\nOnce any tags you want to include are in the markdown document, then the document can be converted into a html file using programs such as knitr, pandoc, the RStudio “knit HTML” button (or any others). Once the resulting html file is compiled, then run the html file through the highlightHTML package and the html file is searched for the tags, the tags are changed to CSS ids, and by default the CSS tags will be inserted automatically back into the document.\n\nMinimal Example\n\n\n\nA\nma\nrk\ndo\nwn\nd\noc\num\nen\nt that looks like the following:\n\n\n`` #\n`m Te\nar st\nkd o\now f\nn {#\nco\nlg\nol\nd highlightHTML} package\n\n\nCa\nn\nhi\ngh\nli\ngh\nt\n{#\nbg\nblack multiple words}.\n\n\nEv\nen\nt\nab\nle\ns:\n\n\n\n\n\n\n| |- | | | |\nCo – Bl Gr Or Re\nlo – ue ee an d\nr –\nNa –\nme –\n| -| | | | |\nN – 5 3 1 2\num –\nber | ——–| #bgblue | | | #bgred |\n\n\n``\n`\n\n\n\n\n\n\n\n\n\n\nYo\nu\nwo\nul\nd\nth\nen\nc\non\nvert the markdown above into a html file (I hit the knit HTML button in RStudio for this file), then run the following commands in R (assuming the highlightHTML package is not installed):\n\n\n::\n:\n{.\nce\nll\n}\n\n\n\n\n\n\n`` li in li\n`{ br st br\n.r ar al ar\n. y( l_ y(\nce de gi hi\nll vt th gh\n-c oo ub li\nod ls (r gh\ne} ) ep tH\n\n\n\nta\ngs\n&lt;\n-\nc(\n“#”# TM\nbg co L(\nre lg in up\nd ol pu da\n{background-color: red;}“,”#bgblue {background-color: blue;}“, d {color: gold;}”, “#bgblack {background-color: black; color: white;}”) t = “path/to/infile.html”, output = “path/to/outfile.html”, teCSS = TRUE, tags = tags, browse = TRUE)\n\n\nTh ![\nis ](\nc /p\nom os\nma t/\nnd 20\nw 14\nil -0\nl 7-\nprocess the html file, look for tags, change the tags to CSS ids, inject the CSS into the document, and lastly open the output file in the browser to see how it looks. The example above would look like the following after the above commands: 30-highlightHTML_figs/mwe.png)\n\n\nYo\nu\nca\nn\nal\nso\ng\no\nto\nthis link to see the post-processed file: educate-r.org/mwe.html.\n\n\n##\n#\nUp\nco\nmi\nng\nF\nea\ntu\nres\n\n\n\nCurrently the package assumes that you know CSS and can supply your own tags. In the future I’d like to relax this and allow for some basic tags that work without needing to supply the CSS. I’m hoping to allow background color and text color changes to be made without needing to specify the CSS. For example, when specifying #bgblue in the markdown file, the R program knows that you want the background color (bg) to be blue.\nTry it out and let me know of new features or bugs as you work through the package."
  },
  {
    "objectID": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html",
    "href": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html",
    "title": "Introduction of the pdfsearch package",
    "section": "",
    "text": "I’m happy to introduce an add-on package, pdfsearch, that adds the ability to do keyword searches on pdf files. This add-on package uses the excellent pdftools package from the ropensci project to read in pdf files and perform keyword searches based character strings of interest."
  },
  {
    "objectID": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#installation",
    "href": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#installation",
    "title": "Introduction of the pdfsearch package",
    "section": "Installation",
    "text": "Installation\nThe package is currently only hosted on github and can be installed with the devtools library.\n\ndevtools::install_github('lebebr01/pdfsearch')"
  },
  {
    "objectID": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#basic-example",
    "href": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#basic-example",
    "title": "Introduction of the pdfsearch package",
    "section": "Basic Example",
    "text": "Basic Example\nDoing a simple keyword search on a single pdf file uses the keyword_search function. The following is a simple example using a pdf from arXiv.\n\nlibrary(pdfsearch)\n\nfile &lt;- system.file('pdf', '1501.00450.pdf', package = 'pdfsearch')\n\nkey_res &lt;- keyword_search(file, \n                          keyword = c('repeated measures', 'mixed effects'),\n                          path = TRUE)\n\nIn the following example, the function keyword_search takes two required arguments, the path to the pdf file and the keyword(s) to search for in the pdf. The optional argument shown above, path tells the function to read in the raw pdf using the pdftools package.\n\ndata.frame(key_res)\n\n            keyword page_num line_num\n1 repeated measures        1        9\n2 repeated measures        2       31\n3 repeated measures        2       58\n4 repeated measures        2       60\n5 repeated measures        3       70\n6 repeated measures        6      169\n7 repeated measures        6      180\n8 repeated measures        6      185\n9 repeated measures        9      315\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                line_text\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not introduce more sophisticated experimental designs, specifi-           only would we miss potentially beneficial effects, we may also cally the repeated measures design, including the crossover           get false confidence about lack of negative effects. \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We also discuss practical considfast iterations and testing many ideas can reap the most           erations to repeated measures design, with variants to the rewards.                                                           crossover design to study the carry over effect, including the “re-randomized” design (row 5 in table 1). \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        To facilitate our illustration, in all the derivation repeated measures design in different stages of treatment          in this section we assume all users appear in all periods, assignment. \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We also restrict ourselves ing the repeated measures analysis, reporting a “per week”         to metrics that are defined as simple average and assume treatment effect, as show in row 3 “parallel” design in ta-        treatment and control have the same sample size. \n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This way, various In fact, the crossover design is a type of repeated measures       designs considered can be examined in the same framework design commonly used in biomedical research to control for         and easily compared.\n6                                                                                                                                                                                                                     FLEXIBLE AND SCALABLE REPEATED One way to see measurements are not missing at random is                MEASURES ANALYSIS VIA FORME to realize infrequent users are more likely to have missing         5.1 Review of Existing Methods values and the absence in a specific time window can still          It is common to analyze data from repeated measures design provide information on the user behavior and in reality there       with the repeated measures ANOVA model and the F-test, might be other factors causing user to be missing that are          under certain assumptions, such as normality, sphericity (honot even observed. \n7 \\022P            P            \\023          In our cases they are indicators of treatment assignment, k Xik Pk0 Xi k 0 0 Cov(Xi , Xi ) = Cov 0          P        ,                         periods of the measurement, user id, and any other covariate. k Iik     k 0 Ii k 0 0 \\022           \\023                         As an example, one possible model for repeated measures Xi Xi0                              using lme4’s formula syntax (Bates et al. 2012a;b) is = Cov       , Ii Ii0                                   Y ∼ 1 + IsT reatment + P eriod + (1|U serID), where the last equality is by dividing both numerator and de-       where the only difference of this model to the usual linnominator by the same total number of users who have ever           ear model behind two sample test is the extra random efappeared in the experiments. \n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In repeated measures data, users might appear in book treatment of the delta-method.                                 multiple periods, represented as multiple rows in the dataset. \n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    PRACTICAL CONSIDERATIONS                                          ryover effect, the re-randomized design enables us to At the design stage, we face a few choices under the same               measure it directly and should be used here. framework of repeated measures design. \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 token_text\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                not, introduce, more, sophisticated, experimental, designs, specifi, only, would, we, miss, potentially, beneficial, effects, we, may, also, cally, the, repeated, measures, design, including, the, crossover, get, false, confidence, about, lack, of, negative, effects\n2                                                                                                                                                                                                                                                                                                                                                                                                                                   we, also, discuss, practical, considfast, iterations, and, testing, many, ideas, can, reap, the, most, erations, to, repeated, measures, design, with, variants, to, the, rewards, crossover, design, to, study, the, carry, over, effect, including, the, re, randomized, design, row, 5, in, table, 1\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            to, facilitate, our, illustration, in, all, the, derivation, repeated, measures, design, in, different, stages, of, treatment, in, this, section, we, assume, all, users, appear, in, all, periods, assignment\n4                                                                                                                                                                                                                                                                                                                                                                                                                                              we, also, restrict, ourselves, ing, the, repeated, measures, analysis, reporting, a, per, week, to, metrics, that, are, defined, as, simple, average, and, assume, treatment, effect, as, show, in, row, 3, parallel, design, in, ta, treatment, and, control, have, the, same, sample, size\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    this, way, various, in, fact, the, crossover, design, is, a, type, of, repeated, measures, designs, considered, can, be, examined, in, the, same, framework, design, commonly, used, in, biomedical, research, to, control, for, and, easily, compared\n6                                             flexible, and, scalable, repeated, one, way, to, see, measurements, are, not, missing, at, random, is, measures, analysis, via, forme, to, realize, infrequent, users, are, more, likely, to, have, missing, 5.1, review, of, existing, methods, values, and, the, absence, in, a, specific, time, window, can, still, it, is, common, to, analyze, data, from, repeated, measures, design, provide, information, on, the, user, behavior, and, in, reality, there, with, the, repeated, measures, anova, model, and, the, f, test, might, be, other, factors, causing, user, to, be, missing, that, are, under, certain, assumptions, such, as, normality, sphericity, honot, even, observed\n7 p, p, in, our, cases, they, are, indicators, of, treatment, assignment, k, xik, pk0, xi, k, 0, 0, cov, xi, xi, cov, 0, p, periods, of, the, measurement, user, id, and, any, other, covariate, k, iik, k, 0, ii, k, 0, 0, as, an, example, one, possible, model, for, repeated, measures, xi, xi0, using, lme4, s, formula, syntax, bates, et, al, 2012a, b, is, cov, ii, ii0, y, 1, ist, reatment, p, eriod, 1, u, serid, where, the, last, equality, is, by, dividing, both, numerator, and, de, where, the, only, difference, of, this, model, to, the, usual, linnominator, by, the, same, total, number, of, users, who, have, ever, ear, model, behind, two, sample, test, is, the, extra, random, efappeared, in, the, experiments\n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     in, repeated, measures, data, users, might, appear, in, book, treatment, of, the, delta, method, multiple, periods, represented, as, multiple, rows, in, the, dataset\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                               practical, considerations, ryover, effect, the, re, randomized, design, enables, us, to, at, the, design, stage, we, face, a, few, choices, under, the, same, measure, it, directly, and, should, be, used, here, framework, of, repeated, measures, design\n\nhead(key_res$line_text, n = 2)\n\n[[1]]\n[1] \"Not introduce more sophisticated experimental designs, specifi-           only would we miss potentially beneficial effects, we may also cally the repeated measures design, including the crossover           get false confidence about lack of negative effects. \"\n\n[[2]]\n[1] \"We also discuss practical considfast iterations and testing many ideas can reap the most           erations to repeated measures design, with variants to the rewards.                                                           crossover design to study the carry over effect, including the “re-randomized” design (row 5 in table 1). \"\n\n\nThe output includes the keyword, the page number it is located, the line number the keyword was found, and the line of text. By default, only the line matching the keyword is returned. If the context of the result is desired, there is an optional argument surround_lines that can include the lines around the line of the matching keyword.\n\nkey_res &lt;- keyword_search(file, \n                          keyword = c('repeated measures', 'mixed effects'),\n                          path = TRUE, \n                          surround_lines = 2)\nhead(key_res$line_text, n = 2)\n\n[[1]]\n[1] \"This limits the number of candidate variations              is, we wish to be able to detect the effect when there is any. to be evaluated, and the speed new feature iterations. \"                                                                                  \n[2] \"We             Running under powered experiments have many perils. \"                                                                                                                                                                                                 \n[3] \"Not introduce more sophisticated experimental designs, specifi-           only would we miss potentially beneficial effects, we may also cally the repeated measures design, including the crossover           get false confidence about lack of negative effects. \"\n[4] \"Statistical design and related variants, to increase KPI sensitivity with         power increases with larger effect size, and smaller variances. the same traffic size and duration of experiment. \"                                                                \n[5] \"In this pa-         Let us look at these aspects in turn. per we present FORME (Flexible Online Repeated Measures Experiment), a flexible and scalable framework for these de-          While the actual effect size from a potential new feature may signs. \"       \n\n[[2]]\n[1] \"This poses\"                                                                                                                                                                                                                                                                                                                                 \n[2] \"a limitation to any online experimentation platform, where         within-subject variation. \"                                                                                                                                                                                                                                              \n[3] \"We also discuss practical considfast iterations and testing many ideas can reap the most           erations to repeated measures design, with variants to the rewards.                                                           crossover design to study the carry over effect, including the “re-randomized” design (row 5 in table 1). \"\n[4] \"1.1    Motivation To improve sensitivity of measurement, apart from accurate         1.2     Main Contributions implementation and increase sample size and duration, we           In this paper, we propose a framework called FORME (Flexcan employ statistical methods to reduce variance. \"                                             \n[5] \"Using           ible Online Repeated Measures Experiment). \""
  },
  {
    "objectID": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#directory-search",
    "href": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#directory-search",
    "title": "Introduction of the pdfsearch package",
    "section": "Directory Search",
    "text": "Directory Search\nThis package also has the ability to loop over a directory of pdf files in a single run. To do this, the keyword_directory function is of interest. Much of the arguments are the same, except a directory is specified instead of a single path to the location of the pdf files.\n\n# find directory\ndirectory &lt;- system.file('pdf', package = 'pdfsearch')\n\n# do search over two files\nhead(keyword_directory(directory, \n       keyword = c('repeated measures', 'measurement error'),\n       surround_lines = 1, full_names = TRUE), n = 12)\n\n   ID       pdf_name           keyword page_num line_num\n1   1 1501.00450.pdf repeated measures        1        9\n2   1 1501.00450.pdf repeated measures        2       31\n3   1 1501.00450.pdf repeated measures        2       58\n4   1 1501.00450.pdf repeated measures        2       60\n5   1 1501.00450.pdf repeated measures        3       70\n6   1 1501.00450.pdf repeated measures        6      169\n7   1 1501.00450.pdf repeated measures        6      180\n8   1 1501.00450.pdf repeated measures        6      185\n9   1 1501.00450.pdf repeated measures        9      315\n10  2 1610.00147.pdf measurement error        1        2\n11  2 1610.00147.pdf measurement error        1       10\n12  2 1610.00147.pdf measurement error        1       12\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     line_text\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                              We             Running under powered experiments have many perils. , Not introduce more sophisticated experimental designs, specifi-           only would we miss potentially beneficial effects, we may also cally the repeated measures design, including the crossover           get false confidence about lack of negative effects. , Statistical design and related variants, to increase KPI sensitivity with         power increases with larger effect size, and smaller variances. the same traffic size and duration of experiment. \n2                                                                                                                                                                                                                                                                   a limitation to any online experimentation platform, where         within-subject variation. , We also discuss practical considfast iterations and testing many ideas can reap the most           erations to repeated measures design, with variants to the rewards.                                                           crossover design to study the carry over effect, including the “re-randomized” design (row 5 in table 1). , 1.1    Motivation To improve sensitivity of measurement, apart from accurate         1.2     Main Contributions implementation and increase sample size and duration, we           In this paper, we propose a framework called FORME (Flexcan employ statistical methods to reduce variance. \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                        In the Table 1: Repeated Measures Designs                        following section we assume the minimum experimentation “period” to be one full week, and may extend to up to two In this paper we extend the idea further by employing the          weeks. , To facilitate our illustration, in all the derivation repeated measures design in different stages of treatment          in this section we assume all users appear in all periods, assignment. , The traditional A/B test can be analyzed us-           i.e. no missing measurement. \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The traditional A/B test can be analyzed us-           i.e. no missing measurement. , We also restrict ourselves ing the repeated measures analysis, reporting a “per week”         to metrics that are defined as simple average and assume treatment effect, as show in row 3 “parallel” design in ta-        treatment and control have the same sample size. , We furble 1. \n5                                                                                                                                                                                                                                                                                                This way              average treatment effect (ATE) δ = µT − µC which is a each user serves as his/her own control in the measurement.        fixed effects in the model in this section. , This way, various In fact, the crossover design is a type of repeated measures       designs considered can be examined in the same framework design commonly used in biomedical research to control for         and easily compared., We will proceed to show, with theoretical derivations, that        2.1    Two Sample T-test given the same total traffic                                       Let X denote the observed average metric value in control group and Y denote that in the treatment group. \n6             5.  , FLEXIBLE AND SCALABLE REPEATED One way to see measurements are not missing at random is                MEASURES ANALYSIS VIA FORME to realize infrequent users are more likely to have missing         5.1 Review of Existing Methods values and the absence in a specific time window can still          It is common to analyze data from repeated measures design provide information on the user behavior and in reality there       with the repeated measures ANOVA model and the F-test, might be other factors causing user to be missing that are          under certain assumptions, such as normality, sphericity (honot even observed. , Instead of throwing away data points             mogeneity of variances in differences between each pair of where user appeared in only one period and is exposed to            within-subject values), equal time points between subjects, only one of the two treatments, in practice, we included an         and no missing data. \n7  X and Z are covariates in the model. , \\022P            P            \\023          In our cases they are indicators of treatment assignment, k Xik Pk0 Xi k 0 0 Cov(Xi , Xi ) = Cov 0          P        ,                         periods of the measurement, user id, and any other covariate. k Iik     k 0 Ii k 0 0 \\022           \\023                         As an example, one possible model for repeated measures Xi Xi0                              using lme4’s formula syntax (Bates et al. 2012a;b) is = Cov       , Ii Ii0                                   Y ∼ 1 + IsT reatment + P eriod + (1|U serID), where the last equality is by dividing both numerator and de-       where the only difference of this model to the usual linnominator by the same total number of users who have ever           ear model behind two sample test is the extra random efappeared in the experiments. , Thanks to the central limit            fect(clustered by UserID) to model user “baseline”. \n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (2013, Appendix B) for            Random effect makes modeling within-subject variability a similar example; also see (Van der Vaart 2000) for a text         possible. , In repeated measures data, users might appear in book treatment of the delta-method.                                 multiple periods, represented as multiple rows in the dataset. , As a result, rows of the dataset are not independent but 4.2    Metrics Beyond Average                                       with dependencies clustered by user. \n9                                                                                                                                                                                                                                                                                                                                                                                                                                           • Re-randomized: If we suspect the presence of car7. , PRACTICAL CONSIDERATIONS                                          ryover effect, the re-randomized design enables us to At the design stage, we face a few choices under the same               measure it directly and should be used here. framework of repeated measures design. , Experimenters should           • Wash-out and decide: If we have little informause domain knowledge and past experiments to inform the                 tion to judge carry over effect, we can run the first design. \n10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Data Fusion for Correcting Measurement Errors Tracy Schifeling, Jerome P. , Reiter, Maria DeYoreo∗ arXiv:1610.00147v1 [stat.ME] 1 Oct 2016 Abstract Often in surveys, key items are subject to measurement errors. , Given just the data, it can be difficult to determine the distribution of this error process, and hence to obtain accurate inferences that involve the error-prone variables. \n11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In doing so, we account for the informative sampling design used to select the National Survey of College Graduates. , We also present a process for assessing the sensitivity of various analyses to different choices for the measurement error models. , Supplemental material is available online. \n12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Supplemental material is available online. , KEY WORDS: fusion, imputation, measurement error, missing, survey. , ∗ This research was supported by The National Science Foundation under award SES-11-31897. \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          token_text\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        we, running, under, powered, experiments, have, many, perils, not, introduce, more, sophisticated, experimental, designs, specifi, only, would, we, miss, potentially, beneficial, effects, we, may, also, cally, the, repeated, measures, design, including, the, crossover, get, false, confidence, about, lack, of, negative, effects, statistical, design, and, related, variants, to, increase, kpi, sensitivity, with, power, increases, with, larger, effect, size, and, smaller, variances, the, same, traffic, size, and, duration, of, experiment\n2                                                                                                                                                                                                                                                                                                                                         a, limitation, to, any, online, experimentation, platform, where, within, subject, variation, we, also, discuss, practical, considfast, iterations, and, testing, many, ideas, can, reap, the, most, erations, to, repeated, measures, design, with, variants, to, the, rewards, crossover, design, to, study, the, carry, over, effect, including, the, re, randomized, design, row, 5, in, table, 1, 1.1, motivation, to, improve, sensitivity, of, measurement, apart, from, accurate, 1.2, main, contributions, implementation, and, increase, sample, size, and, duration, we, in, this, paper, we, propose, a, framework, called, forme, flexcan, employ, statistical, methods, to, reduce, variance\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  in, the, table, 1, repeated, measures, designs, following, section, we, assume, the, minimum, experimentation, period, to, be, one, full, week, and, may, extend, to, up, to, two, in, this, paper, we, extend, the, idea, further, by, employing, the, weeks, to, facilitate, our, illustration, in, all, the, derivation, repeated, measures, design, in, different, stages, of, treatment, in, this, section, we, assume, all, users, appear, in, all, periods, assignment, the, traditional, a, b, test, can, be, analyzed, us, i.e, no, missing, measurement\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    the, traditional, a, b, test, can, be, analyzed, us, i.e, no, missing, measurement, we, also, restrict, ourselves, ing, the, repeated, measures, analysis, reporting, a, per, week, to, metrics, that, are, defined, as, simple, average, and, assume, treatment, effect, as, show, in, row, 3, parallel, design, in, ta, treatment, and, control, have, the, same, sample, size, we, furble, 1\n5                                                                                                                                                                                                                                                                                                                                   this, way, average, treatment, effect, ate, δ, µt, µc, which, is, a, each, user, serves, as, his, her, own, control, in, the, measurement, fixed, effects, in, the, model, in, this, section, this, way, various, in, fact, the, crossover, design, is, a, type, of, repeated, measures, designs, considered, can, be, examined, in, the, same, framework, design, commonly, used, in, biomedical, research, to, control, for, and, easily, compared, we, will, proceed, to, show, with, theoretical, derivations, that, 2.1, two, sample, t, test, given, the, same, total, traffic, let, x, denote, the, observed, average, metric, value, in, control, group, and, y, denote, that, in, the, treatment, group\n6  5, flexible, and, scalable, repeated, one, way, to, see, measurements, are, not, missing, at, random, is, measures, analysis, via, forme, to, realize, infrequent, users, are, more, likely, to, have, missing, 5.1, review, of, existing, methods, values, and, the, absence, in, a, specific, time, window, can, still, it, is, common, to, analyze, data, from, repeated, measures, design, provide, information, on, the, user, behavior, and, in, reality, there, with, the, repeated, measures, anova, model, and, the, f, test, might, be, other, factors, causing, user, to, be, missing, that, are, under, certain, assumptions, such, as, normality, sphericity, honot, even, observed, instead, of, throwing, away, data, points, mogeneity, of, variances, in, differences, between, each, pair, of, where, user, appeared, in, only, one, period, and, is, exposed, to, within, subject, values, equal, time, points, between, subjects, only, one, of, the, two, treatments, in, practice, we, included, an, and, no, missing, data\n7                                                                                                                                                                     x, and, z, are, covariates, in, the, model, p, p, in, our, cases, they, are, indicators, of, treatment, assignment, k, xik, pk0, xi, k, 0, 0, cov, xi, xi, cov, 0, p, periods, of, the, measurement, user, id, and, any, other, covariate, k, iik, k, 0, ii, k, 0, 0, as, an, example, one, possible, model, for, repeated, measures, xi, xi0, using, lme4, s, formula, syntax, bates, et, al, 2012a, b, is, cov, ii, ii0, y, 1, ist, reatment, p, eriod, 1, u, serid, where, the, last, equality, is, by, dividing, both, numerator, and, de, where, the, only, difference, of, this, model, to, the, usual, linnominator, by, the, same, total, number, of, users, who, have, ever, ear, model, behind, two, sample, test, is, the, extra, random, efappeared, in, the, experiments, thanks, to, the, central, limit, fect, clustered, by, userid, to, model, user, baseline\n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2013, appendix, b, for, random, effect, makes, modeling, within, subject, variability, a, similar, example, also, see, van, der, vaart, 2000, for, a, text, possible, in, repeated, measures, data, users, might, appear, in, book, treatment, of, the, delta, method, multiple, periods, represented, as, multiple, rows, in, the, dataset, as, a, result, rows, of, the, dataset, are, not, independent, but, 4.2, metrics, beyond, average, with, dependencies, clustered, by, user\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         re, randomized, if, we, suspect, the, presence, of, car7, practical, considerations, ryover, effect, the, re, randomized, design, enables, us, to, at, the, design, stage, we, face, a, few, choices, under, the, same, measure, it, directly, and, should, be, used, here, framework, of, repeated, measures, design, experimenters, should, wash, out, and, decide, if, we, have, little, informause, domain, knowledge, and, past, experiments, to, inform, the, tion, to, judge, carry, over, effect, we, can, run, the, first, design\n10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      data, fusion, for, correcting, measurement, errors, tracy, schifeling, jerome, p, reiter, maria, deyoreo, arxiv, 1610.00147v1, stat.me, 1, oct, 2016, abstract, often, in, surveys, key, items, are, subject, to, measurement, errors, given, just, the, data, it, can, be, difficult, to, determine, the, distribution, of, this, error, process, and, hence, to, obtain, accurate, inferences, that, involve, the, error, prone, variables\n11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         in, doing, so, we, account, for, the, informative, sampling, design, used, to, select, the, national, survey, of, college, graduates, we, also, present, a, process, for, assessing, the, sensitivity, of, various, analyses, to, different, choices, for, the, measurement, error, models, supplemental, material, is, available, online\n12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          supplemental, material, is, available, online, key, words, fusion, imputation, measurement, error, missing, survey, this, research, was, supported, by, the, national, science, foundation, under, award, ses, 11, 31897\n\n\nTwo relavent arguments for the keyword_directory function are full_names and recursive. These functions ask whether the full path for the pdf files in the directory will be used and whether subfolders within the directory will also be searched."
  },
  {
    "objectID": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#uses-for-pdfsearch",
    "href": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#uses-for-pdfsearch",
    "title": "Introduction of the pdfsearch package",
    "section": "Uses for pdfsearch",
    "text": "Uses for pdfsearch\nThis package may be extremely useful when conducting research syntheses or meta analyses, particularly when screening articles for inclusion into the research synthesis or meta analysis. This aim is hopeful to be explored later in more depth.\n\nLimitations\nThe limitations of the package and the quality of text matches will depend on the pdfs being searched. For example, words that wrap across lines (i.e. hyphenated words) will not be included in the matches as entire words are currently being searched to be matched."
  },
  {
    "objectID": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#moving-forward",
    "href": "posts/2016-12-02-introduction-of-the-pdfsearch-package.html#moving-forward",
    "title": "Introduction of the pdfsearch package",
    "section": "Moving Forward",
    "text": "Moving Forward\nThe package will be submitted to CRAN next week, however, any bugs or problems can be submitted to the github site https://github.com/lebebr01/pdfsearch/issues."
  },
  {
    "objectID": "posts/simglm-jsm.html",
    "href": "posts/simglm-jsm.html",
    "title": "Announcing simglm version 0.6.0: Power and Simulation of Generalized Linear Mixed Models",
    "section": "",
    "text": "The simglm package has an update on CRAN bumping the version up to 0.6.0. This update has added the ability to simulate count data (poisson) and also has fixed (I think) the Shiny app that comes with the package. As I have not posted about this package since the first CRAN release (v 0.5.0), I plan to give an overview of all that the package offers in addition to the new additions."
  },
  {
    "objectID": "posts/simglm-jsm.html#installation",
    "href": "posts/simglm-jsm.html#installation",
    "title": "Announcing simglm version 0.6.0: Power and Simulation of Generalized Linear Mixed Models",
    "section": "Installation",
    "text": "Installation\nThe package can be installed directly from CRAN:\n\ninstall.packages(\"simglm\")"
  },
  {
    "objectID": "posts/simglm-jsm.html#simglm-features",
    "href": "posts/simglm-jsm.html#simglm-features",
    "title": "Announcing simglm version 0.6.0: Power and Simulation of Generalized Linear Mixed Models",
    "section": "simglm Features",
    "text": "simglm Features\nThe package allows for flexible simulation of general(-ized) linear (mixed) models with up to three levels of nesting. The flexibility comes from the ability to easily vary data generation procedures. For example one can use any R data generating function to simulate error, random effect, or covariate distributions. Other data generating options can include:\n\nheterogeneity of variance\nmissing data\nARIMA models for within error structure\nflexible time specification for longitudinal models\nunbalanced data generation for nested designs\ncorrelated covariates\nfactor/categorical variable generation\n\nThe primary functions for data generation within the package are sim_reg and sim_glm. The main distinction being that sim_reg is used for continuous outcomes whereas sim_glm is used for non-continuous outcomes. Currently sim_glm supports dichotomous outcomes (ie., 0/1; logistic) and count outcomes (poisson distributed).\nBelow are two examples using the sim_reg and sim_glm to generate a simple two level cross sectional model.\n\nfixed &lt;- ~ 1 + act_o + income + grad_degree_f\nfixed_param &lt;- c(3.2, 0.4, 0.02, 0.6)\nrandom &lt;- ~ 1 \nrandom_param &lt;- list(random_var = 10, rand_gen = 'rnorm')\ncov_param &lt;- list(dist_fun = 'rnorm',\n                  var_type = 'level1')\nn &lt;- 50\np &lt;- 5\nerror_var &lt;- 5\nwith_err_gen &lt;- 'rnorm'\ndata_str &lt;- 'cross'\nfact_vars &lt;- list(numlevels = c(36, 2), \n                  var_type = c('level1', 'level1'))\n\nsim_reg(fixed = fixed, fixed_param = fixed_param, random = random,\n        random_param = random_param, cov_param = cov_param, k = NULL, n = n, p = p,\n        error_var = error_var, with_err_gen = with_err_gen, \n        data_str = data_str, fact_vars = fact_vars)\n\nSimilar values are used for the sim_glm function, however the within error is not specified.\n\nfixed &lt;- ~ 1 + act_o + income + grad_degree_f\nfixed_param &lt;- c(0.5, 0.4, 0.02, 0.6)\nrandom &lt;- ~ 1 \nrandom_param &lt;- list(random_var = 10, rand_gen = 'rnorm')\ncov_param &lt;- list(dist_fun = 'rnorm',\n                  var_type = 'level1')\nn &lt;- 50\np &lt;- 5\ndata_str &lt;- 'cross'\nfact_vars &lt;- list(numlevels = c(36, 2), \n                  var_type = c('level1', 'level1'))\n\nsim_glm(fixed = fixed, fixed_param = fixed_param, random = random,\n        random_param = random_param, cov_param = cov_param, k = NULL, n = n, p = p,\n        data_str = data_str, fact_vars = fact_vars, outcome_type = 'logistic')\n\nYou could easily simulate a count outcome by changing outcome_type = 'logistic' in the last example to outcome_type = 'poisson'."
  },
  {
    "objectID": "posts/simglm-jsm.html#power",
    "href": "posts/simglm-jsm.html#power",
    "title": "Announcing simglm version 0.6.0: Power and Simulation of Generalized Linear Mixed Models",
    "section": "Power",
    "text": "Power\nThe package also includes wrappers around the simulation functions to explore empirical power under these frameworks. For example for a simple regression:\n\nfixed &lt;- ~ 1 + act + diff + numCourse + act:numCourse\nfixed_param &lt;- c(0.5, 1.1, 0.6, 0.9, 1.1)\ncov_param &lt;- list(dist_fun = c('rnorm', 'rnorm', 'rnorm'),\n                  var_type = c(\"single\", \"single\", \"single\"),\n                  opts = list(list(mean = 0, sd = 2),\n                              list(mean = 0, sd = 2),\n                              list(mean = 0, sd = 1)))\nn &lt;- 150\nerror_var &lt;- 20\nwith_err_gen &lt;- 'rnorm'\n\npow_param &lt;- c('(Intercept)', 'act', 'diff', 'numCourse')\nalpha &lt;- .01\npow_dist &lt;- \"t\"\npow_tail &lt;- 2\nreplicates &lt;- 50\n\npower_out &lt;- sim_pow(fixed = fixed, fixed_param = fixed_param, cov_param = cov_param,\n                     n = n, error_var = error_var, with_err_gen = with_err_gen, \n                     data_str = \"single\", pow_param = pow_param, alpha = alpha,\n                     pow_dist = pow_dist, pow_tail = pow_tail, \n                     replicates = replicates, raw_power = FALSE)\npower_out\n\nWhere now the simulation function sim_reg replicates 50 times and the estimates are used to generate empirical power for each parameter in the model. This idea becomes more powerful when a Monte Carlo like varying of parameters are used. This can ensure a much larger, explorative, and descriptive power analysis. These analyses if done well with the coverage of the parameters varied, may be more appropriate/realistic based on the data to be collected (or more similar to the data that was collected for post-hoc power analyses). Below is a simple example of varying parameters in the power analysis.\n\nfixed &lt;- ~ 1 + act + diff + numCourse + act:numCourse\nfixed_param &lt;- c(0.5, 1.1, 0.6, 0.9, 1.1)\ncov_param &lt;- list(dist_fun = c('rnorm', 'rnorm', 'rnorm'),\n                  var_type = c(\"single\", \"single\", \"single\"),\n                  opts = list(list(mean = 0, sd = 2),\n                              list(mean = 0, sd = 2),\n                              list(mean = 0, sd = 1)))\nn &lt;- NULL\nerror_var &lt;- NULL\nwith_err_gen &lt;- 'rnorm'\npow_param &lt;- c('(Intercept)', 'act', 'diff', 'numCourse')\nalpha &lt;- .01\npow_dist &lt;- \"t\"\n\npow_tail &lt;- 2\nreplicates &lt;- 50\nterms_vary &lt;- list(n = c(20, 40, 60, 80, 100), \n                   error_var = c(5, 10, 20))\n\npower_out &lt;- sim_pow(fixed = fixed, fixed_param = fixed_param, cov_param = cov_param,\n                     n = n, error_var = error_var, with_err_gen = with_err_gen, \n                     data_str = \"single\", pow_param = pow_param, alpha = alpha,\n                     pow_dist = pow_dist, pow_tail = pow_tail, \n                     replicates = replicates, terms_vary = terms_vary, \n                     raw_power = FALSE)\n\npower_out\n\nSimilar syntax can be used for nested designs as well as with generalized models. These models and data types take more time to run compared to the single level designs above. Therefore, it is recommended to perform small simulations initially to ensure no errors and estimate how much time the simulation may take."
  },
  {
    "objectID": "posts/simglm-jsm.html#shiny-app",
    "href": "posts/simglm-jsm.html#shiny-app",
    "title": "Announcing simglm version 0.6.0: Power and Simulation of Generalized Linear Mixed Models",
    "section": "Shiny App",
    "text": "Shiny App\nThe package also comes with a Shiny app that contains most (but not all) of the features found in the full package. These will likely be implemented slowly over time. You can run the Shiny app locally with the following function:\n\nsimglm::run_shiny()\n\nOne note, there are errors that occur in the stack trace, however as far as I can tell in my testing the app runs okay.\n\nJSM\nOf note to anyone attending JSM this year, I am talking about the simglm package https://ww2.amstat.org/meetings/jsm/2017/onlineprogram/ActivityDetails.cfm?SessionID=214411. Slides of this talk will be posted likely after the talk on my website.\nBugs and feature requests are welcomed and can be submitted directly to GitHub https://github.com/lebebr01/simglm/issues."
  },
  {
    "objectID": "posts/2025-10-07-euroleague-tt.html",
    "href": "posts/2025-10-07-euroleague-tt.html",
    "title": "My First Tidy Tuesday!",
    "section": "",
    "text": "I’ve always wanted to do Tidy Tuesday, but never made time to spend on it. I liked to recommend the data to students as a way to explore data or practice creating effective visualizations. Today, I decided to dive in to create something. This week, the data was on EuroLeague Basketball.\nThe data had information on the 20 teams, the city and country they are based out of, the size of their stadium, and the number of times they have won the EuroLeague Basketball title. I initially thought about explore something related to the size of the stadium and the number of times they won the League title, but I was struck by how varied the locations were. This led me to want to visualize how far apart the different teams were.\nI’m sure there is a way to do this in R or Python, but I ultimately asked Claude to help me identify the latitude and longitude for each team’s home city (note, some have the same city, so will have the same location). This way I could focus on creating and honing the visualization. I was struck a few years ago by the plane arc visualization that was extremely popular. I had never created a figure like this before, so I set off to create something like this."
  },
  {
    "objectID": "posts/2025-10-07-euroleague-tt.html#packages-used",
    "href": "posts/2025-10-07-euroleague-tt.html#packages-used",
    "title": "My First Tidy Tuesday!",
    "section": "Packages Used",
    "text": "Packages Used\nHere are the packages used for the following visualizations / interactive features. I first create a static image as a proof of concept, then I converted the static image to appropriate leaflet code (duplicating some processes along the way) to create interactive versions. At the very end, I use DT to create a summary table for each team and their average (and minimum and maximum) distance they need to travel.\n\n\nCode\n# Load required libraries\nlibrary(tidyverse)\nlibrary(geosphere)\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(viridis)\nlibrary(leaflet)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(crosstalk)\nlibrary(DT)\nlibrary(shiny)"
  },
  {
    "objectID": "posts/2025-10-07-euroleague-tt.html#static-map-with-madrid-as-reference",
    "href": "posts/2025-10-07-euroleague-tt.html#static-map-with-madrid-as-reference",
    "title": "My First Tidy Tuesday!",
    "section": "Static map with Madrid as reference",
    "text": "Static map with Madrid as reference\nFirst, I used Madrid as the reference city to visualize how far each city is from Madrid. I picked Madrid as it was on one extreme, the most western city in the group. Madrid has many cities that are closer, but some that would be a long flight.\n\n\nCode\n# Create the dataset with team locations\nteams &lt;- tibble(\n  Team = c(\"Anadolu Efes\", \"Barcelona\", \"Baskonia\", \"Bayern Munich\", \n           \"Crvena zvezda\", \"Dubai Basketball\", \"Fenerbahce\", \"Hapoel Tel Aviv\",\n           \"LDLC ASVEL\", \"Maccabi Tel Aviv\", \"Monaco\", \"Olimpia Milano\",\n           \"Olympiacos\", \"Panathinaikos\", \"Paris Basketball\", \"Partizan\",\n           \"Real Madrid\", \"Valencia Basket\", \"Virtus Bologna\", \"Zalgiris\"),\n  City = c(\"Istanbul\", \"Barcelona\", \"Vitoria-Gasteiz\", \"Munich\", \n           \"Belgrade\", \"Dubai\", \"Istanbul\", \"Tel Aviv\",\n           \"Villeurbanne\", \"Tel Aviv\", \"Monaco\", \"Milan\",\n           \"Piraeus\", \"Athens\", \"Paris\", \"Belgrade\",\n           \"Madrid\", \"Valencia\", \"Bologna\", \"Kaunas\"),\n  Country = c(\"Turkey\", \"Spain\", \"Spain\", \"Germany\", \n              \"Serbia\", \"UAE\", \"Turkey\", \"Israel\",\n              \"France\", \"Israel\", \"Monaco\", \"Italy\",\n              \"Greece\", \"Greece\", \"France\", \"Serbia\",\n              \"Spain\", \"Spain\", \"Italy\", \"Lithuania\"),\n  Latitude = c(41.0082, 41.3851, 42.8467, 48.1351, \n               44.7866, 25.2048, 41.0082, 32.0853,\n               45.7660, 32.0853, 43.7384, 45.4642,\n               37.9478, 37.9838, 48.8566, 44.7866,\n               40.4168, 39.4699, 44.4949, 54.8985),\n  Longitude = c(28.9784, 2.1734, -2.6716, 11.5820, \n                20.4489, 55.2708, 28.9784, 34.7818,\n                4.8795, 34.7818, 7.4246, 9.1900,\n                23.6473, 23.7275, 2.3522, 20.4489,\n                -3.7038, -0.3763, 11.3426, 23.9036)\n)\n\n# Remove duplicate cities for cleaner visualization\nteams_unique &lt;- teams |&gt;\n  group_by(City) |&gt;\n  slice(1) |&gt;\n  ungroup()\n\n# Function to calculate distance\ncalc_distance &lt;- function(lat1, lon1, lat2, lon2) {\n  # Haversine formula for great circle distance\n  R &lt;- 6371 # Earth's radius in km\n  \n  lat1_rad &lt;- lat1 * pi / 180\n  lat2_rad &lt;- lat2 * pi / 180\n  delta_lat &lt;- (lat2 - lat1) * pi / 180\n  delta_lon &lt;- (lon2 - lon1) * pi / 180\n  \n  a &lt;- sin(delta_lat/2)^2 + cos(lat1_rad) * cos(lat2_rad) * sin(delta_lon/2)^2\n  c &lt;- 2 * atan2(sqrt(a), sqrt(1-a))\n  \n  R * c\n}\n\n# Create distance matrix\ndistance_data &lt;- expand.grid(\n  from_city = teams_unique$City,\n  to_city = teams_unique$City,\n  stringsAsFactors = FALSE\n) |&gt;\n  filter(from_city != to_city) |&gt;\n  left_join(teams_unique |&gt; select(City, from_lat = Latitude, from_lon = Longitude), \n            by = c(\"from_city\" = \"City\")) |&gt;\n  left_join(teams_unique |&gt; select(City, to_lat = Latitude, to_lon = Longitude), \n            by = c(\"to_city\" = \"City\")) |&gt;\n  mutate(distance = calc_distance(from_lat, from_lon, to_lat, to_lon)) |&gt;\n  arrange(from_city, distance)\n\n\n\n\nCode\n# Function to create curved paths between points\ncreate_arc &lt;- function(lon1, lat1, lon2, lat2, n = 50) {\n  # Calculate great circle path\n  gc &lt;- gcIntermediate(c(lon1, lat1), c(lon2, lat2), n = n, addStartEnd = TRUE)\n  \n  if (is.matrix(gc)) {\n    return(data.frame(lon = gc[, 1], lat = gc[, 2]))\n  } else {\n    return(data.frame(lon = c(lon1, lon2), lat = c(lat1, lat2)))\n  }\n}\n\n# Create connections between all pairs of cities\nconnections &lt;- list()\nidx &lt;- 1\n\nn_cities &lt;- nrow(teams_unique)\nfor (i in 1:(n_cities - 1)) {\n  for (j in (i + 1):n_cities) {\n    arc_data &lt;- create_arc(\n      teams_unique$Longitude[i], teams_unique$Latitude[i],\n      teams_unique$Longitude[j], teams_unique$Latitude[j]\n    )\n    arc_data$group &lt;- idx\n    arc_data$from &lt;- teams_unique$City[i]\n    arc_data$to &lt;- teams_unique$City[j]\n    \n    # Calculate distance\n    distance &lt;- distHaversine(\n      c(teams_unique$Longitude[i], teams_unique$Latitude[i]),\n      c(teams_unique$Longitude[j], teams_unique$Latitude[j])\n    ) / 1000\n    arc_data$distance &lt;- distance\n    \n    connections[[idx]] &lt;- arc_data\n    idx &lt;- idx + 1\n  }\n}\n\n# Combine all connections\nall_connections &lt;- bind_rows(connections)\n\n# Get world map data\nworld_map &lt;- map_data(\"world\")\n\n# Filter to relevant region\neurope_map &lt;- world_map |&gt;\n  filter(long &gt;= -15 & long &lt;= 60 & lat &gt;= 20 & lat &lt;= 65)\n\n# Alternative version: Show only connections from one city (e.g., Madrid)\nreference_city &lt;- \"Madrid\"\n\nconnections_from_madrid &lt;- all_connections |&gt;\n  filter(from == reference_city | to == reference_city)\n\nggplot() +\n  geom_polygon(data = europe_map, \n               aes(x = long, y = lat, group = group),\n               fill = \"gray95\", color = \"gray70\", linewidth = 0.2) +\n  \n  geom_path(data = connections_from_madrid,\n            aes(x = lon, y = lat, group = group, color = distance),\n            alpha = 0.6, linewidth = 0.8) +\n  \n  geom_point(data = teams_unique,\n             aes(x = Longitude, y = Latitude),\n             size = 4, color = \"#2C3E50\", shape = 21, \n             fill = \"#3498DB\", stroke = 1.5) +\n  \n  geom_point(data = teams_unique |&gt; filter(City == reference_city),\n             aes(x = Longitude, y = Latitude),\n             size = 6, color = \"#2C3E50\", shape = 21, \n             fill = \"#E74C3C\", stroke = 2) +\n  \n  geom_text(data = teams_unique,\n            aes(x = Longitude, y = Latitude, label = City),\n            size = 3, hjust = -0.2, vjust = -0.5, \n            fontface = \"bold\", color = \"#2C3E50\") +\n  \n  scale_color_viridis(option = \"plasma\", name = \"Distance\\n(km)\", \n                      direction = -1) +\n  coord_fixed(ratio = 1.3, xlim = c(-10, 58), ylim = c(23, 60)) +\n  labs(title = sprintf(\"Travel Distances from %s\", reference_city),\n       subtitle = \"Great circle routes to all other EuroLeague cities\",\n       x = \"Longitude\", y = \"Latitude\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5),\n    panel.grid.major = element_line(color = \"gray80\", linewidth = 0.3),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )"
  },
  {
    "objectID": "posts/2025-10-07-euroleague-tt.html#interactive-individual-city-maps",
    "href": "posts/2025-10-07-euroleague-tt.html#interactive-individual-city-maps",
    "title": "My First Tidy Tuesday!",
    "section": "Interactive Individual City Maps",
    "text": "Interactive Individual City Maps\nNow, I used leaflet and some embedded JavaScript to create an interactive visualization that let’s you pick the origin city and update the figure appropriately.\n\n\nCode\npal &lt;- colorNumeric(\n  palette = viridisLite::plasma(256),\n  domain = distance_data$distance\n)\n\n\ncreate_city_map &lt;- function(city_name) {\n  # Get city coordinates\n  city_coords &lt;- teams_unique |&gt; filter(City == city_name)\n  \n  # Get connections from this city\n  city_connections &lt;- distance_data |&gt;\n    filter(from_city == city_name)\n  \n  # Create map\n  map &lt;- leaflet() |&gt;\n    addProviderTiles(providers$CartoDB.Positron) |&gt;\n    setView(lng = city_coords$Longitude, \n            lat = city_coords$Latitude, \n            zoom = 4)\n  \n  # Add curved connections using create_arc\n  for (i in 1:nrow(city_connections)) {\n    # Create great circle arc\n    arc &lt;- create_arc(\n      city_connections$from_lon[i],\n      city_connections$from_lat[i],\n      city_connections$to_lon[i],\n      city_connections$to_lat[i]\n    )\n    \n    label_text &lt;- sprintf(\"%s → %s: %.0f km\",\n                         city_connections$from_city[i],\n                         city_connections$to_city[i],\n                         city_connections$distance[i])\n    \n    map &lt;- map |&gt;\n      addPolylines(\n        lng = arc$lon,\n        lat = arc$lat,\n        color = pal(city_connections$distance[i]),\n        weight = 2,\n        opacity = 0.6,\n        label = label_text,\n        labelOptions = labelOptions(\n          style = list(\"font-weight\" = \"normal\", \"padding\" = \"3px 8px\"),\n          textsize = \"12px\",\n          direction = \"auto\"\n        ),\n        highlightOptions = highlightOptions(\n          weight = 4,\n          opacity = 1,\n          bringToFront = TRUE\n        )\n      )\n  }\n  \n  # Add markers\n  map &lt;- map |&gt;\n    addCircleMarkers(\n      data = teams_unique,\n      lng = ~Longitude,\n      lat = ~Latitude,\n      radius = 6,\n      color = \"#2C3E50\",\n      fillColor = ~ifelse(City == city_name, \"#E74C3C\", \"#3498DB\"),\n      fillOpacity = 0.8,\n      weight = 2,\n      label = ~paste0(City, \", \", Country),\n      labelOptions = labelOptions(\n        style = list(\"font-weight\" = \"normal\", \"padding\" = \"3px 8px\"),\n        textsize = \"12px\",\n        direction = \"auto\"\n      )\n    ) |&gt;\n    addLegend(\n      position = \"bottomright\",\n      pal = pal,\n      values = city_connections$distance,\n      title = \"Distance (km)\",\n      opacity = 0.7\n    )\n  \n  return(map)\n}\n\n# Generate maps for all cities\ngenerate_all_city_maps &lt;- function() {\n  city_list &lt;- sort(unique(teams_unique$City))\n  maps_list &lt;- list()\n  \n  for (city in city_list) {\n    map_id &lt;- paste0(\"map_\", gsub(\"[^A-Za-z0-9]\", \"_\", city))\n    maps_list[[city]] &lt;- create_city_map(city)\n    attr(maps_list[[city]], \"map_id\") &lt;- map_id\n  }\n  \n  return(maps_list)\n}\n\ncreate_multi_city_viewer &lt;- function() {\n  city_list &lt;- sort(unique(teams_unique$City))\n  maps_list &lt;- generate_all_city_maps()\n  \n  # --- Dropdown HTML ---\n  dropdown_html &lt;- tags$div(\n    style = \"margin-bottom: 15px;\",\n    tags$label(\n      `for` = \"citySelector\",\n      style = \"font-weight: bold; margin-right: 10px; font-size: 14px;\",\n      \"Select Origin City:\"\n    ),\n    tags$select(\n      id = \"citySelector\",\n      style = \"padding: 5px 10px; font-size: 14px; border-radius: 4px; border: 1px solid #ccc;\",\n      lapply(city_list, function(city) {\n        tags$option(value = gsub(\"[^A-Za-z0-9]\", \"_\", city), city)\n      })\n    )\n  )\n  \n  # --- Map containers (only one visible at a time) ---\n  map_containers &lt;- lapply(seq_along(city_list), function(i) {\n    city &lt;- city_list[i]\n    map_id &lt;- paste0(\"map_\", gsub(\"[^A-Za-z0-9]\", \"_\", city))\n    display_style &lt;- if (i == 1) \"block\" else \"none\"\n    \n    tags$div(\n      id = map_id,\n      class = \"city-map-container\",\n      style = paste0(\"display: \", display_style, \"; width: 100%; height: 600px;\"),\n      maps_list[[city]]\n    )\n  })\n  \n  # --- Robust JavaScript Switcher ---\n  switch_js &lt;- tags$script(HTML(\"\n    document.addEventListener('DOMContentLoaded', function() {\n      const mapRefs = {};\n\n      // Capture leaflet map instances as they render\n      if (window.HTMLWidgets) {\n        const widgets = Object.values(HTMLWidgets.widgets).filter(w =&gt; w.name === 'leaflet');\n        widgets.forEach(w =&gt; {\n          const oldRender = w.renderValue;\n          w.renderValue = function(el, x) {\n            oldRender(el, x);\n            const container = el.closest('.city-map-container');\n            if (container && el._leaflet_map) {\n              mapRefs[container.id] = el._leaflet_map;\n            }\n          };\n        });\n      }\n\n      const selector = document.getElementById('citySelector');\n      if (!selector) return;\n\n      selector.addEventListener('change', function(e) {\n        const selected = 'map_' + e.target.value;\n\n        // Hide all maps\n        document.querySelectorAll('.city-map-container').forEach(div =&gt; {\n          div.style.display = 'none';\n        });\n\n        // Show selected map\n        const active = document.getElementById(selected);\n        if (active) {\n          active.style.display = 'block';\n          \n          // Force reflow after visibility change\n          const map = mapRefs[selected];\n          if (map && map.invalidateSize) {\n            // call invalidateSize twice to ensure full redraw\n            setTimeout(() =&gt; map.invalidateSize(true), 200);\n            setTimeout(() =&gt; {\n              map.invalidateSize(true);\n              map.fire('zoomend'); // triggers tile refresh\n            }, 600);\n          }\n        }\n      });\n    });\n  \"))\n  \n  # --- Combine and return ---\n  browsable(\n    tagList(\n      dropdown_html,\n      map_containers,\n      switch_js\n    )\n  )\n}\n\n\n\nSelect Origin City:\n\nAthens\nBarcelona\nBelgrade\nBologna\nDubai\nIstanbul\nKaunas\nMadrid\nMilan\nMonaco\nMunich\nParis\nPiraeus\nTel Aviv\nValencia\nVilleurbanne\nVitoria-Gasteiz"
  },
  {
    "objectID": "posts/2025-10-07-euroleague-tt.html#summary-statistics",
    "href": "posts/2025-10-07-euroleague-tt.html#summary-statistics",
    "title": "My First Tidy Tuesday!",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nI finally present some descriptive statistics on the average, median, minimum, and maximum distances that teams have to travel. Some of them are very close, but others have significant amounts of travel to contend with. Travel to play games is not only costly, but could make recovery more difficult for the players.\n\n\nCode\n# Average distance by city\navg_by_city &lt;- distance_data |&gt;\n  group_by(from_city) |&gt;\n  summarise(\n    `Average Distance (km)` = mean(distance),\n    `Median Distance (km)` = median(distance),\n    `Max Distance (km)` = max(distance),\n    `Min Distance (km)` = min(distance)\n  ) |&gt;\n  arrange(desc(`Average Distance (km)`)) |&gt; \n  rename(\"City\" = from_city) |&gt;\n  left_join(select(teams, City, Team, Country), by = join_by(City)) |&gt;\n  select(City, Country, Team, everything())\n\navg_by_city |&gt;\n    datatable(\n      options = list(pageLength = 20, dom = 'tip'),\n      rownames = FALSE,\n      caption = \"Descriptive Statistics for each City\"\n    ) %&gt;%\n    formatRound(4:7, 0)"
  },
  {
    "objectID": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html",
    "href": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html",
    "title": "Use CSS to format markdown or HTML files",
    "section": "",
    "text": "Markdown (and Rmarkdown) are great ways to quickly develop material without worrying about the formatting. The documents can then be compiled using the knitr or rmarkdown packages to output formats such as HTML, latex, or even word. The main drawback of this approach is that formatting of documents is limited to italics, bold, or strikethrough. Markdown does have support for inline HTML, therefore you can add your own formatting inline using CSS or other HTML attributes, however this moves away from the quick markdown flavor.\nTo help solve this problem, many R packages are useful for formatting tables, either through conditional formatting or otherwise. The most interesting to me is the formattable package. Other options include the ReporteRs and condformat packages. These packages however focus on table formatting. An option I started working on a few years ago, highlightHTML, is a relatively simple package that will help inject CSS automatically into an HTML document to take care of formatting of text and tables.\nSince this package uses CSS for the formatting, knowledge of CSS is required to create the tags to be injected. This has the advantage of allowing users a lot of flexibility with the look they wish to achieve, however, it will be more difficult for users if they do not know CSS. Below is a short demo of functions of interest."
  },
  {
    "objectID": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#installation",
    "href": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#installation",
    "title": "Use CSS to format markdown or HTML files",
    "section": "Installation",
    "text": "Installation\nThe package was published on CRAN a few days ago and can be installed using install.packages:\n\ninstall.packages('highlightHTML')\n\nTo get the most out of the package, rmarkdown and knitr are useful to have installed as well, although not required."
  },
  {
    "objectID": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#simple-example",
    "href": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#simple-example",
    "title": "Use CSS to format markdown or HTML files",
    "section": "Simple Example",
    "text": "Simple Example\nSuppose you have a table like the following:\n\n\n\nColor Name\nNumber\n\n\n\n\nBlue\n5\n\n\nGreen\n35\n\n\nOrange\n100\n\n\nRed\n200\n\n\n\nYou could then add some conditional formatting by adding the following tags to the table.\n\n\n\nColor Name\nNumber\n\n\n\n\nBlue\n5 #bgblue\n\n\nGreen\n35\n\n\nOrange\n100\n\n\nRed\n200 #bgred\n\n\n\nThe addition of the #bgblue and #bgred indicates which cells will be changed. After turning the markdown document into an html file, this package can now be used to post-process the html file. The post-processing will add an id value for each cell with the #bgblue or #bgred and remove those from the table.\nThe function to use for the post-processing is highlight_html and requires three arguments, the input file, the output file, and the CSS tags themselves. This will look something like the following using an example file from the package:\n\nlibrary(highlightHTML)\nfile &lt;- system.file('examples', 'bgtable.html', \n                    package = 'highlightHTML')\ntags &lt;- c(\"#bgred {background-color: #FF0000;}\", \n  \"#bgblue {background-color: #0000FF;}\")\nhighlight_html(input = file, \n               output = tempfile(fileext = \".html\"), \n               tags = tags,\n               update_css = TRUE, \n               browse = TRUE,\n               print = FALSE)\n\nThis command will return an HTML file that automatically injects the CSS tags shown above. The new HTML file will add background color to the HTML file as such:"
  },
  {
    "objectID": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#formatting-text",
    "href": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#formatting-text",
    "title": "Use CSS to format markdown or HTML files",
    "section": "Formatting Text",
    "text": "Formatting Text\nThe package also allows for the formatting of text with CSS as well. The following is markdown text that will be formatted:\nCan highlight {#bgblack multiple words}.\nThe key is the use of braces following by the CSS id to add to the HTML file. Example usage can be shown with an example file that comes with the package and generated with the following code:\n\nfile &lt;- system.file('examples', 'bgtext.html', package = 'highlightHTML')\n\n# Change background color and text color with CSS\ntags &lt;- c(\"#bgblack {background-color: black; color: white;}\", \n  \"#bgblue {background-color: #0000FF; color: white;}\",\n  \"#colgreen {color: green;}\")\n\n# Post-process HTML file\nhighlight_html(input = file, output = tempfile(fileext = \".html\"),\n               tags = tags, update_css = TRUE, browse = TRUE)\n\nThe HTML file would look as follows:"
  },
  {
    "objectID": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#markdown-to-html-directly",
    "href": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#markdown-to-html-directly",
    "title": "Use CSS to format markdown or HTML files",
    "section": "Markdown to HTML Directly",
    "text": "Markdown to HTML Directly\nFinally, with help of the rmarkdown package, files can be rendered directly from markdown to an HTML file. Below is an example of this:\n\nfile &lt;- system.file('examples', 'mwe.md', package = 'highlightHTML')\ntags &lt;- c(\"#bgred {background-color: #FF0000; color: white;}\",\n   \"#bgblue {background-color: #0000FF; color: white;}\",\n   \"#bgblack {background-color: #000000; color: white;}\",\n   \"#colgold {color: #FFD700;}\")\nhighlight_html(input = file, output = tempfile(fileext = '.html'),\n  tags = tags, update_css = TRUE, browse = TRUE, render = TRUE)"
  },
  {
    "objectID": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#summary",
    "href": "posts/2017-01-03-use-css-to-format-markdown-or-html-files.html#summary",
    "title": "Use CSS to format markdown or HTML files",
    "section": "Summary",
    "text": "Summary\nThe package has a few additional features including the ability to inject tags directly into R tables, see for an example of this. To come are a few basic CSS tags that will be built into the package using specific CSS ids. Bug reports are appreciated and can be logged on GitHub https://github.com/lebebr01/highlightHTML/issues.\nEnjoy!"
  },
  {
    "objectID": "posts/2014-01-24-when-i-use-plyrdplyr.html",
    "href": "posts/2014-01-24-when-i-use-plyrdplyr.html",
    "title": "When I use plyr/dplyr",
    "section": "",
    "text": "My last post I talked about how I use the data.table package for aggregating and removing duplicate observations. Although I use the data.table package quite often, there are many times when I use plyr (and now the new dplyr) package, primarily because of its easy, intuitive syntax.\n\nArrange\nOne of my personal favorite functions in the plyr suite of basic functions is the arrange function. The base functions for sorting/ordering are more difficult to use. Not to mention there have been many times that I have used the base::sort function when I really need to use the base::order function (sort to me is the word I think of first). arrange is great due to the easy, general syntax used for it as shown below:\n\nlibrary(dplyr)\narrange(dataframe, col1, col2, col3)\n\nWhen using the base::order function, this needs to be done through the indexing operators and is not nearly as intuitive to me. I always have to think for a second to get it right. Here are two general examples:\n\ndataframe[order(dataframe$col1, dataframe$col2, dataframe$col3), ]\nwith(dataframe, dataframe[order(col1, col2, col3), ])\n\nBoth involve much more typing and are more difficult to read the code in my opinion.\n\n\nSimple, Intuitive syntax\nThe other aspect of the plyr (and dplyr) suite of functions that keeps me coming back is their simple, intuitive syntax. For example, if I am teaching a student how to aggregate or sort, plyr is my go to package. Easy to explain, easy to understand. The common structure across all of the functions is brilliantly programmed and a standard for everyone else to replicate.\n\n\nNew bonus use with dplyr\nThe new ability to use the chain function or alternatively the %&gt;% operator is a great addition to R. One of the difficulties with code readability in R is the whenever functions are nested together. By default R interprets from inside to out, not how most of us read written words let along code. The chain function and %&gt;% operator allows the user to write the functions in the order they will be processed by R, therefore the code can read from left to right.\nUsing the mtcars dataset, suppose we wanted to select the columns we wanted, aggregate the miles per gallon, and filter so we select the average miles per gallon greater than 20.\n\nlibrary(dplyr)\nmtcars %&gt;% \n  group_by(cyl, am) %&gt;%\n  select(mpg, cyl, wt, am) %&gt;%\n  summarise(avgmpg = mean(mpg), avgwt = mean(wt)) %&gt;%\n  filter(avgmpg &gt; 20)\n\n# A tibble: 3 × 4\n# Groups:   cyl [2]\n    cyl    am avgmpg avgwt\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     4     0   22.9  2.94\n2     4     1   28.1  2.04\n3     6     1   20.6  2.76\n\n\nCompare the above syntax to:\n\nfilter(\n  summarise(\n    select(\n      group_by(mtcars, cyl, am),\n      mpg, cyl, wt, am),\n    avgmpg = mean(mpg), avgwt = mean(wt)),\n  avgmpg &gt; 20)\n\n# A tibble: 3 × 4\n# Groups:   cyl [2]\n    cyl    am avgmpg avgwt\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     4     0   22.9  2.94\n2     4     1   28.1  2.04\n3     6     1   20.6  2.76\n\n\nBoth chunks give you the same result, however the first one is much easier to see the process taken to get to the end result. Much easier to adapt the code to add/remove parts of it.\n\n\nConclusion\nI use both data.table and plyr/dplyr packages. All of these packages are a great tool to for certian data problems. If I want to write a single line of code to do a lot of manipulations I will revert toward data.table. However, if I am writing code where I am doing more exploration or if I am collaborating with others I tend to write my code using plyr/dplyr. The versatility that both packages bring together in tandem is a great combination. I do not have time to be a complete package elitest, the correct tool for the right data problem."
  },
  {
    "objectID": "posts/2013-11-01-formatting-markdown-table-with-r.html",
    "href": "posts/2013-11-01-formatting-markdown-table-with-r.html",
    "title": "Formatting Markdown Table with R",
    "section": "",
    "text": "This past summer when the twin cities R user group was starting to get back up and running, I offered to present on some R related things that I was working on. One thing I was working on was a part of my last post. In a response to discussing this during a meeting, I was posed with the problem of how to do this with a markdown table. I replied I was unsure how to do this directly with R, but it could likely be possible.\nAfter replying to this e-mail, I went to work thinking about how this could be done. I knew you could add some CSS to the resulting HTML file, but the question remained how could this be done succintly and with conditional formatting.\nThe resulting thought process led me to create the highlightHTML which post processes the HTML file to inject some CSS into the HTML file. Here is a simple example to get you started.\nFirst you have a table written in markdown that looks like the following:\n\n\n\nColor Name\nNumber\n\n\n\n\nBlue\n5\n\n\nGreen\n35\n\n\nOrange\n100\n\n\nRed\n200\n\n\n\nNow suppose we want to turn the background color of those less than 5 to blue, and those greater than 100 to red. To do this using the highlightHTML package, the table would change to this:\n\n\n\nColor Name\nNumber\n\n\n\n\nBlue\n5 #bgblue\n\n\nGreen\n35\n\n\nOrange\n100\n\n\nRed\n200 #bgred\n\n\n\nThe addition of the ‘#bgblue’ and ‘#bgred’ tags indicate which cells to change and will also define new id values to assign directly to these cells through CSS. After adding the tags to the cells to format and converting the markdown file to HTML, it is now time to run the highlightHTMLcells command within R. This command will remove the tags from the table, inject CSS into the resulting HTML document, and assign the id to the specific cells. Below are the commands needed to install the package and post-process the file:\n\nlibrary(devtools)\ninstall_github(repo = \"highlightHTML\", username = \"lebebr01\")\nlibrary(hightlightHTML)\ntags &lt;- c(\"#bgred {background-color: #FF0000;}\", \"#bgblue {background-color: #0000FF;}\")\nhighlightHTMLcells(input = \"path/to/file\", output = \"path/to/saved/file\", updateCSS = TRUE, tags = tags)\n\nThis results in an HTML table that looks like the following:\n\n\n\n\nColor Name\n\n\nNumber\n\n\n\n\n\n\nBlue\n\n\n5\n\n\n\n\nGreen\n\n\n35\n\n\n\n\nOrange\n\n\n100\n\n\n\n\nRed\n\n\n200\n\n\n\n\nMore explanation of the highlightHTMLcells command, the input argument is the location of the pre-processed HTML file, the output argument is the place the post-processed HTML file is saved (note: if no output argument is given, it will overwrite the input file), the updateCSS argument tells the function whether to inject the CSS or not, finally the tags argument is a vector of the CSS that will be injected into the post-processed HTML file. The last argument highlights a drawback of the package in it’s current state, the user must know some CSS to tell the function what to inject into the HTML file. On the one hand, CSS is not overly difficult to learn, but some default behavior would be nice. I hope to add this in the future.\nThere you have it, a way to add some formatting to a table written in markdown and being presented in HTML. Future additions will hopefully add the conditional part into the mix as well. Stay tuned."
  },
  {
    "objectID": "posts/2014-08-05-dodged-bar-charts-why-not-a-line-graph.html",
    "href": "posts/2014-08-05-dodged-bar-charts-why-not-a-line-graph.html",
    "title": "Dodged bar charts, why not a line graph?",
    "section": "",
    "text": "I often see graphs that are poorly implemented in that they do not achieve their goal. One such type of graph that I see are dodged bar charts. Here is an example of a dodged bar chart summarizing the number of all star players by team (focusing specifically on the AL central division) and year from the Lahman r package:\nlibrary(Lahman)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\nAllstarFull$selected &lt;- 1\n\nnumAS &lt;- AllstarFull  %&gt;% \n  filter(yearID &gt; 2006, lgID == 'AL', teamID %in% c('MIN', 'CLE', 'DET', 'CHA', 'KCA')) %&gt;%\n  group_by(teamID, yearID) %&gt;%\n  summarise(number = sum(selected))\n\nb &lt;- ggplot(numAS, aes(x = teamID, y = number, fill = factor(yearID))) + theme_bw()\nb + geom_bar(stat = \"identity\", position = \"dodge\") + \n  scale_fill_brewer(\"Year\", palette = \"Dark2\") \n\nNote: If you are curious from the above graph, there appears to be two typos in the teamIDs, where CHA should be CHW (Chicago White Sox) and KCA should be KCR (Kansas City Royals).\nThe plot above can be good for a few things, predominantly for comparison within a team. It is more difficult to compare between teams (although not impossible). One way to possibly improve the plot would be to add the number either above each bar or inside of each bar. This can be done in ggplot2 with the geom_text function. For example:\nb &lt;- ggplot(numAS, aes(x = teamID, y = number, fill = factor(yearID))) + theme_bw()\nb + geom_bar(stat = \"identity\", position = \"dodge\") + \n  scale_fill_brewer(\"Year\", palette = \"Dark2\") + \n  geom_text(aes(label = number), position = position_dodge(width = 0.9), \n            vjust = 1.5)\nWarning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Dark2 is 8\nReturning the palette you asked for with that many colors\n\nA better alternative to a dodged bar chart in my opinion would be a simple line graph. The line graph simplifies the graph to only include one variable on the x-axis and uses colors or shapes to differentiate the teams. See below.\nl &lt;- ggplot(numAS, aes(x = yearID, y = number, color = teamID, shape = teamID))\nl + geom_point(size = 4) + geom_line(size = 1) +\n  scale_y_continuous(limits = c(0, 7), expand = c(0,0)) + \n  scale_color_brewer(\"Team\", palette = \"Dark2\") + scale_shape_discrete(\"Team\") + \n  xlab(\"Year\") + theme_bw()\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nThis presentation makes it much easier to compare teams within a single year and also see how the teams have changed over time. In my opinion this is a much simpler graphic and usually is a better option to serve the purpose for the graphic. As always though, the best graphic is one that conveys the message in the simplest, easiest to understand form. As always, you could improve this by making it interactive with rCharts. You could see my post on rCharts here and here."
  },
  {
    "objectID": "posts/cluster-coaches.html",
    "href": "posts/cluster-coaches.html",
    "title": "Hierarchical Cluster Analysis of Coach ELO Ratings",
    "section": "",
    "text": "For the past few years (quickly approaching a decade), a colleague and I started scraping information about college football coaches to explore the question, which coach should the University of Minnesota hire. This project started, as most interesting project do, at happy hour one day debating who should be hired to replace Tim Brewster, the latest college football coaching disaster at the University of Minnesota. We decided to collect some data to explore the idea in more detail.\nFast forward about 8 years and we have gathered a fair amount of college football data going back to the 19th century. We have prepared a few presentations using the data (https://brandonlebeau.org/slides/2016-07-31-jsm2016/ and https://brandonlebeau.org/slides/2015-12-04-centraliowaruser/) and are currently working on a new manuscript. Hope to share more on that soon.\nIn addition to these topics, I’ve been fiddling with hierarchical cluster analysis for another project and wanted to try it on the college football data to try and get a better handle on all the details. Below is a cluster analysis based on some of our data."
  },
  {
    "objectID": "posts/cluster-coaches.html#hierarchical-clustering-elo-ratings-of-college-football-coaches",
    "href": "posts/cluster-coaches.html#hierarchical-clustering-elo-ratings-of-college-football-coaches",
    "title": "Hierarchical Cluster Analysis of Coach ELO Ratings",
    "section": "Hierarchical clustering ELO ratings of college football coaches",
    "text": "Hierarchical clustering ELO ratings of college football coaches\nFor the paper we are exploring, we computed ELO ratings for college football coaches. This is more commonly done on teams instead of coaches, but we are more interested in the coach level compared to the team level. Admittedly, there are some coach/team pairs that would be identical for the two, but many coaches move around a bit and the two need not be the same.\nThe following bit of code reads the ELO rating data and some coach information from a GitHub repository. Then these two data frames are merged together, the data are filtered to only include ratings since 2010, and missing data is omitted. Cluster analysis works best without missing data, therefore this is a simple way to achieve no missing data.\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(forcats)\nlibrary(cluster)\nlibrary(dendextend)\nlibrary(colorspace)\n\nELO = read_csv(\"https://raw.githubusercontent.com/lebebr01/sloan-2018/master/data/elo-ratings-2017-09-18.csv\")\ncoach_info = read_csv(\"https://raw.githubusercontent.com/lebebr01/sloan-2018/master/data/coaches_to2015.csv\")\n\n\n# transform\nELO_wide &lt;- ELO %&gt;%\n  filter(Year &gt; 2010) %&gt;%\n  select(Coach, Year, Rating, Rank) %&gt;%\n  gather(Rating:Rank, key = 'variable', value = 'value') %&gt;%\n  unite(col = 'variable', variable, Year) %&gt;%\n  spread(variable, value) %&gt;%\n  na.omit()\n\nThis results in a total of 71 coaches that have been coaching continuously between 2011 and 2015, a total of five years. This does not mean that each coach was with the same team for these five years, rather that they have had no gaps in coaching between 2011 and 2015 (i.e. taking a year off of coaching) or are new coaches since 2011.\nThen to perform the cluster analysis, I used the select function to select only the ELO Rating variables. This is passed to the daisy function from the cluster package to create a pairwise distance matrix. I chose euclidean distance for this example which represents the root sum of squares of differences, but other options are available and results may differ based on the choice of distance metric.\nLastly, I also chose the ward clustering method, primarily for simplicity. From what I’ve read this tends to be a good starting method, but others do exist that use slightly different criteria when combining clusters. Therefore others should be explored to undertsand the sensitivity of the results to a specific clustering method.\n\n# Attempt cluster analysis\nward_clust &lt;- ELO_wide %&gt;%\n  select(starts_with('Rating')) %&gt;%\n  daisy(metric = 'euclidean') %&gt;%\n  agnes(diss = TRUE, method = 'ward')\n\nOnce the clustering algorithm is done, visualizing this with a dendrogram. Below is an example using the dendextend package.\n\ncoach_names &lt;- rev(levels(ELO_wide$Coach))\ndend &lt;- as.dendrogram(ward_clust)\n# order the observations:\ndend &lt;- rotate(dend, 1:71)\n\n# Color the branches based on the clusters:\ndend &lt;- color_branches(dend, k=3)\n\n# Manually match the labels\nlabels_colors(dend) &lt;-\n  rainbow_hcl(3)[sort_levels_values(\n    as.numeric(ELO_wide$Coach)[order.dendrogram(dend)]\n  )]\n\n# We shall add the flower type to the labels:\nlabels(dend) &lt;- paste(as.character(ELO_wide$Coach)[order.dendrogram(dend)],\n                      \"(\",labels(dend),\")\", \n                      sep = \"\")\n# We hang the dendrogram a bit:\ndend &lt;- hang.dendrogram(dend, hang_height=0.2)\ndend &lt;- set(dend, \"labels_cex\", 0.5)\n# And plot:\npar(mar = c(3,3,3,7))\nplot(dend, \n     main = \"Clustered ELO data set\n     (the labels give the Coach Name)\", \n     horiz =  TRUE,  nodePar = list(cex = .007))\n\n\n\n\n\n\n\n\nFrom the dendrogram, it appears from these 71 coaches, there are three groups that stand out from the rest. A further exploration of the results can be done with a heatmap. Below I create a d3 heatmap of the ELO Ratings and also include the dendrogram for the rows. The heatmap can be useful to explore similarities within the different groups. From the heatmap, it is a bit easier to see that the top cluster is the highest performing coaches, the middle cluster is the lowest performing, and the bottom cluster is an average group of coaches over this period based on ELO ratings.\n\nd3heatmap::d3heatmap(as.matrix(select(ELO_wide, starts_with('Rating'))),\n                     dendrogram = \"row\",\n                     Rowv = dend,\n                     colors = \"Greens\",\n                     # scale = \"row\",\n                     height = 1200,\n                     show_grid = FALSE)\n\nWarning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette RdYlBu is 11\nReturning the palette you asked for with that many colors\nWarning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette RdYlBu is 11\nReturning the palette you asked for with that many colors"
  },
  {
    "objectID": "posts/2014-03-27-evolution-of-code.html",
    "href": "posts/2014-03-27-evolution-of-code.html",
    "title": "Evolution of Code",
    "section": "",
    "text": "Recently while scraping some data from the college football data warehouse site, I started to realize the evolution of my code. To preface this, I am definitely not a trained programmer, just a self taught junky who enjoys doing it when I have time. I’ve slowly evolved my programming skills from simply statistics languages like r or SPSS, to some other languages like LaTeX, HTML, CSS, Javascript, and I’ve started to work through some python.\nNow back to my realization. As I mentioned, I was scraping some data from CFB Data Warehouse for a project that I’m working on with a colleague and was adapting some code that was written about 3 years ago. The problem was that my old code was broken. The original code was about 100 lines of code just to select the correct table and format it. Here is a chunk of the original code to select the correct table.\n\n  ##Identifying correct tables\n    tb &lt;- vector(\"list\", length(tableNodes))\n      for(i in 1:length(tableNodes)){\n        tb[[i]] &lt;- readHTMLTable(tableNodes[[i]])\n      }\n\n  ##Tables that are the correct length\n    tabNum &lt;- matrix(nrow=length(tableNodes), ncol=2)\n    tabNum[,1] &lt;- sapply(tb, length)\n    tabNum[,2] &lt;- 1:length(tableNodes)\n\n   Num &lt;- subset(tabNum, tabNum[,1] == 7)[,2]\n\n  ##Selecting and combining tables\nif(length(Num) == 5){\n   tb1 &lt;- tb[[Num[3]]]\n   tb1$Other &lt;- 0\n   tb2 &lt;- tb[[Num[5]]]\n   tb2$Other &lt;- 1\n   tab &lt;- rbind(tb1, tb2)\n } else { \n  if(length(Num) ==3){\n   tab &lt;- tb[[Num[3]]]\n   tab$Other &lt;- 1\n } else {\n  tab &lt;- matrix(NA, ncol= 8, nrow=1)  \n }\n }\n\nThis code was looped over many different pages and was run once for every page. Essentially the code is complicated and inconsistent, but at the time 3 years ago the code ran and that was enough for me. Extract the data from the website no matter how much code was needed to do the work. This was back in an era when I was just becoming familiar with much or R, the XML package, and attempting to scrape data from a messy/complicated site.\nMy new code to extract the tables looks like this:\n\n# extracting tables\n  tabs &lt;- lapply(seq(3, length(Nodes), 1), function(x) \n    readHTMLTable(Nodes[[x]], stringsAsFactors = FALSE))\n  \n  # Combine tables\n  bowl &lt;- do.call(\"rbind\", tabs)\n\nMuch cleaner, simpler, more consistent, and quite possibly quicker. The ability to focus on speed, readability, and consistency is something that comes later after one becomes more comfortable with the language. I have been focusing on this for awhile, but these stark differences and ease I was able to adapt my old code especially struck me this time. I haven’t decided if this evolution for me is mastery or expert status of the r language, but I now feel I have progressed to a point where I feel confident and am able to shift my focus from having code that works, to code that is now clean, consistent, and readable.\nHas anyone else had similar epiphanies with their code?\nLastly, if you want to see the raw code, go to the github page: https://github.com/lebebr01/cfbFootball."
  },
  {
    "objectID": "posts/2014-03-03-rcharts-in-slidy.html",
    "href": "posts/2014-03-03-rcharts-in-slidy.html",
    "title": "rCharts in slidy",
    "section": "",
    "text": "My last post I talked about using rCharts to create interactive graphics for my presentation. They seemed to go over pretty well in my interviews and helped me greatly as I did not need to remember or write down specific numbers to talk about. I use slidy to create my HTML slideshows and there was some interest to see exactly how I had these charts into a slidy html presentation.\nFirst off, I did not use rCharts and knitr in tandem, but that would make the workflow a bit easier. The major thing you’d want to remember is to make sure to add the following chunk option: results = 'asis'. This will ensure that the raw html printed from rCharts will be included in the markdown file as is.\nI personally just copy and pasted the javascript into my markdown presentation. This was easier for me as I edited many specific options in the raw Javascript to come to my final version. It would be possible to make all the edits directly through the rCharts framework, but it was easier for me to just look at the highcharts.js documentation to get the figure I was looking for.\nFor those who did not see my last post, here is the R code I used to create my graphic:\n\nlibrary(rCharts)\n\nh1 &lt;- hPlot(x = \"GenSerCor\", y = \"percent\", group = \"FitSerCor\", data = converge)\nh1$yAxis(title = list(text = \"Convergence Rate\"), min = 0, max = 100, tickInterval = 10)\nh1$xAxis(title = list(text = \"Generated Serial Correlation Structure\"),\n         categories = c(\"Ind\", \"AR1\", \"MA1\", \"MA2\", \"ARMA\"))\nh1$legend(verticalAlign = \"top\", align = \"right\", layout = \"vertical\", title = list(text = \"Fitted SC\"))\nh1$plotOptions(series = list(lineWidth = 4))\nh1$print('chart1', include_assets = TRUE, cdn = TRUE)\n\nAfter I ran this command in R, I edited the resulting Javascript code that was printed from the last line of the R code above. My final Javascript code can be seen below.\n\n\n\n\n\n\n\n\n\nOnce you have that in markdown format, you can turn it into a slidy html presentation with the following command in pandoc:\npandoc -s --mathjax -i -t slidy inputfile.md -o outfile.html\nThis gives you a file that looks something like this:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n&lt;head&gt;\n  &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /&gt;\n  &lt;meta http-equiv=\"Content-Style-Type\" content=\"text/css\" /&gt;\n  &lt;meta name=\"generator\" content=\"pandoc\" /&gt;\n  &lt;meta name=\"author\" content=\"Brandon LeBeau\" /&gt;\n  &lt;title&gt;Impact of serial correlation structures on random effect misspecification with the linear mixed model.&lt;/title&gt;\n  &lt;style type=\"text/css\"&gt;code{white-space: pre;}&lt;/style&gt;\n  &lt;link rel=\"stylesheet\" type=\"text/css\" media=\"screen, projection, print\"\n    href=\"stylesheets/slidy.css\" /&gt;\n&lt;script src=\"stylesheets/slidy.js\" charset=\"utf-8\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script type='text/javascript' src=stylesheets/jquery-1.9.1.min.js&gt;&lt;/script&gt;\n&lt;script type='text/javascript' src=stylesheets/highcharts.js&gt;&lt;/script&gt;\n&lt;script type='text/javascript' src=stylesheets/highcharts-more.js&gt;&lt;/script&gt;\n&lt;script type='text/javascript' src=stylesheets/exporting.js&gt;&lt;/script&gt; \n &lt;style&gt;\n  .rChart {\n    display: block;\n    margin-left: auto; \n    margin-right: auto;\n    width: 1000px;\n    height: 800px;\n    font-size: 200%;\n  }  \n  &lt;/style&gt;\n&lt;script src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"&gt;MathJax.Hub.Queue([\"Typeset\",MathJax.Hub]);&lt;/script&gt;\n &lt;!--   &lt;script src=\"http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js\"\n    charset=\"utf-8\" type=\"text/javascript\"&gt;&lt;/script&gt; --&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div id = 'chart1' class = 'rChart'&gt;&lt;/div&gt;\n&lt;script type='text/javascript'&gt;\n    (function($){\n        $(function () {\n            var chart = new Highcharts.Chart({\n \"dom\": \"chart1\",\n\"width\":            1000,\n\"height\":            600,\n\"credits\": {\n \"href\": null,\n\"text\": null \n},\n\"exporting\": {\n \"enabled\": false \n},\n\"title\": {\n \"text\": null \n},\n\"yAxis\": [\n {\n title: {\n text: \"Convergence Rate\",\n  style: {\n   fontWeight: 'bold',\n   fontSize: '20px'\n   }\n },\n labels: {\n  formatter: function() {\n   return this.value + '%';\n  },\n  style: {\n   fontSize: '18px'\n  }\n },\n\"min\":              0,\n\"max\":            100,\n\"tickInterval\":             10 ,\nminRange: 10\n} \n],\n\"series\": [\n {\n \"data\": [\n [\n \"Ind\",\n         68.38 \n],\n[\n \"AR1\",\n         64.88 \n],\n[\n \"MA1\",\n         55.12 \n],\n[\n \"MA2\",\n         61.98 \n],\n[\n \"ARMA\",\n         42.17 \n] \n],\nevents: {\n            mouseOver: function () {\n                this.update({\n                    color: 'black'\n                });                \n            },\n            mouseOut: function () {\n                this.update({\n                    color: '#e41a1c'\n                }); \n            }\n        },\n\"color\": \"#e41a1c\",\n\"name\": \"AR1\",\n\"type\": null,\ndashStyle: 'Solid',\n\"marker\": {\n \"radius\":              6\n} \n},\n{\n \"data\": [\n [\n \"Ind\",\n          65.1 \n],\n[\n \"AR1\",\n         60.45 \n],\n[\n \"MA1\",\n         63.68 \n],\n[\n \"MA2\",\n         54.88 \n],\n[\n \"ARMA\",\n          63.6 \n] \n],\nevents: {\n            mouseOver: function () {\n                this.update({\n                    color: 'black'\n                });                \n            },\n            mouseOut: function () {\n                this.update({\n                    color: '#377eb8'\n                }); \n            }\n        },\n\"color\": \"#377eb8\",\n\"name\": \"ARMA\",\n\"type\": null,\ndashStyle: 'ShortDash',\n\"marker\": {\n \"radius\":              6 \n} \n},\n{\n \"data\": [\n [\n \"Ind\",\n         72.48 \n],\n[\n \"AR1\",\n         93.88 \n],\n[\n \"MA1\",\n         92.23 \n],\n[\n \"MA2\",\n         95.62 \n],\n[\n \"ARMA\",\n         98.37 \n] \n],\nevents: {\n            mouseOver: function () {\n                this.update({\n                    color: 'black'\n                });                \n            },\n            mouseOut: function () {\n                this.update({\n                    color: '#4daf4a'\n                }); \n            }\n        },\n\"color\": \"#4daf4a\",\n\"name\": \"Ind\",\n\"type\": null,\ndashStyle: 'Dash',\n\"marker\": {\n \"radius\":              6 \n} \n},\n{\n \"data\": [\n [\n \"Ind\",\n         71.02 \n],\n[\n \"AR1\",\n         81.37 \n],\n[\n \"MA1\",\n         69.15 \n],\n[\n \"MA2\",\n          84.5 \n],\n[\n \"ARMA\",\n         88.02 \n] \n],\nevents: {\n            mouseOver: function () {\n                this.update({\n                    color: 'black'\n                });                \n            },\n            mouseOut: function () {\n                this.update({\n                    color: '#984ea3'\n                }); \n            }\n        },\n\"color\": \"#984ea3\",\n\"name\": \"MA1\",\n\"type\": null,\ndashStyle: 'ShortDot',\n\"marker\": {\n \"radius\":              6\n} \n},\n{\n \"data\": [\n [\n \"Ind\",\n         67.23 \n],\n[\n \"AR1\",\n         70.78 \n],\n[\n \"MA1\",\n         65.93 \n],\n[\n \"MA2\",\n         68.83 \n],\n[\n \"ARMA\",\n          72.9 \n] \n],\nevents: {\n            mouseOver: function () {\n                this.update({\n                    color: 'black'\n                });                \n            },\n            mouseOut: function () {\n                this.update({\n                    color: '#ff7f00'\n                }); \n            }\n        },\n\"color\": \"#ff7f00\",\n\"name\": \"MA2\",\n\"type\": null,\ndashStyle: 'DashDot',\n\"marker\": {\n \"radius\":              6 \n} \n} \n],\n\"xAxis\": [\n {\n title: {\n text: \"Generated Serial Correlation Structure\",\n  style:{\n   fontWeight: 'bold',\n   fontSize: '20px'\n }\n},\nlabels: {\n style: {\n  fontSize: '18px',\n  fontWeight: 'bold'\n }\n},\n\"categories\": [ \"Ind\", \"AR1\", \"MA1\", \"MA2\", \"ARMA\" ] \n} \n],\n\"subtitle\": {\n \"text\": null \n},\n\"legend\": {\n \"verticalAlign\": \"top\",\n\"align\": \"right\",\n\"layout\": \"vertical\",\nsymbolWidth: 40,\n\"title\": {\n \"text\": \"Fitted SC\" \n} \n},\n\"plotOptions\": {\n \"series\": {\n \"lineWidth\":              4 \n} \n},\n\"id\": \"chart1\",\n\"chart\": {\n \"renderTo\": \"chart1\", \n zoomType: \"y\",\n \"style\": {\n fontSize: \"24px\"\n },\n resetZoomButton: {\n  position: {\n   align: 'left'\n  }\n }\n} \n});\n        });\n    })(jQuery);\n&lt;/script&gt;\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nThat should give you a html presentation with an interactive Javascript based figure."
  },
  {
    "objectID": "posts/2014-02-02-picking-a-gui-interface-for-r.html",
    "href": "posts/2014-02-02-picking-a-gui-interface-for-r.html",
    "title": "Picking a gui interface for R",
    "section": "",
    "text": "Recently I decided to switch statistical programs used for the master’s level introductory statistics course I teach here at the University of Arkansas. Historically this course has been taught with SPSS, but I am attempting the switch to R this semester.\nMy reasons for having students use the gui interface is primarily due to the lack of programming experience. A brief initial poll revealed that only one student had prior programming/code writing experience. Therefore, I did not want to have students become acclimated to the statistics content as well as the R code. The two primary gui systems/packages I explored were the Deducer package and Rcmdr package. Here are my initial thoughts on the switch.\n\nWhich gui to use?\nThis ultimately comes down to personal preference, however below I’ve highlighted my thoughts on pros/cons of each package.\n\n\nDeducer\n\n\n\nPros\nCons\n\n\n\n\nShows syntax of commands run\nUses Java\n\n\nUses ggplot2 for plots\nMenus differ from PC to MAC\n\n\nVery interactive menu structures\nrJava package can be troublesome\n\n\nSimple condensed data loading\n\n\n\n\n\n\nRcmdr\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nShows syntax of commands run\nUses base graphics\n\n\nDoes not use Java\nData loading structures not integrated\n\n\nVery similar data menus across OS\n\n\n\n\nIn general I prefer the Deducer package as it uses ggplot2 and has a more unified menu structure. For example, there are not differing menu options for loading data like in Rcmdr. Instead, Deducer has a single load data menu where it is possible to load many types of data including csv, txt, rda, etc. This is helpful for students who are not very familiar with differing file types. Deducer also uses ggplot2 for its graphics which I enjoy much more than base graphics. In my opinion they look better and the syntax is ultimately easier to create high quality graphics.\nMy biggest complaint of the Deducer package is that it uses Java. This is one more thing that the user needs to install and with my class we have had trouble on a few computers getting the rJava package to work properly. In general it is difficult to troubleshoot a student’s computer, especially when I am unable to recreate the problem on my own machine. Lastly, the differing look of Deducer has made it more difficult for me as I need to have two explanations, one for those on a PC and another for those on a Mac. Just tedious and has been difficult for me as I do not use Mac.\n\nConcluding thoughts\nI hope to write more about my experiences using both gui systems for my class, but upon initial inspection of them now I definitely prefer Deducer. The package just has not completely won me over as little problems have made me use both packages in my class so far. Does anyone else have experience using one or the other in a class before? I’d enjoy hearing any stories using these or different gui systems for R."
  },
  {
    "objectID": "posts/simglm-0-7-4.html",
    "href": "posts/simglm-0-7-4.html",
    "title": "simglm v0.7.4: Tidy Simulation",
    "section": "",
    "text": "I’m happy to formally announce a major update to the simglm R package. In brief, the updated package contains a new more robust syntax for simulating data, adds parallel processing support for replicating the simulation (or power analysis) using the future.apply package/framework, and new updated vignettes showing off the many options available in the tidy simulation syntax.\nThe package can be installed with the following code:\ninstall.packages(\"simglm\")\nThe package can then be loaded with:\nlibrary(simglm)"
  },
  {
    "objectID": "posts/simglm-0-7-4.html#tidy-simulation-syntax",
    "href": "posts/simglm-0-7-4.html#tidy-simulation-syntax",
    "title": "simglm v0.7.4: Tidy Simulation",
    "section": "Tidy simulation syntax",
    "text": "Tidy simulation syntax\nThe tidy simulation syntax and new simulation functions take full advantage of using the pipe (i.e. %&gt;%) from the magrittr package to chain together commands for readable code that highlights what is being simulated and when. In this framework, the simulation arguments are specified outside of the simulation commands to separate the specification of how the data should be simulated from the actual simulation. An example is most useful here.\n\nTidy simulation example\nSuppose we wanted to generate data for two groups, such as the sex of the individual (male or female). We could write this statistical model as the following:\n\\[\noutcome = \\beta_{0} + \\beta_{1} male + \\epsilon\n\\] where \\(male\\) is an indicator (i.e. dichomotous or dummy variable) that represents if the individual is a male or not. The outcome is the observed score of interest, could be test scores, depression scores, blood pressure, really anything of interest. The regression coefficients, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are estimated from the data and would be the intercept (in this case would be the mean of the females on the outcome) and the mean difference between the males and females respectively. Finally, it is common to have error, represented with \\(\\epsilon\\). This model is the linear regression version of a two-sample t-test.\nTo simulate this data, the values for the regression coefficients (\\(\\beta_{0}\\) and \\(\\beta_{1}\\)) need to be specified, the \\(male\\) indicator variable, and the variance and distribution of the error term. The outcome is then determined based on those three terms. Let’s now use simglm to simulate data based on this two group model.\n\n\nSpecify simulation arguments\nWhen using the simglm R package, the simulation arguments are first defined in a named list object. Required arguments to this named list that include the following arguments with a short description of what each represents.\n\nformula: This mimics the model to be generated excluding any random error. If you have used the lm function to fit a linear regression, this would be the model equation that is fitted to that function.\nfixed: This includes variable specification as a list for the variable specified on the right hand side of the equation (i.e. to the right of the ~ in the formula argument). The fixed argument does not include the intercept or interaction terms.\nsample_size: The sample size or in this example, the number of rows in the data.\nerror: This is a list of characteristics for the random error (i.e. variance, distribution, etc.), \\(\\epsilon\\) in the above equation.\nreg_weights: These are the values for the \\(\\beta\\) terms in the model. In this example there are two, one for the intercept and one representing the mean difference between males and females.\n\nBelow is an example specification for the linear model shown above that mimics a two-sample t-test.\n\nsim_arguments &lt;- list(\n  formula = outcome ~ 1 + sex,\n  fixed = list(sex = list(var_type = 'factor', \n                            levels = c('male', 'female'))),\n  sample_size = 100,\n  error = list(variance = 1),\n  reg_weights = c(0, .15)\n)\n\nFrom the above simulation arguments, the formula mimics the equation shown earlier with one small change, sex is used instead of male to reflect the grouping variable of male or female. The fixed argument warrants additional discussion. This argument is a named list where the named elements match the variable names specified on the right hand side of the formula argument. In this case, only one variable is specified, the sex variable. When specifying a variable, the type of variable to be generated is controlled with the argument, var_type. The var_type argument can take the following values:\n\nfactor: This represents a categorical or grouping variable that commonly only takes on a few unique values.\ncontinuous: This is a continuous variable that is generated using an r distribution function.\nordinal: This generates ordinal or discrete data and is generated using the sample function.\ntime: This is not as useful here, but would be used when repeated measures or longitudinal data are simulated to represent when multiple observations are collected.\n\nOf note, the “factor” and “ordinal” variable types are generated the same way, using the sample function, but are distinguished due to different general philosophies about the two variable types.\nWith “factor” and “ordinal” variable types, the different values that are possible for this variable can be specified with the levels argument. In the example above, “male” and “female” are passed as character strings indicating these are the values that will be passed to the sample function. The levels argument can take on characters or numeric values, although if character strings are meaningful, it is recommended to pass these to be more explicit about the different values generated.\nLet’s now generate the data based on the simulation arguments above.\n\n\nGenerate one data set\nThere are four main simulation functions that will be used most frequently when doing the simulating of data. These simulation functions include:\n\nsimulate_fixed: This function simulates the fixed portion of the model or the portion to the right of the ~ in the model formula.\nsimulate_error: This function simulates the random error (i.e. \\(\\epsilon\\)).\nsimulate_randomeffect: This simulates random effects if the data are nested or cross-classified. The use of this will come in another post soon.\ngenerate_response: This function generates the outcome variable or the term to the left of the ~ in the model formula.\n\nThe goal of each function is to simulate one aspect of the model which keeps the function specification simpler, allows users to see what is being simulated in specific steps, and allows users flexibility into what is being generated. Each of these functions take data as the first argument and the simulation arguments as the second argument. With the exception of the generate_response function, the three simulate_* function above can be specified in any order. The first time that a simulate_* function is called, the data argument needs to be specified as NULL meaning that no data has been generated.\nBelow is one way to generate the data using the simulation conditions and the first few rows of the simulated data are shown. Note, the seed was specified with the set.seed function to ensure replicable results. Without setting a seed, different results would be obtained everytime the simulation code was run.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(10)\n\none_data_set &lt;- simulate_fixed(data = NULL, sim_arguments) %&gt;%\n  simulate_error(sim_arguments) %&gt;%\n  generate_response(sim_arguments)\n\nhead(one_data_set)\n\n  X.Intercept. sex_1    sex level1_id      error fixed_outcome random_effects\n1            1     0   male         1 -0.4006375          0.00              0\n2            1     0   male         2 -0.3345566          0.00              0\n3            1     1 female         3  1.3679540          0.15              0\n4            1     1 female         4  2.1377671          0.15              0\n5            1     1 female         5  0.5058193          0.15              0\n6            1     0   male         6  0.7863424          0.00              0\n     outcome\n1 -0.4006375\n2 -0.3345566\n3  1.5179540\n4  2.2877671\n5  0.6558193\n6  0.7863424\n\n\nThe data is similar regardless of the model run and the columns are appended based on the order that the simulate_* functions are called. For example, since the simulate_fixed function was called first above, these columns appear first in the simulated data. Then the error is shown and the outcome variable will always be last. In between the error and the outcome are two columns that can provide a check on various portions of the simulation. In this case, the “fixed_outcome” column is of most interest and can provide a check that the simulate_fixed function created data appropriately. The value for the “fixed_outcome” column should reflect the outcome for each row that does not depend on random quantities. In this case, since the only variable in the model is a dichotomous indicator variable, the “fixed_outcome” column should have two values, the mean for females and the mean for males.\nTo ensure that things look as they should based on the simulation arguments, we can do some descriptive and/or visual inspection.\n\nExplore error\nWe can visualize the density of the random error term to ensure this makes sense.\n\nggplot(one_data_set, aes(x = error)) + \n  geom_density() + \n  theme_bw() + \n  xlab(\"Random Error\")\n\n\n\n\n\n\n\n\nWe could also calculate the variance of this term to ensure it is close to the value specified. The larger the sample size, the closer it should be to the variance specified. Here is happens to be slightly less than 1, but it is close.\n\none_data_set %&gt;%\n  summarise(error_var = var(error))\n\n  error_var\n1 0.9402422\n\n\n\n\nExplore sex effect\nThe sex effect can also be explored. One simple check would be to count how many males and females are in the data. Note, I’m using the “sex1” variable which is the character version. The one named “sex” in the data represents the character variable turned into a numeric dummy variable used in the regression model.\n\none_data_set %&gt;%\n  count(sex)\n\n     sex  n\n1   male 53\n2 female 47\n\n\nAs can be seen, the number of males and females is about the same, but not exactly equal. The default behavior of the sample function is to generate groups with equal proportions. This can be specified directly however with an optional prob argument. See the sample function documentation for additional details for this.\nAnother useful check is to see how different the outcome is for the two groups. I will do this visually, but you can do it descriptively as well by calculating the mean values.\n\nggplot(one_data_set, aes(x = sex, y = outcome)) + \n  geom_boxplot() + \n  geom_jitter(alpha = 0.7) + \n  theme_bw() + \n  xlab(\"Sex\") + \n  ylab(\"Outcome\")\n\n\n\n\n\n\n\n\nAs you can see, the males have a larger median, but there is quite a bit of variation in the two groups.\n\n\n\nReplicate the simulation\nIt is often common to generate multiple simulated data sets. This can be done with the help of a single function, replicate_simulation. This function takes the simulation arguments as its only argument. The only additional argument that needs to be specified is the number of replications which is controlled by the replications argument. Below is the inclusion of the replications argument to do 100 replications of the simulation arguments. Note, the future.apply package is used to simulate the data using all available cores on the local machine. This is done by setting plan(multisession) which should word regardless of the operating system this code is ran on.\n\nlibrary(future)\n\nplan(multisession)\n\nsimulation_arguments &lt;- list(\n  formula = outcome ~ 1 + sex,\n  fixed = list(sex = list(var_type = 'factor', \n                            levels = c('male', 'female'))),\n  sample_size = 100,\n  error = list(variance = 1),\n  reg_weights = c(0, .15),\n  replications = 100\n)\n\nreplicate_sim &lt;- replicate_simulation(simulation_arguments)\n\nThe returned data element is a list that has the same length as the number of replications specified. The elements can be extracted as with any list.\n\nhead(replicate_sim[[100]])\n\n  X.Intercept. sex_1    sex level1_id      error fixed_outcome random_effects\n1            1     1 female         1 -0.5592370          0.15              0\n2            1     0   male         2  1.3319519          0.00              0\n3            1     1 female         3  2.0973185          0.15              0\n4            1     1 female         4  0.5767978          0.15              0\n5            1     1 female         5 -1.2387176          0.15              0\n6            1     1 female         6  2.1479337          0.15              0\n     outcome\n1 -0.4092370\n2  1.3319519\n3  2.2473185\n4  0.7267978\n5 -1.0887176\n6  2.2979337\n\n\nThe replication_simulation function calls the specific simulate_* functions based on what is included in the simulation arguments. For example, if the error simulation argument was omitted, the data would not include random error.\n\nsimulation_arguments &lt;- list(\n  formula = outcome ~ 1 + sex,\n  fixed = list(sex = list(var_type = 'factor', \n                            levels = c('male', 'female'))),\n  sample_size = 100,\n  reg_weights = c(0, .15),\n  replications = 100\n)\n\nreplicate_sim &lt;- replicate_simulation(simulation_arguments)\n\nhead(replicate_sim[[1]])\n\n  X.Intercept. sex_1    sex level1_id fixed_outcome random_effects error\n1            1     1 female         1          0.15              0     0\n2            1     0   male         2          0.00              0     0\n3            1     0   male         3          0.00              0     0\n4            1     0   male         4          0.00              0     0\n5            1     0   male         5          0.00              0     0\n6            1     0   male         6          0.00              0     0\n  outcome\n1    0.15\n2    0.00\n3    0.00\n4    0.00\n5    0.00\n6    0.00\n\n\nNotice that the error is specified as 0 for all records. This is likely not that as useful as specifying random error."
  },
  {
    "objectID": "posts/simglm-0-7-4.html#next-steps",
    "href": "posts/simglm-0-7-4.html#next-steps",
    "title": "simglm v0.7.4: Tidy Simulation",
    "section": "Next Steps",
    "text": "Next Steps\nI plan to regularly post examples using the simglm package for simulating different models and also doing power for different models. I recently wrote a tutorial that includes some of this code and does power by simulation that can be found here: https://brandonlebeau.org/publication/simglm-power/.\nMore unit testing is needed for the new simulation syntax to test the new implementation and ensure that updates do not change other aspects of the code base. Implementing additional model features (such as increased support for GLM models, simulation non-linear models, etc) and writing about more examples of doing simulation or power analysis with the package.\nFeedback on the syntax, suggested changes, and edits or collaborations are especially welcomed at this stage. Issues and collaborations can be submitted through the following two links: GitHub Issues and Pull Requests."
  },
  {
    "objectID": "workshops/index.html",
    "href": "workshops/index.html",
    "title": "Workshops",
    "section": "",
    "text": "Data Visualization - Static and Interactive Graphics using R\n\n\n\n\n\n\n\n\nJun 12, 2019\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to tidyverse/R - short version\n\n\n\n\n\n\n\n\nJul 19, 2018\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to tidyverse/R\n\n\n\n\n\n\n\n\nJan 10, 2018\n\n\nBrandon LeBeau\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2013-05-03-blog-logo.html",
    "href": "posts/2013-05-03-blog-logo.html",
    "title": "Blog Logo",
    "section": "",
    "text": "Here is the code used to create the blog logo. Took a bit of trial and error, especially with the official R logo.\n\n### Creating words with connected points.\ncapE &lt;- data.frame(x = c(1.5,1,1,1.25,1,1,1.5), \n                   y = c(3,3,2,2,2,1,1))\ncapE$time &lt;- 1:nrow(capE)\n\nletd &lt;- data.frame(x = c(2, 1.75, 1.65, 1.75, 2, 2), \n                   y = c(2,2,1.5,1,1,2.5))\nletd$time &lt;- 1:nrow(letd)\n\nletu &lt;- data.frame(x = c(2.2, 2.2, 2.35, 2.55, 2.55, 2.55), \n                   y = c(2,1.15,1,1.15,2,1))\nletu$time &lt;- 1:nrow(letu)\n\nletc &lt;- data.frame(x = c(3.05, 2.8, 2.7, 2.8, 3.05), \n                   y = c(2,2,1.5,1,1))\nletc$time &lt;- 1:nrow(letc)\n\nleta &lt;- data.frame(x = c(3.55, 3.35, 3.2, 3.35, 3.55, 3.55, 3.55), \n                   y = c(1.85, 2, 1.5, 1, 1.15, 1.85, 1))\nleta$time &lt;- 1:nrow(leta)\n\nlett &lt;- data.frame(x = c(3.75, 3.75, 3.575, 3.925, 3.75, 3.75), \n                   y = c(2.5, 2.25, 2.25, 2.25, 2.25, 1))\nlett$time &lt;- 1:nrow(lett)\n\nlete &lt;- data.frame(x = c(4.3, 4.05, 3.95, 4.125, 4.3, 4.125, 3.95), \n                   y = c(1.05, 1, 1.75, 2, 1.75, 1.6, 1.75))\nlete$time &lt;- 1:nrow(lete)\n\nrlogo &lt;- data.frame(x = c(5.1, 5.1, 5.3, 5.4, 5.45, 5.4, 5.3, 5.1, 5.2,\n                          5.25, 5.35, 5.45),\n                    y = c(.5, 2, 2, 1.85, 1.675, 1.4, 1.25, 1.25, 1.25,\n                          1.2, 1.05, .5))\nrlogo$time &lt;- 1:nrow(rlogo)\n\nrcirclogo &lt;- data.frame(x = c(5.6, 5.6, 5.55, 5.45, 5.35, 5.25, 5.15,\n                              5.05, 4.95, 4.85,4.75,  4.75, 4.75,\n                              4.8, 4.95, 5.05, 5.15, 5.25, 5.35, 5.45,\n                              5.55, 5.6, 5.6),\n                        y= c(1.65, 1.8, 2, 2.15, 2.25, 2.3, 2.3, 2.25,\n                             2.15, 2.05, 1.85, 1.7, 1.55, \n                             1.35, 1.15, 1.05, .95, .95, 1.05, 1.15,\n                             1.35, 1.55, 1.65),\n                        size = c(2.25, 2.75, 3, 3.5, 4, 4, 4.5, 5, 5,\n                                 5, 5, 5, 5, 4.5, 4, 3.75, 3.25, 3,\n                                 2.75, 2.5, 2.25, 2.25, 2))\nrcirclogo$time &lt;- 1:nrow(rcirclogo)\n\n\n\nlibrary(ggplot2)\nlibrary(scales)\n\np &lt;- ggplot(capE, aes(x = x, y = y)) + theme_bw()\np + geom_path(lwd = 3, lineend = \"round\") + \n  geom_path(data = letd, lwd = 3, lineend = \"round\") + \n  geom_path(data = letu, lwd = 3, lineend = \"round\") + \n  geom_path(data = letc, lwd = 3, lineend = \"round\") + \n  geom_path(data = leta, lwd = 3, lineend = \"round\") + \n  geom_path(data = lett,lwd = 3, lineend = \"round\") + \n  geom_path(data = lete, lwd = 3, lineend = \"round\") +\n  geom_path(data = rcirclogo, aes(size = size), color = \"grey\", \n            lineend = \"round\", linejoin = \"bevel\") + \n  geom_path(data = rlogo, color = \"steelblue\", lwd = 6, \n            lineend = \"round\") + \n  geom_path(data = rlogo, color = \"grey60\", lwd = .5, \n            lineend= \"round\") +\n  theme(legend.position = \"none\", text = element_blank(), \n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = \"transparent\", \n                                       color = NA),\n        panel.background = element_rect(fill = \"transparent\", \n                                        color = NA),\n        panel.border = element_blank(),\n        axis.line = element_blank(), axis.ticks = element_blank(),\n        line = element_blank()) + scale_size(range = c(2, 8))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "posts/2017-05-23-simglm-submission-to-cran-this-week.html",
    "href": "posts/2017-05-23-simglm-submission-to-cran-this-week.html",
    "title": "simglm submission to CRAN this week",
    "section": "",
    "text": "This is a quick note looking for any further feedback on the simglm package prior to CRAN submission later this week. The goal is to submit Thursday or Friday this week. The last few documentation finishing touches are happening now working toward a version 0.5.0 release on CRAN.\nFor those who have not seen this package yet, the aim is to simulate regression models (single level and multilevel models) as well as employ empirical power analyses based on Monte Carlo simulation. The package is relatively flexible in user control of inputs to generate data.\nTo install the package and also build the vignettes:\n\ndevtools::install_github(\"lebebr01/simglm\", build_vignettes = TRUE)\n\nThen to generate a simple single level data set.\n\nlibrary(simglm)\n\nfixed &lt;- ~ 1 + act + diff + numCourse + act:numCourse\nfixed_param &lt;- c(2, 4, 1, 3.5, 2)\ncov_param &lt;- list(dist_fun = c('rnorm', 'rnorm', 'rnorm'), \n                  var_type = c(\"single\", \"single\", \"single\"),\n                  cov_param = list(list(mean = 0, sd = 4),\n                                   list(mean = 0, sd = 3),\n                                   list(mean = 0, sd = 3)))\nn &lt;- 150\nerror_var &lt;- 3\nwith_err_gen = 'rnorm'\ntemp_single &lt;- sim_reg(fixed = fixed, fixed_param = fixed_param, \n                       cov_param = cov_param,\n                       n = n, error_var = error_var, \n                       with_err_gen = with_err_gen, \n                       data_str = \"single\")\nhead(temp_single)\n\nThen one can extend this to conduct of power analysis. The benefit of this approach is that you are able to generate data that hopefully more closely resembles data that is to be collected and can also evaluate assumption violations, sample size differences, and other conditions directly into the power analysis similar to a Monte Carlo simulation.\n\nfixed &lt;- ~ 1 + act + diff + numCourse + act:numCourse\nfixed_param &lt;- c(0.5, 1.1, 0.6, 0.9, 1.1)\ncov_param &lt;- list(dist_fun = c('rnorm', 'rnorm', 'rnorm'), \n                  var_type = c(\"single\", \"single\", \"single\"),\n                  opts = list(list(mean = 0, sd = 2),\n                              list(mean = 0, sd = 2),\n                              list(mean = 0, sd = 1)))\nn &lt;- NULL\nerror_var &lt;- NULL\nwith_err_gen &lt;- 'rnorm'\npow_param &lt;- c('(Intercept)', 'act', 'diff', 'numCourse')\nalpha &lt;- .01\npow_dist &lt;- \"t\"\npow_tail &lt;- 2\nreplicates &lt;- 10\nterms_vary &lt;- list(n = c(20, 40, 60, 80, 100), error_var = c(5, 10, 20),\n                   fixed_param = list(c(0.5, 1.1, 0.6, 0.9, 1.1), \n                                      c(0.6, 1.1, 0.6, 0.9, 1.1)),\n                cov_param = list(list(dist_fun = c('rnorm', 'rnorm', 'rnorm'),\n                                       mean = c(0, 0, 0), sd = c(2, 2, 1), \n                                  var_type = c(\"single\", \"single\", \"single\")),\n                                  list(dist_fun = c('rnorm', 'rnorm', 'rnorm'),\n                                       mean = c(0.5, 0, 0), sd = c(2, 2, 1), \n                                  var_type = c(\"single\", \"single\", \"single\"))\n                                  )\n                   )\npower_out &lt;- sim_pow(fixed = fixed, fixed_param = fixed_param, \n                     cov_param = cov_param,\n                     n = n, error_var = error_var, with_err_gen = with_err_gen, \n                     data_str = \"single\", pow_param = pow_param, alpha = alpha,\n                     pow_dist = pow_dist, pow_tail = pow_tail, \n                     replicates = replicates, terms_vary = terms_vary)\nhead(power_out)\n\nQuestions and feedback are welcomed by filing an issue on GitHub here: https://github.com/lebebr01/simglm/issues."
  },
  {
    "objectID": "posts/2013-11-26-spsstor-an-r-package-to-convert-spss-syntax.html",
    "href": "posts/2013-11-26-spsstor-an-r-package-to-convert-spss-syntax.html",
    "title": "SPSStoR An R package to convert SPSS syntax",
    "section": "",
    "text": "My first statistical software package I used as an undergrad was SPSS. I was fortunate during my senior year at Luther College to be initially introduced to R. I did not realize it at the time (except for the pretty graphics) that this was the start of something big for me. Fast forward a year to graduate school at the University of Minnesota and the majority of my program was again using SPSS. Under the tutelage of Andy in my first graduate statistics course I started to do analyses in both R and SPSS. After that I started to do all my analyses in R. Fast forward five years this time to my first full-time job after graduate school at Saint Paul Public Schools and again everyone was using SPSS. Fortunately in my year there I was able to significantly push them toward using R more and more.\nThis long introduction motivates the following package I started writing mostly just to play with regular expressions, but kept playing and came up with a package with many routines. The package I came up with is called SPSStoR which converts SPSS syntax into R syntax. Using regular expressions to parse the SPSS syntax to figure out which routine is being run then prints out the R syntax to do the same analysis done by the SPSS syntax.\nAs you can imagine, the SPSS language has many routines and this is not all encompassing, however it provides many basic commands such as: * Aggregate * Correlations * Crosstab * Sort Cases * Descriptives * One sample t-test * Independent sample t-test * Get for sav files (SPSS data files) * Master SPSStoR function * Graphics * Frequencies"
  },
  {
    "objectID": "posts/2013-11-26-spsstor-an-r-package-to-convert-spss-syntax.html#examples",
    "href": "posts/2013-11-26-spsstor-an-r-package-to-convert-spss-syntax.html#examples",
    "title": "SPSStoR An R package to convert SPSS syntax",
    "section": "Examples",
    "text": "Examples\nYou can install the package directly from my github page using the devtools library:\n\nlibrary(devtools)\ninstall_github(\"SPSStoR\", \"lebebr01\")\n\nOnce you install the package, I’ve included some example SPSS syntax within the package to use as examples. One example syntax file reads in an SPSS data file, sorts the file, then computes descriptive statistics on a handful of variables. The following should run the command from your computer:\n\nlibrary(SPSStoR)\n\nLoading required package: stringr\n\nfile &lt;- paste(system.file('SPSSsyntax', package = 'SPSStoR'), \n              \"/getDescExamp.txt\", sep = '')\nspss_to_r(file)\n\nlibrary(haven)\nx &lt;- read_sav(paste(getwd(),'/data/hubtemp.sav', sep = '/'))\nlibrary(dplyr)\n# x is the name of your data frame\nx &lt;- arrange(DIVISION, STORE, -AGE)\nconvert_num &lt;- function(x) as.numeric(as.character(x));x[c(\"longmon\", \"tollmon\", \"equipmon\", \"cardmon\", \"wiremon\", \"DESCRIPTIVES\")] &lt;- sapply(x[c(\"longmon\", \"tollmon\", \"equipmon\", \"cardmon\", \"wiremon\", \"DESCRIPTIVES\")], convert_num)\nx %&gt;% summarise_each(funs(mean(., na.rm = TRUE), sd(., na.rm = TRUE), min(., na.rm = TRUE), max(., na.rm = TRUE)), one_of(\"longmon\", \"tollmon\", \"equipmon\", \"cardmon\", \"wiremon\", \"DESCRIPTIVES\")) \n\n\nHere is another example doing a one-sample t-test:\n\nfile &lt;- paste(system.file('SPSSsyntax', package = 'SPSStoR'), \n              \"/ttestOneSampExamp.txt\", sep = '')\nspss_to_r(file)"
  },
  {
    "objectID": "posts/2013-11-26-spsstor-an-r-package-to-convert-spss-syntax.html#summary",
    "href": "posts/2013-11-26-spsstor-an-r-package-to-convert-spss-syntax.html#summary",
    "title": "SPSStoR An R package to convert SPSS syntax",
    "section": "Summary",
    "text": "Summary\nAlthough some working R knowledge is needed to run this package, such as knowledge of installing packages and what an object is, this package may be useful to those trying to figure out a similar command in R. Over time I hope to slowly add support for more SPSS routines, see my github page for those that I am currently prioritizing. Let me know via the issues on github if you’d like support for a specific routine or run into problems.\nThe package home can be found on github: SPSStoR"
  },
  {
    "objectID": "posts/2014-04-02-aera-preview.html",
    "href": "posts/2014-04-02-aera-preview.html",
    "title": "AERA Preview",
    "section": "",
    "text": "The American Educational Research Association (AERA) annual conference is this weekend in Philadelphia. I was lucky to have a paper accepted into the conference. I am presenting a meta analysis that I have been working on for the past two years or so titled: Model misspecification and assumption violations with the linear mixed model: A meta analysis.\nIn this paper, I have compiled numerous monte carlo studies perform a quantitative synthesis of the literature. I have focused primarily on longitudinal linear mixed models as that was what my dissertation topic, and practically speaking, I already had many monte carlo studies in hand making the task a bit simpler.\nHere is a sneak peak of some of the results from my paper in the form of an interactive chart using the rChart package to get started. Here is my r code to generate the initial chart:\n\nlibrary(rCharts)\nh1 &lt;- hPlot(x = \"fitSerCor2\", y = \"avgt1e\", group = \"missRE\", data = intmean)\nh1$yAxis(title = list(text = \"Empirical Type I Error Rate\"), min = 0.00, max = 0.2, tickInterval = 0.05)\nh1$xAxis(title = list(text = \"Fitted Serial Correlation Structure\"),\n         categories = c(\"Ind\", \"AR1\", \"MA1\", \"MA2\", \"ARMA\"))\nh1$legend(verticalAlign = \"top\", align = \"right\", layout = \"vertical\", title = list(text = \"Miss RE\"))\nh1$print('chart1', include_assets = TRUE, cdn = TRUE)\n\nAs in one of my prior posts about rCharts I manually added a few features to the Javascript manually. I find that easier than bundling lists upon lists to achieve the desired result. Below is the final image:\n\n\n\n\n\n\n\n\n\nIf anyone is attending AERA this year and wants to listen to my presentation as well as others dealing with Methodological Considerations in Modeling Latent Growth (the title of the session) stop by the Convention Center on Sunday, April 6th from 4:05 to 5:35 pm in room 117. Even if you do not want to hear about modeling latent growth, but would rather talk about r, perhaps we can meetup somewhere else."
  },
  {
    "objectID": "posts/2014-01-06-two-of-my-favorite-datatable-features.html",
    "href": "posts/2014-01-06-two-of-my-favorite-datatable-features.html",
    "title": "Two of my favorite data.table features",
    "section": "",
    "text": "When I started to use the data.table package I was primarily using it to aggregate. I had read about data.table and its blazing speed compared to the other options from base or the plyr package especially with large amounts of data. As an example, I remember calculating averages or percentages while at Saint Paul Public Schools and while the calculations were running would walk away for 5 minutes to wait for them to finish. When using data.table to do the same calculations I didn’t need to wait 5 minutes to see the calculated values.\nThe speed of data.table is publicized widely, however there are two features found in data.table that are not talked about as frequently that I use very often.\n\nAdd aggregated variables to the raw data file\nThe ability to add aggregated variables to the raw data file can be very helpful in numerous data situations. At Saint Paul Public Schools I used this feature to give differing levels of data to external clients requesting data. I also used this feature when creating graphics. Outside of the district world, this feature is extremely helpful when fitting linear mixed models with lme4 or nlme. Instead of creating a set of aggregated variables in a new data frame and merging back in, data.table allows you to do it one one command. Here is a small example:\n\n# generate a small dataset\nset.seed(1234)\nsmalldat &lt;- data.frame(group1 = rep(1:2, each = 5), \n                       group2 = rep(c('a','b'), times = 5), \n                       x = rnorm(10))\n\n# convert to data.frame to data.table\nlibrary(data.table)\nsmalldat &lt;- data.table(smalldat)\n\n# convert aggregated variable into raw data file\nsmalldat[, aggGroup1 := mean(x), by = group1]\n\n# print file\nsmalldat\n\n    group1 group2          x  aggGroup1\n     &lt;int&gt; &lt;char&gt;      &lt;num&gt;      &lt;num&gt;\n 1:      1      a -1.2070657 -0.3523537\n 2:      1      b  0.2774292 -0.3523537\n 3:      1      a  1.0844412 -0.3523537\n 4:      1      b -2.3456977 -0.3523537\n 5:      1      a  0.4291247 -0.3523537\n 6:      2      b  0.5060559 -0.4139612\n 7:      2      a -0.5747400 -0.4139612\n 8:      2      b -0.5466319 -0.4139612\n 9:      2      a -0.5644520 -0.4139612\n10:      2      b -0.8900378 -0.4139612\n\n# aggregate with 2 variables\nsmalldat[, aggGroup1.2 := mean(x), by = list(group1, group2)]\n\n# print file\nsmalldat\n\n    group1 group2          x  aggGroup1 aggGroup1.2\n     &lt;int&gt; &lt;char&gt;      &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n 1:      1      a -1.2070657 -0.3523537   0.1021667\n 2:      1      b  0.2774292 -0.3523537  -1.0341342\n 3:      1      a  1.0844412 -0.3523537   0.1021667\n 4:      1      b -2.3456977 -0.3523537  -1.0341342\n 5:      1      a  0.4291247 -0.3523537   0.1021667\n 6:      2      b  0.5060559 -0.4139612  -0.3102046\n 7:      2      a -0.5747400 -0.4139612  -0.5695960\n 8:      2      b -0.5466319 -0.4139612  -0.3102046\n 9:      2      a -0.5644520 -0.4139612  -0.5695960\n10:      2      b -0.8900378 -0.4139612  -0.3102046\n\n\nThe key part of the syntax is the :=, which tells data.table to compute an aggregated variable and merge it back into the original data. This is very compact syntax to create aggregated variables that are automatically placed within the original data. The only drawback is the inability to create more than one aggregated variable at a time. If I wanted to created the mean and the median of x for each level of the variable group1, I would have to write two commands. If a lot of variables need to be aggregated in this fashion, it may be more concise to create an aggregated data set and merge back into the original. Below is an example of what I mean by aggregate then merge back:\n\nlibrary(plyr)\n\n# create aggregated data\naggdat1 &lt;- ddply(smalldat, .(group1), summarize,\n                 aggGroup1plyr = mean(x))\naggdat12 &lt;- ddply(smalldat, .(group1, group2), summarize, \n                aggGroup1.1plyr = mean(x))\n\n# join back into data\nsmalldat &lt;- join(smalldat, aggdat1, by = 'group1')\nsmalldat &lt;- join(smalldat, aggdat12, by = c('group1', 'group2'))\n\n# print data\nsmalldat\n\n    group1 group2          x  aggGroup1 aggGroup1.2 aggGroup1plyr\n     &lt;int&gt; &lt;char&gt;      &lt;num&gt;      &lt;num&gt;       &lt;num&gt;         &lt;num&gt;\n 1:      1      a -1.2070657 -0.3523537   0.1021667    -0.3523537\n 2:      1      b  0.2774292 -0.3523537  -1.0341342    -0.3523537\n 3:      1      a  1.0844412 -0.3523537   0.1021667    -0.3523537\n 4:      1      b -2.3456977 -0.3523537  -1.0341342    -0.3523537\n 5:      1      a  0.4291247 -0.3523537   0.1021667    -0.3523537\n 6:      2      b  0.5060559 -0.4139612  -0.3102046    -0.4139612\n 7:      2      a -0.5747400 -0.4139612  -0.5695960    -0.4139612\n 8:      2      b -0.5466319 -0.4139612  -0.3102046    -0.4139612\n 9:      2      a -0.5644520 -0.4139612  -0.5695960    -0.4139612\n10:      2      b -0.8900378 -0.4139612  -0.3102046    -0.4139612\n    aggGroup1.1plyr\n              &lt;num&gt;\n 1:       0.1021667\n 2:      -1.0341342\n 3:       0.1021667\n 4:      -1.0341342\n 5:       0.1021667\n 6:      -0.3102046\n 7:      -0.5695960\n 8:      -0.3102046\n 9:      -0.5695960\n10:      -0.3102046\n\n\nThe above code using plyr likely won’t take anymore time for R to run, however it does take more time to write out the code.\n\n\nRemoving duplicate observations\nFor most situations, using data.table has become my go to way to remove duplicate observations. This is especially useful when it does not matter which value of the variables you want to keep in the final data set (e.g. when values of the variables are repeated). The ability of data.table to create keyed values makes it extremely easy to remove duplicate observations based on those keyed variables.\nUsing the dataset created above:\n\n# Set keys - this sorts the data based on these values\nsetkeyv(smalldat, c('group1','group2'))\n\n# keep unique observations (I also remove the variable x)\nuniqdat &lt;- subset(unique(smalldat), select = -x)\n\n# print data\nuniqdat\n\nKey: &lt;group1, group2&gt;\n    group1 group2  aggGroup1 aggGroup1.2 aggGroup1plyr aggGroup1.1plyr\n     &lt;int&gt; &lt;char&gt;      &lt;num&gt;       &lt;num&gt;         &lt;num&gt;           &lt;num&gt;\n 1:      1      a -0.3523537   0.1021667    -0.3523537       0.1021667\n 2:      1      a -0.3523537   0.1021667    -0.3523537       0.1021667\n 3:      1      a -0.3523537   0.1021667    -0.3523537       0.1021667\n 4:      1      b -0.3523537  -1.0341342    -0.3523537      -1.0341342\n 5:      1      b -0.3523537  -1.0341342    -0.3523537      -1.0341342\n 6:      2      a -0.4139612  -0.5695960    -0.4139612      -0.5695960\n 7:      2      a -0.4139612  -0.5695960    -0.4139612      -0.5695960\n 8:      2      b -0.4139612  -0.3102046    -0.4139612      -0.3102046\n 9:      2      b -0.4139612  -0.3102046    -0.4139612      -0.3102046\n10:      2      b -0.4139612  -0.3102046    -0.4139612      -0.3102046\n\n\nThe code above first sets two keys for the data.table. The key acts as an identifier and the data are automatically sorted based on the key variables. This is one of the reasons why the data.table package can be so fast at doing many of its tasks. Then unique observations are kept. The unique function in the data.table package is similar to the same function in the base package, but when keys are defined for a data.table, the unique function automatically selects unique observations based on those key variables.\nFor more complicated cases, I tend to use a combination of order and duplicated from base R, however for easy cases where observations are repeated on the variables I want to keep, this is a quick and easy way to remove duplicate observations."
  },
  {
    "objectID": "posts/2013-12-30-guessCorr.html",
    "href": "posts/2013-12-30-guessCorr.html",
    "title": "Guessing Correlations: A shiny app",
    "section": "",
    "text": "A recent hobby of mine (as with many other R users) is to play around with the relatively new R package: shiny. I started creating demo applications about a year ago just to figure out how powerful the platform could be, but it was not until this fall that I started creating some applications for others to use.\nI encountered a problem this fall at the University of Arkansas due to the huge Java exploit going public. As a result of this, Java was blocked on all of the computers in the classrooms that I used for my intro statistics courses. Most days this does not impact my lectures, however on a few days throughout the semester I use applets to help show certain concepts.\nEnter my solution, create shiny applications that attempts to mimic many of the same features found in the Java applets using shiny. The application I’ve been working on lately is an application that allows users to estimate the correlation based on a scatterplot of data.\nYou can run the shiny application from my github page using the following commands (make sure you have the shiny package installed first:\n\nshiny::runGitHub(repo = \"shinyApps\", username = \"lebebr01\", subdir = \"guessCorr\")\n\nThis should open a session in your browser that should look like the following: \nAs you can see from this screenshot, there is a scatterplot in the main window and the user inputs a guess into the text box. When they hit the ‘Submit Guess’ button, the grey panel above the scatterplot updates to give hints about the direction the actual correlation is compared to the guess. Once the user is within .05 (.05 above or below) the actual correlation, the actual correlation is printed in the top text box.\nThe app also includes the ability to restrict the range of the scatterplot. This can be seen in the image below (and can be done on the app by clicking the ‘Restriction of Range’ checkbox): \nWhen that checkbox is clicked, a new slider shows up that restricts the range of the original scatterplot. The data that lies outside of the restricted range shows up as a light grey in the updated graph. Now the user attempts to guess the correlation for the restricted range data. The correlation for the entire data can be seen in the graph.\nThe one thing I was unable to implement was a second button that allows the user to click it to refresh the data to guess at another correlation. The ‘Submit Button’ in shiny is too cumbersome and I could not get two ‘Action Buttons’ to work side by side, although this may be possible. Lastly, adding the option for a counter as an indication of how well the user is doing could make it an interesting contest in the classroom."
  },
  {
    "objectID": "posts/2025-09-24-simglm-propensity.html",
    "href": "posts/2025-09-24-simglm-propensity.html",
    "title": "simglm v0.9.22: Propensity Score Simulation",
    "section": "",
    "text": "I haven’t written about my simglm package in a while. Behind the scenes, numerous changes have been made to enhance the package. The current CRAN version is significantly behind, but my goal is to submit the most recent version to CRAN soon, once I finalize the details for my upcoming book on simulation and power analysis with R, published by CRC Press.\nOne of the final implementations required for this book is conducting a simulation and power analysis for propensity scores. Propensity scores can be helpful to adjust for observed/collected data when randomization was not done or was not feasible. Propensity scores are used to estimate the probability that a data unit is included in the treatment condition using a function of observed data attributes. This procedure can help adjust, statistically, for differences in data attributes across the treatment and control groups.\nFor example, we would not be able to assign people to smoke randomly, so instead, we need to collect information from people who already smoke compared to those who do not smoke. Due to people self-selecting to smoke or not, these groups would likely differ, which would make the impact of any potential health outcomes vary due to baseline differences. Propensity scores help balance the groups based on observed characteristics before any intervention.\nWith these simulation tools, you can now not only model realistic quasi-experimental data but also assess how much sample size you need to detect treatment effects reliably."
  },
  {
    "objectID": "posts/2025-09-24-simglm-propensity.html#simglm-simulation-for-propensity-scores",
    "href": "posts/2025-09-24-simglm-propensity.html#simglm-simulation-for-propensity-scores",
    "title": "simglm v0.9.22: Propensity Score Simulation",
    "section": "simglm simulation for propensity scores",
    "text": "simglm simulation for propensity scores\nIn the most recent version of the simglm package on GitHub, support for simulating data using propensity scores is now available. The simulation procedure takes a two-step approach.\n\nStep 1: Simulate the group membership using logistic regression as a function of some data attributes.\nStep 2: Simulate any additional attributes and incorporate the group membership from step 1. Step 1 is essential, as now the group membership would not be randomly assigned; instead, whether someone is simulated to be in the treatment group would depend on the attributes specified in step 1.\n\nFirst, ensure that you install the updated simglm package from GitHub, along with the remotes package.\n\nremotes::install_github('lebebr01/simglm')\n\nHere is an example of how step 1 can be specified within the simglm package. A new named element to the simulation arguments, called “propensity,” specifies the components used to generate the group membership. The required elements in here are similar to those from the Tidy Simulation vignette. It is essential to define the attributes that distinguish a person’s inclusion in the treatment or control group using the model formula. In the example below, age and socio-economic status (SES) are the two data attributes being used to define who belongs in the treatment or control group. The regression weights argument specifies the relationship between being in the treatment or control group.\nFinally, with the outcome_type = 'binary' specification, logistic regression is used to estimate the propensity scores and generate the probability of being in the treatment or control group.\n\nlibrary(simglm)\n\nsim_arguments &lt;- list(\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 250,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, 0.15),\n        outcome_type = 'binary'\n    )\n)\n\nprop_data &lt;- simulate_propensity(sim_arguments)\n\nOnce we simulate the propensity data, we can visualize the balance between the treatment and control groups by the two data attributes, age and SES.\n\nAge balance\nFirst, examining the balance by age reveals that individuals in the treatment group tend to be older than those in the control group. These are differences that may be meaningful and impact the estimation of the actual treatment effect.\n\nlibrary(ggformula)\n\nprop_data |&gt; \n    gf_density(~ age, fill = ~factor(trt)) |&gt;\n    gf_labs(fill = \"Treatment\")\n\n\n\n\n\n\n\n\n\n\nSES balance\nThe SES balance is again in the positive direction, with the treatment group coming from slightly higher SES backgrounds compared to those in the control group. Without adjusting for this, the treatment effect would likely be biased.\n\nprop_data |&gt; \n    gf_density(~ ses, fill = ~factor(trt)) |&gt;\n    gf_labs(fill = \"Treatment\")"
  },
  {
    "objectID": "posts/2025-09-24-simglm-propensity.html#full-data-simulation-with-propensity-scores",
    "href": "posts/2025-09-24-simglm-propensity.html#full-data-simulation-with-propensity-scores",
    "title": "simglm v0.9.22: Propensity Score Simulation",
    "section": "Full data simulation with propensity scores",
    "text": "Full data simulation with propensity scores\nBelow is an example that incorporates a full data simulation with the treatment group generated from the propensity score model (i.e., logistic regression). In this hypothetical example, we aim to model student achievement as a function of motivation, the treatment group (generated via propensity scores), age, and SES. The simulated treatment effect was specified as 1.2. This effect was generated not to be a standardized effect size here, as this is entirely hypothetical; however, it is often easiest to specify effects as standardized effect sizes.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 250,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 250,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    )\n)\nsim_data &lt;- simulate_fixed(data = NULL, sim_args = sim_arguments) |&gt;\n    simulate_error(sim_args = sim_arguments) |&gt;\n    generate_response(sim_arguments)\n\nDT::datatable(sim_data)\n\n\n\n\n\nThe simulation process uses the treatment, age, and SES data generated from the propensity score process. That is, these attributes are not regenerated in the process, but are simulated first, then any additional fixed effects are simulated to work through the simulation pipeline."
  },
  {
    "objectID": "posts/2025-09-24-simglm-propensity.html#estimate-propensity-models-with-simglm",
    "href": "posts/2025-09-24-simglm-propensity.html#estimate-propensity-models-with-simglm",
    "title": "simglm v0.9.22: Propensity Score Simulation",
    "section": "Estimate propensity models with simglm",
    "text": "Estimate propensity models with simglm\nThe model estimation process is done similarly to the simulation process. Two models are estimated via a two-step process.\n\nStep 1: A propensity score model is estimated to estimate the probability of being in the treatment group. This is controlled via the propensity_model simulation arguments below.\nStep 2: A full model is estimated, specified via the model_fit simulation arguments. The propensity scores can be included in three different ways via the “propensity_type” simulation argument:\n\ncovariate: The propensity score probabilities are included as a covariate in the final model.\nipw: Inverse propensity score weighting is used and passed to the weights argument of the model.\nsbw: Stabilized propensity score weighting is used and passed to the weights arguments of the model. The stabilized weights use the mean of the propensity scores instead of 1, which can help limit weights that are excessively large or small.\n\n\n\nCovariate-adjusted model\nThe covariate-adjusted model includes the propensity score as a covariate in the final model via the propensity_type = 'covariate' argument.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'covariate'\n    )\n)\nsimulate_fixed(data = NULL, sim_args = sim_arguments) |&gt;\n  simulate_error(sim_args = sim_arguments) |&gt;\n  generate_response(sim_arguments)|&gt; \n  fit_propensity(sim_arguments) |&gt;\n  model_fit(sim_arguments) |&gt;\n  extract_coefficients()\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   51.1     0.912       56.0  6.71e-310\n2 motivation     0.402   0.00480     83.7  0        \n3 trt            1.34    0.257        5.21 2.30e-  7\n4 age            0.145   0.0383       3.79 1.58e-  4\n5 ses            0.208   0.0582       3.57 3.75e-  4\n6 propensity    -1.75    1.31        -1.34 1.80e-  1\n\n\n\n\nInverse Propensity Score Weighting\nTo specify inverse propensity score weighting, propensity_type = 'ipw' is specified. Note that the propensity scores are no longer included as a covariate.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'ipw'\n    )\n)\nsimulate_fixed(data = NULL, sim_args = sim_arguments) |&gt;\n  simulate_error(sim_args = sim_arguments) |&gt;\n  generate_response(sim_arguments)|&gt; \n  fit_propensity(sim_arguments) |&gt;\n  model_fit(sim_arguments) |&gt;\n  extract_coefficients()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   49.9     0.203      246.   0       \n2 motivation     0.406   0.00510     79.7  0       \n3 trt            1.12    0.253        4.43 1.03e- 5\n4 age            0.129   0.0241       5.35 1.07e- 7\n5 ses            0.272   0.0226      12.0  3.50e-31\n\n\n\n\nStabilized Propensity Score Weighting\nTo specify stabilized propensity score weighting, propensity_type = 'sbw' is specified. Note that the propensity scores are no longer included as a covariate.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'sbw'\n    )\n)\nsimulate_fixed(data = NULL, sim_args = sim_arguments) |&gt;\n  simulate_error(sim_args = sim_arguments) |&gt;\n  generate_response(sim_arguments)|&gt; \n  fit_propensity(sim_arguments) |&gt;\n  model_fit(sim_arguments) |&gt;\n  extract_coefficients()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  50.2      0.206      244.   0       \n2 motivation    0.402    0.00528     76.1  0       \n3 trt           1.03     0.259        3.99 6.99e- 5\n4 age           0.0942   0.0248       3.80 1.54e- 4\n5 ses           0.239    0.0232      10.3  1.35e-23\n\n\nEach of these approaches has the potential to produce a more balanced comparison between the treatment and control groups for observed data attributes. The primary limitation of propensity scores compared to actual random assignment is that they cannot adjust for any differences in data attributes that are unobserved. True random assignment can help ensure that the treatment and control groups are balanced with respect to observed and unobserved data attributes. Even with this limitation, propensity scores are preferable to doing nothing when random assignment is not possible or cannot be performed."
  },
  {
    "objectID": "posts/2025-09-24-simglm-propensity.html#next-steps",
    "href": "posts/2025-09-24-simglm-propensity.html#next-steps",
    "title": "simglm v0.9.22: Propensity Score Simulation",
    "section": "Next Steps",
    "text": "Next Steps\nThe next step is to ensure that these propensity score simulation and model estimation steps are included in the broader functions to perform replications. This will enable the estimation of power for planning a study that aims to implement propensity scores.\nIf you work with non-randomized designs, simglm can now help you simulate realistic scenarios, test different adjustment methods, and plan studies with more confidence."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Educate-R",
    "section": "",
    "text": "My First Tidy Tuesday!\n\n\n\nR\n\nTidy Tuesday\n\nVisualization\n\n\n\n\n\n\n\n\n\nOct 7, 2025\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nsimglm v0.9.23: Propensity Score Modeling\n\n\n\nR\n\nsimglm\n\nmodeling\n\npropensity scores\n\n\n\n\n\n\n\n\n\nOct 6, 2025\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nsimglm v0.9.22: Propensity Score Simulation\n\n\n\nR\n\nsimglm\n\nsimulation\n\npropensity scores\n\n\n\n\n\n\n\n\n\nSep 24, 2025\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nsimglm v0.7.4: Tidy Simulation\n\n\n\nR\n\nSimglm\n\nSimulation\n\nTidy\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\npdfsearch v0.3.0\n\n\n\nR\n\nPdfsearch\n\nPdf\n\n\n\n\n\n\n\n\n\nAug 22, 2018\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Cluster Analysis of Coach ELO Ratings\n\n\n\nR\n\nCollege Football\n\nCluster Analysis\n\n\n\n\n\n\n\n\n\nDec 4, 2017\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing simglm version 0.6.0: Power and Simulation of Generalized Linear Mixed Models\n\n\n\nR\n\nSimglm\n\nPackage\n\n\n\n\n\n\n\n\n\nJul 24, 2017\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nsimglm submission to CRAN this week\n\n\n\nR\n\nSimglm\n\nPackage\n\n\n\n\n\n\n\n\n\nMay 23, 2017\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nUse CSS to format markdown or HTML files\n\n\n\nR\n\nPackage\n\nHighlighthtml\n\nMarkdown\n\nHtml\n\n\n\n\n\n\n\n\n\nJan 3, 2017\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction of the pdfsearch package\n\n\n\nR\n\nPdfsearch\n\nPackage\n\n\n\n\n\n\n\n\n\nDec 2, 2016\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed test of sequence generation for unbalanced simulation\n\n\n\nR\n\nSequence\n\nSpeedtest\n\n\n\n\n\n\n\n\n\nMay 31, 2015\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nRemove leading 0 with ggplot2\n\n\n\nR\n\nGraphics\n\nGgplot2\n\n\n\n\n\n\n\n\n\nMar 23, 2015\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nDodged bar charts, why not a line graph?\n\n\n\nR\n\nGraphics\n\nGgplot2\n\nTrend\n\n\n\n\n\n\n\n\n\nAug 5, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nFormat Markdown Documents in R\n\n\n\nR\n\nHighlighthtml\n\nPackage\n\nCss\n\nMarkdown\n\nHtml\n\n\n\n\n\n\n\n\n\nJul 30, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nAERA Preview\n\n\n\nR\n\nRcharts\n\nJavascript\n\nAera\n\n\n\n\n\n\n\n\n\nApr 2, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of Code\n\n\n\nR\n\nCode\n\nXml\n\n\n\n\n\n\n\n\n\nMar 27, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate to highlightHTML package\n\n\n\nR\n\nHighlighthtml\n\nMarkdown\n\nHtml\n\n\n\n\n\n\n\n\n\nMar 14, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nrCharts in slidy\n\n\n\nR\n\nRcharts\n\nJavascript\n\nHtml\n\nSlidy\n\n\n\n\n\n\n\n\n\nMar 3, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nPicking a gui interface for R\n\n\n\nR\n\nGui\n\n\n\n\n\n\n\n\n\nFeb 2, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nWhen I use plyr/dplyr\n\n\n\nR\n\nPlyr\n\nDplyr\n\n\n\n\n\n\n\n\n\nJan 24, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nTwo of my favorite data.table features\n\n\n\nR\n\nData.table\n\n\n\n\n\n\n\n\n\nJan 6, 2014\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nGuessing Correlations: A shiny app\n\n\n\nR\n\nShiny\n\nTeaching\n\n\n\n\n\n\n\n\n\nDec 30, 2013\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nSPSStoR An R package to convert SPSS syntax\n\n\n\nR\n\nSpss\n\nPackage\n\n\n\n\n\n\n\n\n\nNov 26, 2013\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting Markdown Table with R\n\n\n\nR\n\nMarkdown\n\nLatex\n\nSlidy\n\n\n\n\n\n\n\n\n\nNov 18, 2013\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting Markdown Table with R\n\n\n\nR\n\nMarkdown\n\nCss\n\n\n\n\n\n\n\n\n\nNov 1, 2013\n\n\nBrandon LeBeau\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Logo\n\n\n\nR\n\nggplot2\n\n\n\n\n\n\n\n\n\nMay 3, 2013\n\n\nBrandon LeBeau\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pdfsearch-0-3-0.html",
    "href": "posts/pdfsearch-0-3-0.html",
    "title": "pdfsearch v0.3.0",
    "section": "",
    "text": "I’m happy to formally announce a major update to the developmental version of the pdfsearch R package. In brief, this version includes support for splitting the PDF by sentences instead of by lines of text. Secondly, initial testing of splitting PDFs that are aligned in multiple columns has been promising. This functionality attempts to align the multiple columns into a single column in which the keyword searching peformed by pdfsearch can be stronger with the search being done in context. Both of these new additions will be explored in more detail below."
  },
  {
    "objectID": "posts/pdfsearch-0-3-0.html#split-pdfs-into-sentences",
    "href": "posts/pdfsearch-0-3-0.html#split-pdfs-into-sentences",
    "title": "pdfsearch v0.3.0",
    "section": "Split PDFs into sentences",
    "text": "Split PDFs into sentences\nWe will use the following PDF of a paper found on arXiv by Schifeling, Reiter, and DeYoreo called Data Fusion for Correcting Measurement Errors paper here. Before version 0.3.0 of pdfsearch, the pdf was only split into individual lines, see this blog post for more examples of pdfsearch. To be more concrete, let’s see what that would look like for a search looking for the term “measurement error”. For this the keyword_search function can be used to parse the PDF to text using the pdftools package.\n\n# load the package\nlibrary(pdfsearch)\n\npdf_url &lt;- \"https://arxiv.org/pdf/1610.00147.pdf\"\nsearch_result &lt;- keyword_search(pdf_url, keyword = c('measurement error'),\n  path = TRUE, remove_hyphen = TRUE, convert_sentence = FALSE)\n\n# Collapse to view results\nhead(data.frame(search_result))\n\n            keyword page_num line_num\n1 measurement error        1        5\n2 measurement error        1       19\n3 measurement error        1       21\n4 measurement error        2       28\n5 measurement error        2       31\n6 measurement error        2       32\n                                                                                      line_text\n1                 Often in surveys, key items are subject to measurement errors. Given just the\n2            the sensitivity of various analyses to different choices for the measurement error\n3                            KEY WORDS: fusion, imputation, measurement error, missing, survey.\n4          Survey data often contain items that are subject to measurement errors. For example,\n5 these measurement errors can result in degraded inferences (Kim et al., 2015). Unfortunately,\n6                the distribution of the measurement errors typically is not estimable from the\n                                                                                           token_text\n1             often, in, surveys, key, items, are, subject, to, measurement, errors, given, just, the\n2       the, sensitivity, of, various, analyses, to, different, choices, for, the, measurement, error\n3                                 key, words, fusion, imputation, measurement, error, missing, survey\n4      survey, data, often, contain, items, that, are, subject, to, measurement, errors, for, example\n5 these, measurement, errors, can, result, in, degraded, inferences, kim, et, al, 2015, unfortunately\n6           the, distribution, of, the, measurement, errors, typically, is, not, estimable, from, the\n\n\nThe results show the keyword, the page number and line number where the match was found, the specific line of text that contains the search result, and the line of text tokenized into individual words. You may notice from the line_text output that the search result may be multiple sentences or a partial sentence instead of a full sentence. What the pdfsearch package was doing was taking the text in a single line of the PDF file and using that as the search string. One limitation of this approach is when phrases instead of single words are searched, if the phrase wraps to more than one line, the old version of pdfsearch would always miss these keywords.\nAs a side note, you may have noticed that both “measurement error” and “measurement errors” are returned (look at the first match above). If we did not want to include the “s” in the match, we would need to add some regular expression to omit the “s” using negative look ahead (there are likely other ways to omit the “s” as well).\n\nsearch_result_nos &lt;- keyword_search(pdf_url, keyword = c('measurement error(?!s)'),\n  path = TRUE, remove_hyphen = TRUE, convert_sentence = FALSE)\n\n# Collapse to view results\nhead(data.frame(search_result_nos))\n\n                 keyword page_num line_num\n1 measurement error(?!s)        1       19\n2 measurement error(?!s)        1       21\n3 measurement error(?!s)        4       82\n4 measurement error(?!s)        4       84\n5 measurement error(?!s)        4       89\n6 measurement error(?!s)        4      100\n                                                                              line_text\n1    the sensitivity of various analyses to different choices for the measurement error\n2                    KEY WORDS: fusion, imputation, measurement error, missing, survey.\n3         the general framework for specifying measurement error models to leverage the\n4            measurement error in educational attainment in the 2010 American Community\n5 to different measurement error model specifications. In Section 5, we provide a brief\n6                                             attainment is prone to measurement error.\n                                                                                       token_text\n1   the, sensitivity, of, various, analyses, to, different, choices, for, the, measurement, error\n2                             key, words, fusion, imputation, measurement, error, missing, survey\n3         the, general, framework, for, specifying, measurement, error, models, to, leverage, the\n4             measurement, error, in, educational, attainment, in, the, 2010, american, community\n5 to, different, measurement, error, model, specifications, in, section, 5, we, provide, a, brief\n6                                                   attainment, is, prone, to, measurement, error\n\n\n\nSplitting by sentences\nThe new default behavior in pdfsearch is to split the text into separate sentences instead of by text lines in the PDF document. Let’s do the first search that includes the “s” in the search, but now instead of splitting by lines split the text into sentences.\n\nsearch_result_sent &lt;- keyword_search(pdf_url, keyword = c('measurement error'),\n  path = TRUE, remove_hyphen = TRUE, convert_sentence = TRUE)\n\nnrow(search_result_sent)\n\n[1] 36\n\nnrow(search_result)\n\n[1] 31\n\n\nYou’ll notice now that an additional 5 search results were returned. One note, currently the line_num output in both do not match, in the future these will match and when sentences are returned a new output variable representing sentence number will also be returned. We can view the first few results to see how the line_text output differs from before. Internally, stringi::stri_split_boundaries(text_lines, type = \"sentence\") is used to split the PDF text into sentences instead of lines of text.\n\nhead(data.frame(search_result_sent))\n\n            keyword page_num line_num\n1 measurement error        1        2\n2 measurement error        1       10\n3 measurement error        1       12\n4 measurement error        2       16\n5 measurement error        2       18\n6 measurement error        2       19\n                                                                                                                                line_text\n1 Reiter, Maria DeYoreo∗ arXiv:1610.00147v1 [stat.ME] 1 Oct 2016 Abstract Often in surveys, key items are subject to measurement errors. \n2     We also present a process for assessing the sensitivity of various analyses to different choices for the measurement error models. \n3                                                                     KEY WORDS: fusion, imputation, measurement error, missing, survey. \n4                                             1     Introduction Survey data often contain items that are subject to measurement errors. \n5                                       Left uncorrected, these measurement errors can result in degraded inferences (Kim et al., 2015). \n6                       Unfortunately, the distribution of the measurement errors typically is not estimable from the survey data alone. \n                                                                                                                                            token_text\n1  reiter, maria, deyoreo, arxiv, 1610.00147v1, stat.me, 1, oct, 2016, abstract, often, in, surveys, key, items, are, subject, to, measurement, errors\n2 we, also, present, a, process, for, assessing, the, sensitivity, of, various, analyses, to, different, choices, for, the, measurement, error, models\n3                                                                                  key, words, fusion, imputation, measurement, error, missing, survey\n4                                                    1, introduction, survey, data, often, contain, items, that, are, subject, to, measurement, errors\n5                                              left, uncorrected, these, measurement, errors, can, result, in, degraded, inferences, kim, et, al, 2015\n6                        unfortunately, the, distribution, of, the, measurement, errors, typically, is, not, estimable, from, the, survey, data, alone"
  },
  {
    "objectID": "posts/pdfsearch-0-3-0.html#splitting-multi-column-pdf-documents",
    "href": "posts/pdfsearch-0-3-0.html#splitting-multi-column-pdf-documents",
    "title": "pdfsearch v0.3.0",
    "section": "Splitting Multi-Column PDF Documents",
    "text": "Splitting Multi-Column PDF Documents\nThe next major upgrade is the default behavior for splitting PDF documents that are aligned in multiple columns. One example of this can be found in a paper on arXiv by Guo and Deng called Flexible Online Repeated Measures Experiment paper here.\nLet’s first look at the output of a search without splitting the PDF.\n\npdf_url &lt;- \"https://arxiv.org/pdf/1501.00450.pdf\"\n\nno_split &lt;- keyword_search(pdf_url, keyword = c('repeated measures', 'mixed effects'),\n                           path = TRUE, remove_hyphen = FALSE, \n                           convert_sentence = FALSE)\n\nhead(data.frame(no_split))\n\n            keyword page_num line_num\n1 repeated measures        1       24\n2 repeated measures        2       57\n3 repeated measures        2      108\n4 repeated measures        2      110\n5 repeated measures        2      125\n6 repeated measures        6      454\n                                                                                                                               line_text\n1 cally the repeated measures design, including the crossover           get false confidence about lack of negative effects. Statistical\n2          fast iterations and testing many ideas can reap the most           erations to repeated measures design, with variants to the\n3          repeated measures design in different stages of treatment          in this section we assume all users appear in all periods,\n4            ing the repeated measures analysis, reporting a “per week”         to metrics that are defined as simple average and assume\n5            In fact, the crossover design is a type of repeated measures       designs considered can be examined in the same framework\n6         values and the absence in a specific time window can still          It is common to analyze data from repeated measures design\n                                                                                                                                  token_text\n1 cally, the, repeated, measures, design, including, the, crossover, get, false, confidence, about, lack, of, negative, effects, statistical\n2       fast, iterations, and, testing, many, ideas, can, reap, the, most, erations, to, repeated, measures, design, with, variants, to, the\n3      repeated, measures, design, in, different, stages, of, treatment, in, this, section, we, assume, all, users, appear, in, all, periods\n4         ing, the, repeated, measures, analysis, reporting, a, per, week, to, metrics, that, are, defined, as, simple, average, and, assume\n5    in, fact, the, crossover, design, is, a, type, of, repeated, measures, designs, considered, can, be, examined, in, the, same, framework\n6  values, and, the, absence, in, a, specific, time, window, can, still, it, is, common, to, analyze, data, from, repeated, measures, design\n\n\nYou’ll notice from the output that again the lines of text have a gap in the middle representing the white space between the multi-column layout. What the new split_pdf argument behavior attempts to recreate is the document structure that a reader would perform, namely read the left column first followed by the right column. A specific note, the split_pdf argument has only been tested on two column PDF layouts.\nLet’s now run the same search with splitting the PDF and also converting into sentences. This is done by setting the argument split_pdf = TRUE.\n\nsplit_yes &lt;- keyword_search(pdf_url, keyword = c('repeated measures', 'mixed effects'),\n                           path = TRUE, remove_hyphen = TRUE, split_pdf = TRUE,\n                           convert_sentence = TRUE)\n\nhead(data.frame(split_yes))\n\n            keyword page_num line_num\n1 repeated measures        1        5\n2 repeated measures        2       40\n3 repeated measures        2       42\n4 repeated measures        2       43\n5 repeated measures        2       50\n6 repeated measures        2       51\n                                                                                                                                                                                                                                  line_text\n1 We introduce more sophisticated experimental designs, specifically the repeated measures design, including the crossover design and related variants, to increase KPI sensitivity with the same traffic size and duration of experiment. \n2                                                                                                                        CUPED is in fact a form of repeated measures design, where multiple mea on the same subjects are taken over time. \n3                Groups A/B Test CUPED Parallel Crossover Re-Randomized Table 1: Repeated Measures Designs In this paper we extend the idea further by employing the repeated measures design in different stages of treatment assignment. \n4                                                                      The traditional A/B test can be analyzed using the repeated measures analysis, report a “per week” treatment effect, as show in row 3 “parallel” design in table 1. \n5                                                                                        In fact, the crossover design is a type of repeated measures design commonly used in biomedical research to control for within-subject variation. \n6                                     We also discuss practical considerations to repeated measures design, with variants to the crossover design to study the carry over effect, including the “re-randomized” design (row 5 in table 1). \n                                                                                                                                                                                                                                                          token_text\n1 we, introduce, more, sophisticated, experimental, designs, specifically, the, repeated, measures, design, including, the, crossover, design, and, related, variants, to, increase, kpi, sensitivity, with, the, same, traffic, size, and, duration, of, experiment\n2                                                                                                                                cuped, is, in, fact, a, form, of, repeated, measures, design, where, multiple, mea, on, the, same, subjects, are, taken, over, time\n3           groups, a, b, test, cuped, parallel, crossover, re, randomized, table, 1, repeated, measures, designs, in, this, paper, we, extend, the, idea, further, by, employing, the, repeated, measures, design, in, different, stages, of, treatment, assignment\n4                                                                           the, traditional, a, b, test, can, be, analyzed, using, the, repeated, measures, analysis, report, a, per, week, treatment, effect, as, show, in, row, 3, parallel, design, in, table, 1\n5                                                                                              in, fact, the, crossover, design, is, a, type, of, repeated, measures, design, commonly, used, in, biomedical, research, to, control, for, within, subject, variation\n6                                        we, also, discuss, practical, considerations, to, repeated, measures, design, with, variants, to, the, crossover, design, to, study, the, carry, over, effect, including, the, re, randomized, design, row, 5, in, table, 1\n\n\nIf you look at the text returned in the line_text output, the sentences now appear to make sense, are in context, and phrases of text are more likely to be returned if they happen to wrap to another line. The limitation of wrapping text is more pronounced in multiple column PDFs as there is much less text on a single line in each column of the PDF. Therefore, I think the search results should be more representative of the text in the document and should return better search results."
  },
  {
    "objectID": "posts/pdfsearch-0-3-0.html#next-steps",
    "href": "posts/pdfsearch-0-3-0.html#next-steps",
    "title": "pdfsearch v0.3.0",
    "section": "Next Steps",
    "text": "Next Steps\nA few next steps prior to release to CRAN these new additions.\n\nMore testing of the split_pdf pdfsearch behavior.\nExploration of automatic detection of two column PDFs to split automatically.\nVerify page and line number output when splitting into sentences instead of lines of text.\n\nIf others test out the developmental version of the package, I’d welcome collaborators or testers of the new functionality. If you have issues, please log these through GitHub Issues or have any contributions through Pull Requests."
  },
  {
    "objectID": "posts/2015-03-23-remove-leading-0-with-ggplot2.html",
    "href": "posts/2015-03-23-remove-leading-0-with-ggplot2.html",
    "title": "Remove leading 0 with ggplot2",
    "section": "",
    "text": "I recently had an occasion while working on a three variable interaction plot for a paper where I wanted to remove the leading 0’s in the x-axis text labels using ggplot2. This was primarily due to some space concerns I had for the x-axis labels. Unfortunately, I did not find an obvious way to do this in my first go around. After tickering a bit, I’ve found a workaround. The process is walked through below.\nFirst, some simulated data.\n\n# Sim some data\nsimdata &lt;- data.frame(x = runif(2400, min = .032, max = .210),\n                      y = c(rnorm(2000, mean = 0, sd = .1), \n                            rnorm(400, mean = 1, sd = .25)),\n                      group = c(sample(1:2, 1600, replace = TRUE),\n                                rep(1, 400), \n                                rep(2, 400)),\n                      facet = rep(1:3, each = 800))\n\nAs shown below, initially there is no group differences, but there are facet differences. Exploring the interaction between the grouping variables shows there is a two variable interaction. Note: This example is not identical to the three variable interaction I originally described above, but assume here that the x variable is also important.\n\nwith(simdata, tapply(y, group, mean))\n\n          1           2 \n0.001475125 0.342294443 \n\nwith(simdata, tapply(y, facet, mean))\n\n            1             2             3 \n-0.0004435282  0.0016667522  0.4948340175 \n\nwith(simdata, tapply(y, interaction(group, facet), mean))\n\n          1.1           2.1           1.2           2.2           1.3 \n 0.0057579127 -0.0072293980  0.0027065206  0.0004704595 -0.0043179816 \n          2.3 \n 0.9939860166 \n\n\nIn the example in the paper, I aggregated the unique x values to the third decimal place. That is done with the following dplyr code. Note: The data did not need to be aggregated, but it is a bit easier to work with when plotting later.\n\n# round value to .001 and aggregate\nsimdata$x_rd &lt;- round(simdata$x, 3)\n\n# aggregate\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsimdata_agg &lt;- simdata %&gt;%\n  group_by(x_rd, group, facet) %&gt;%\n  summarise(y = mean(y))\n\n`summarise()` has grouped output by 'x_rd', 'group'. You can override using the\n`.groups` argument.\n\nsimdata_agg \n\n# A tibble: 950 × 4\n# Groups:   x_rd, group [358]\n    x_rd group facet        y\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1 0.032     1     1 -0.156  \n 2 0.032     1     2  0.0546 \n 3 0.032     2     1 -0.0216 \n 4 0.032     2     2 -0.0949 \n 5 0.033     1     1 -0.0937 \n 6 0.033     1     3 -0.0118 \n 7 0.033     2     1  0.0271 \n 8 0.033     2     2 -0.0677 \n 9 0.033     2     3  1.16   \n10 0.034     1     1 -0.00662\n# ℹ 940 more rows\n\n\nNow that the data is aggregated, it can be directly plotted with ggplot2. This is the base plot that contains the leading 0’s by default and treats the x variable as continuous (which it really is continuous).\n\nlibrary(ggplot2)\np &lt;- ggplot(simdata_agg, aes(x = x_rd, y = y, shape = factor(group))) + \n  theme_bw()\np + geom_point(size = 3) + facet_grid(. ~ facet) + \n  scale_x_continuous(\"x\", limits = c(0, .25), \n                     breaks = seq(0, .25, .05))\n\n\n\n\n\n\n\n\nThe plot above is a good start, but I was worried about the x-axis labels being too close together and ultimately being difficult to read. I decided I wanted to omit the leading 0’s to omit some space. This was useful in my scenario as the variable on the x-axis could only take on values between 0 and 1, therefore the leading 0 is not important.\nOne way to remove the leading 0 is to convert the continuous variable into a character variable and use a simple regular expression (with gsub) to remove the 0 at the beginning of the character string. Below is the code to do that and also the resulting plot. The key point of the plotting code below is the use of the breaks argument to scale_x_discrete. Without this all the unique character values will be plotted, not good.\n\nsimdata_agg$x_char &lt;- as.character(simdata_agg$x_rd)\nsimdata_agg$x_char &lt;- gsub(\"^0\", \"\", simdata_agg$x_char)\np &lt;- ggplot(simdata_agg, aes(x = x_char, y = y, shape = factor(group))) + \n  theme_bw()\np + geom_point(size = 3) + facet_grid(. ~ facet) + \n  scale_x_discrete(\"x\", breaks = c('.00', '.05', '.1', '.15', '.2', '.25'))\n\n\n\n\n\n\n\n\nThe plot above has a few flaws. First, there are values at the edge of each facet. This could be fixed with the expand argument to scale_x_discrete, but I still wanted to include the value of .00 on the x-axis. Secondly, the x-axis text labels are not uniformly formatted which is not ideal (e.g. .1 should be .10).\nTo fix this, some made up data needs to be added to the data frame. Some care needs to be done here as well as a value of .00 can not just be added to the x variable plotted. This would place a non-uniform gap between .00 and .05 (not shown, but try it for yourself by adapting the code below). Therefore, all values between 0 and .031 need to be manually added to the data frame to keep the spacing uniform. Finally, to not plot the made up values, I created a transparency variable called alpha. This variable was used to set the alpha values to 0 for the made up values and 1 for the real values. scale_alpha_discrete was used to specify the range of alpha values, this is important otherwise the made up numbers will show up as a light gray. The final code to manually add the new data is shown below. Anyone have a less workaround procedure?\n\n# Reset aggregation vector\nsimdata_agg &lt;- simdata %&gt;%\n  group_by(x_rd, group, facet) %&gt;%\n  summarise(y = mean(y))\n\n`summarise()` has grouped output by 'x_rd', 'group'. You can override using the\n`.groups` argument.\n\n# Create x_char variable again\nsimdata_agg$x_char &lt;- as.character(simdata_agg$x_rd)\nsimdata_agg$x_char &lt;- gsub(\"^0\", \"\", simdata_agg$x_char)\n\n# Needed formatting\nsimdata_agg$x_char &lt;- ifelse(simdata_agg$x_char == '', '.00',\n                             simdata_agg$x_char)\nsimdata_agg$x_char &lt;- ifelse(simdata_agg$x_char == '.2', '.20',\n                             simdata_agg$x_char)\nsimdata_agg$x_char &lt;- ifelse(simdata_agg$x_char == '.1', '.10',\n                             simdata_agg$x_char)\n\n# Final plot\np &lt;- ggplot(simdata_agg, aes(x = x_char, y = y, shape = factor(group))) + \n  theme_bw()\np + geom_point(size = 3) + \n  facet_grid(. ~ facet) + \n  scale_x_discrete(\"x\", breaks = c('.00', '.05', '.10', '.15', '.20'),\n                   expand = c(.05, .05))"
  },
  {
    "objectID": "posts/2014-03-14-update-to-highlighthtml-package.html",
    "href": "posts/2014-03-14-update-to-highlighthtml-package.html",
    "title": "Update to highlightHTML package",
    "section": "",
    "text": "I’ve added a new functionality to my highlightHTML package. This package post-processes HTML files and injects CSS and adds tags to create some further customization (for example highlight cells of a HTML table). This is most useful when writing a document using markdown and converting it into a HTML document using a tool like knitr, slidify, or even pandoc.\nUp to now, my package only worked with tables, see my old post that talks about this if you are interested: http://educate-r.org/2013/11/01/CondFormatMarkdown/. My update adds a similar functionality to text itself by including span tags in the document.\nThe following code will install the package with the new feature from github:\n\nlibrary(devtools)\ninstall_github(repo = 'highlightHTML', username = 'lebebr01', ref = 'testing')\n\nOnce the package is installed, the new function is called highlightHTMLtext. This function takes a HTML file as the input and post processes the file to add span tags to format text according to the CSS calls specified by the user. The function looks for {#id text} to add the span tags. The braces are used to define the text range that will use the id and the #id is the CSS id itself.\nHere is an example using the HTML file that comes with the package and which can also be found in the help file.\n\nlibrary(highlightHTML)\nfile &lt;- system.file('examples', package = 'highlightHTML')\nfile1 &lt;- paste(file, \"bgtext.html\", sep = \"/\")\n\n# Change background color and text color with CSS\ntags &lt;- c(\"#bgblack {background-color: black; color: white;}\",\n  \"#bgblue {background-color: #0000FF; color: white;}\")\n\n# Post-process HTML file\nhighlightHTMLtext(input = file1, output = NULL, updateCSS = TRUE,\n  tags = tags, browse = TRUE)\n\nIf you run the above command, the file should open in your browser to see the result of the new HTML file. The result should have boxes of color in specific areas that we indicated by the {#id text} syntax in the raw markdown and HTML file.\nMy next step is to develop a master function to wrap these other functions so only one call would be needed to format text and tables. Let me know of any issues by going to the github page: https://github.com/lebebr01/highlightHTML\n\n\nBefore and After HTML\nHere is what the body of the HTML file looks like before running the function:\n&lt;body&gt;\n&lt;h1&gt;Test of Text&lt;/h1&gt;\n\n&lt;p&gt;Testing the ability to change the {#bgblue color} of the text.&lt;/p&gt;\n\n&lt;p&gt;Can also do {#bgblack multiple words of text}&lt;/p&gt;\n\n&lt;p&gt;{#bgblack Even entire paragraphs that you want to really stand out from the rest of the document.  More than color could also be changed, anything alterable by CSS.  Test out the function and get creative with the CSS}&lt;/p&gt;\n&lt;/body&gt;\nThis is what the HTML document looks like after running the function:\n&lt;body&gt;\n&lt;h1&gt;Test of Text&lt;/h1&gt;\n&lt;p&gt;Testing the ability to change the &lt;span id='bgblue'&gt; color&lt;/span&gt; of the text.&lt;/p&gt;\n&lt;p&gt;Can also do &lt;span id='bgblack'&gt; multiple words of text&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;&lt;span id='bgblack'&gt; Even entire paragraphs that you want to really stand out from the rest of the document.  More than color could also be changed, anything alterable by CSS.  Test out the function and get creative with the CSS&lt;/span&gt;&lt;/p&gt;\n&lt;/body&gt;\nThe braces identify the location of the span tags."
  },
  {
    "objectID": "posts/2013-11-18-formatting-markdown-table-with-r.html",
    "href": "posts/2013-11-18-formatting-markdown-table-with-r.html",
    "title": "Formatting Markdown Table with R",
    "section": "",
    "text": "This past August I took an opportunity to step back into the University academic world as a Visiting Assistant Professor at the University of Arkansas. I have enjoyed the transition back into the academic world, including a more flexible schedule, variation in topics/duties, and collaborating with colleagues.\nHowever, there has been some growing pains, especially regarding creating my own slides for the courses I teach. Although I am using the same books/curriculum used in previous semesters, I am making my own slides and adding my own pieces as I see fit. In addition, I do not use powerpoint, which all of the existing slides are in. Therefore, I am creating my own versions of the slides using a combination of R, knitr, markdown, pandoc, slidy, and LaTeX. Below is my general process of making my slides and the slides I put online for students to have access to."
  },
  {
    "objectID": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-1---create-source-file",
    "href": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-1---create-source-file",
    "title": "Formatting Markdown Table with R",
    "section": "Step 1 - Create Source File",
    "text": "Step 1 - Create Source File\nI start with a Rmd file. This allows me to embed R code into the source document. This is particularly useful for me to include plots of distributions, graphically showing how ANOVA works, etc. Once I am finished editing my Rmd, if I am using Rstudio I just use the Knit HTML button to automatically generate the markdown and HTML file for me. Alternatively, the knit command from the knitr package will create the markdown file for you (but not the HTML file, although for me the HTML file is not needed in this step). The defaults of the knit command work fine for me.\n\nknit(input = '/path/to/file.Rmd', output = '/path/to/file.md')"
  },
  {
    "objectID": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-2---create-html-presentation",
    "href": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-2---create-html-presentation",
    "title": "Formatting Markdown Table with R",
    "section": "Step 2 - Create HTML Presentation",
    "text": "Step 2 - Create HTML Presentation\nOnce we have the markdown file, I now use pandoc to create my HTML presentation. There are a few ways to create HTML presentation slides, but I personally like slidy the best. I like slidy because it easily fills the whole screen and also allows for content to go over the edges of the slide. If content goes outside of the edges of a single slide, you can scroll to see the missing content. I find this useful if I want to blow up an image or have two plots where I can show one then scroll to the second. The pandoc command I use looks something like this:\npandoc -s –mathjax -i -t slidy inputfile.md -o outfile.html"
  },
  {
    "objectID": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-3-optional---edit-css-for-html-presentation",
    "href": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-3-optional---edit-css-for-html-presentation",
    "title": "Formatting Markdown Table with R",
    "section": "Step 3 (Optional) - Edit CSS for HTML Presentation",
    "text": "Step 3 (Optional) - Edit CSS for HTML Presentation\nI use a custom CSS file to style my HTML presentation so it uses some of the official colors from the University of Arkansas. For example, my header titles use the Arkansas red. To use a custom CSS file, you just need to find the line that mentions CSS in the HTML file and change it to reflect your custom file. The defaults look good, although perhaps slightly bland."
  },
  {
    "objectID": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-4---create-pdf-slides",
    "href": "posts/2013-11-18-formatting-markdown-table-with-r.html#step-4---create-pdf-slides",
    "title": "Formatting Markdown Table with R",
    "section": "Step 4 - Create pdf slides",
    "text": "Step 4 - Create pdf slides\nI then create a different set of slides using LaTeX that I post on the blackboard site for each of my courses. Pandoc is how I get the tex file to compile with LaTeX. The command is very simple:\npandoc -s inputfile.md -o outfile.tex\nTwo things I change, I make sure the base text size is 12 pt. I also make sure to use the float package and change any figure positions from htbp to H which forces the figures to stay in position and not float around. Then I compile the resulting tex using Rstudio or from the command line with:\npdflatex -interaction=nonstopmode -synctex=1 outfile.tex\nIn my opinion this creates great looking html presentations that are highly customizable. One thing to note is that by default to get the slideshow, you need to be connected to the internet. Both slidy and mathjax refer to javascript files that are on downloaded directly from the web. You should be able to download these files, store them locally, and refer to the local versions."
  },
  {
    "objectID": "posts/2015-05-31-speed-test-of-sequence-generation-for-unbalance....html",
    "href": "posts/2015-05-31-speed-test-of-sequence-generation-for-unbalance....html",
    "title": "Speed test of sequence generation for unbalanced simulation",
    "section": "",
    "text": "I have a simulation package that allows for the simulation of regression models including nested data structures. You can see the package on github here: simReg. Over the weekend I updated the package to allow for the simulation of unbalanced designs. I’m hoping to put together a new vigenette soon highlighting the functionality.\nI am working on a simulation that uses the unbalanced functionality and while simulating longitudinal data I’ve found the function is much slower than the cross sectional counterparts (and balanced designs). I’ve ran some additional testing and I believe I have the speed issues narrowed down to the fact that I am generating a time variable. Essentially, I have a vector of number of observations per cluster. The function then turns this vector of lengths into a time variable starting at 0 up to the maximum number of observations minus 1 by 1. As an example:\n\nx &lt;- round(runif(5, min = 3, max = 10), 0)\nunlist(lapply(1:length(x), function(xx) (1:x[xx]) - 1))\n\n [1] 0 1 2 3 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6\n\n\nFrom the code above, you can see that there the number of observations is generated using runif which is saved to the object x. Then I use a combination of lapply, unlist, and the ‘:’ operator to generate the sequence. This is the same code used in my package above to generate the time variable.\nAs such, I was interested in testing various ways to generate the sequence and do a performance comparison. I compared the following ways, the ':' operator, seq.int, seq, do.call with mapply, and rep.int for the balanced case as a comparison to how it was done before. This was all done with the great microbenchmark package.\nHere are the results from the 7 comparisons:\n\nlibrary(microbenchmark)\nx &lt;- round(runif(100, min = 3, max = 15), 0)\nmicrobenchmark(\n  colon = unlist(lapply(1:length(x), function(xx) (1:x[xx]) - 1)),\n  seq.int = unlist(lapply(1:length(x), function(xx) seq.int(0, x[xx] - 1, 1))),\n  seq = unlist(lapply(1:length(x), function(xx) seq(0, x[xx] - 1, 1))),\n  seq.int_mapply = do.call(c, mapply(seq.int, 0, x - 1)),\n  seq_mapply = do.call(c, mapply(seq, 0, x - 1)),\n  colon_mapply = do.call(c, mapply(':', 0, x - 1)),\n  rep.int = rep.int(1:8 - 1, times = 100), # balanced case for reference.\n  times = 1000L\n)\n\nWarning in microbenchmark(colon = unlist(lapply(1:length(x), function(xx) (1:x[xx]) - : less\naccurate nanosecond times to avoid potential integer overflows\n\n\nUnit: nanoseconds\n           expr    min       lq       mean   median       uq     max neval\n          colon  34768  35793.0  38108.680  36285.0  36941.0  918851  1000\n        seq.int  44813  46207.0  50598.592  46986.0  47867.5  826888  1000\n            seq 557190 568895.5 593151.428 575927.0 584475.5 1492031  1000\n seq.int_mapply  42271  43952.0  47280.872  44854.0  46002.0  834186  1000\n     seq_mapply 196226 200592.5 221808.032 203544.5 207009.0 3461835  1000\n   colon_mapply  36859  38335.0  41831.480  39196.0  40098.0  957350  1000\n        rep.int    984   1107.0   1247.056   1189.0   1271.0   10209  1000\n\n\nThe results (in microseconds) show that this is where the significant slowdown is coming in my package implementing the unbalanced cases, although it appears that the ‘:’ operator is the second best alternative. For those that have not seen the significant speed bump of the seq.int and rep.int over the seq and rep alternatives should also pay close attention (compare lines 2 and 3 above).\nI’d be interested in alternative procedures that I am not aware of as well. Although not a big deal when running the package once, doing it 50,000 times does add up.\nLastly, for those that are interested, we can show they are all equivalent methods (except for the rep.int case).\n\nidentical(\n  unlist(lapply(1:length(x), function(xx) (1:x[xx]) - 1)),\n  unlist(lapply(1:length(x), function(xx) seq.int(0, x[xx] - 1, 1))),\n  unlist(lapply(1:length(x), function(xx) seq(0, x[xx] - 1, 1))),\n  do.call(c, mapply(seq.int, 0, x - 1)),\n  do.call(c, mapply(seq, 0, x - 1)),\n  do.call(c, mapply(':', 0, x - 1))\n)\n\n[1] FALSE"
  },
  {
    "objectID": "posts/2025-10-06-simglm-propensity-modeling.html",
    "href": "posts/2025-10-06-simglm-propensity-modeling.html",
    "title": "simglm v0.9.23: Propensity Score Modeling",
    "section": "",
    "text": "Building on last week’s post about propensity score simulation, the simglm package has been extended to now allow for the estimation of power in addition to the propensity score simulations. Recall, propensity scores can be helpful for causal inference with intact groups, when random assignment was either not possible or not done. This allows for the estimation of power under all three supported propensity score methods:\n\nCovariate adjustment\nInverse propensity score weighting\nStabilized propensity score weighting\n\nOne nice feature is that no additional simulation arguments are needed to handle the model fitting and replication procedures. These pass directly from the previous functionality handled from within the package. One small adjustment was made internally to allow for the inclusion of the propensity score covariate when that option is specified. Since this term is not usually of substantive value, it can be filtered out from the final output as well.\nIf you haven’t already, remember to first install the simglm package from GitHub.\n\nremotes::install_github('lebebr01/simglm')\n\n\n\nThe below code does the following steps:\n\nSimulates the data with the propensity model. That is, the treatment attribute is simulated as a function of age and SES so that those that are older and higher SES are more likely to be in the treatment group.\n\nSimulates the full model to generate scores for student achievement.\n\nFit propensity score model to the simulated data.\n\nFit the final model, with the estimated propensity scores included as a covariate, and store model estimates.\n\nRepeat steps 1–4 for 500 replications (illustrative only; use more in practice).\n\nTo fit the propensity score adjusted covariate model, propensity_type = 'covariate' is added to the propensity model simulation arguments.\n\nlibrary(simglm)\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'covariate'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nreplicate_simulation(sim_arguments) |&gt;\n  compute_statistics(sim_arguments, alternative_power = FALSE, \n                     type_1_error = FALSE)\n\n# A tibble: 6 × 7\n  term  avg_estimate  power param_estimate_sd avg_standard_error precision_ratio\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 (Int…      49.9    1                0.856              0.854             1.00 \n2 age         0.0964 0.574            0.0406             0.0401            1.01 \n3 moti…       0.400  1                0.00494            0.00502           0.985\n4 prop…       0.124  0.0320           1.28               1.25              1.02 \n5 ses         0.254  0.98             0.0595             0.0575            1.03 \n6 trt         1.18   0.998            0.256              0.262             0.975\n# ℹ 1 more variable: replications &lt;dbl&gt;\n\n\n\n\n\nThe below code does the following steps:\n\nSimulates the data with the propensity model. That is, the treatment attribute is simulated as a function of age and SES so that those that are older and higher SES are more likely to be in the treatment group.\n\nSimulates the full model to generate scores for student achievement.\n\nFit the propensity score model to the simulated data.\n\nFit the final model, using inverse propensity score weighting, and store model estimates.\n\nRepeat steps 1–4 for 500 replications (illustrative only; use more in practice).\n\nThe only addition needed is to add, propensity_type = 'ipw' to the propensity model simulation arguments.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'ipw'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nreplicate_simulation(sim_arguments) |&gt;\n  compute_statistics(sim_arguments, alternative_power = FALSE, \n                     type_1_error = FALSE)\n\n# A tibble: 5 × 7\n  term   avg_estimate power param_estimate_sd avg_standard_error precision_ratio\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 (Inte…      50.0    1                0.306             0.216              1.42\n2 age          0.0983 0.846            0.0432            0.0252             1.72\n3 motiv…       0.400  1                0.0103            0.00501            2.05\n4 ses          0.256  1                0.0534            0.0204             2.62\n5 trt          1.23   0.962            0.371             0.263              1.41\n# ℹ 1 more variable: replications &lt;dbl&gt;\n\n\n\n\n\nThe code below performs the following steps.\n\nSimulates the data with the propensity model. That is, the treatment attribute is simulated as a function of age and SES so that those that are older and higher SES are more likely to be in the treatment group.\nSimulates the full model to generate scores for student achievement.\nFit the propensity score model to the simulated data\nFit the final model, using stabilized propensity score weighting, and store model estimates.\nRepeat steps 1–4 for 500 replications (illustrative only; use more in practice).\n\nThe only addition needed is to add, propensity_type = 'sbw' to the propensity model simulation arguments.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'sbw'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nreplicate_simulation(sim_arguments) |&gt;\n  compute_statistics(sim_arguments, alternative_power = FALSE, \n                     type_1_error = FALSE)\n\n# A tibble: 5 × 7\n  term   avg_estimate power param_estimate_sd avg_standard_error precision_ratio\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 (Inte…       50.0   1               0.309              0.216              1.43\n2 age           0.102 0.838           0.0443             0.0252             1.76\n3 motiv…        0.400 1               0.00905            0.00502            1.80\n4 ses           0.248 0.998           0.0540             0.0205             2.64\n5 trt           1.18  0.934           0.384              0.263              1.46\n# ℹ 1 more variable: replications &lt;dbl&gt;\n\n\n\n\n\nAs a final example, let’s see what happens when we generate data with propensity scores, but do not account for this within the model fitting. To omit the propensity scores being fitted, the propensity_model simulation arguments can be omitted. I’m fitting three different types of models to this:\n\nThe full model, including a propensity score analysis using stabilized propensity weights.\nThe full model without any propensity score analysis, but statistically adjusting for the observed covariates used in the propensity score model.\nA reduced, misspecified model that omits age and SES and does not include a propensity score model.\n\nThe last misspecified model should produce an average treatment effect that is severely biased as the treatment group was not random, instead, the treatment group specification was a function of age and SES.\n\n\nCode\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nno_propensity &lt;- replicate_simulation(sim_arguments) |&gt;\n    dplyr::bind_rows() |&gt;\n    dplyr::mutate(group = 'no_prop')\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt,\n                   model_function = 'lm'),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nmissspec &lt;- replicate_simulation(sim_arguments) |&gt;\n    dplyr::bind_rows() |&gt;\n    dplyr::mutate(group = 'miss_spec')\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'sbw'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nsbw_propensity &lt;- replicate_simulation(sim_arguments) |&gt;\n    dplyr::bind_rows() |&gt;\n    dplyr::mutate(group = 'sbw')\n\n\nLet’s visualize the differences across the three methods, with a vertical line at the specified population treatment effect. You can see that when nothing is done with the misspecified model, the estimated treatment effect is biased severely. The simple covariate adjusted effect without fitting the propensity score model produces an estimated effect similar to the specified population treatment effect, but may slightly underestimate variance. Finally, the propensity score model with the stabilized propensity score weighting produces an unbiased treatment effect and may produce a more accurate picture of variability in this treatment effect.\n\nsbw_propensity |&gt; \n    dplyr::bind_rows(no_propensity)|&gt; \n    dplyr::bind_rows(missspec) |&gt;\n    dplyr::filter(term == 'trt') |&gt; \n    ggformula::gf_density(~estimate, fill = ~ group) |&gt; \n    ggformula::gf_vline(xintercept = ~ 1.2)\n\n\n\n\n\n\n\n\nThese examples show how simglm can be used not only to simulate propensity score models but also to estimate power under different adjustment strategies. By simulating under known conditions, you can evaluate the robustness of your causal modeling choices before applying them to real data. Because these same simulations can also track significance rates across replications, you can easily extend them to estimate statistical power under different adjustment strategies.\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 26.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] C.UTF-8/C.UTF-8/C.UTF-8/C/C.UTF-8/C.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] future_1.58.0 simglm_0.9.23 ggplot2_4.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.6          generics_0.1.4      tidyr_1.3.1        \n [4] gtools_3.9.5        stringi_1.8.7       listenv_0.9.1      \n [7] hms_1.1.3           digest_0.6.37       magrittr_2.0.3     \n[10] evaluate_1.0.5      grid_4.5.0          RColorBrewer_1.1-3 \n[13] fastmap_1.2.0       ggformula_0.12.0    jsonlite_2.0.0     \n[16] backports_1.5.0     purrr_1.1.0         scales_1.4.0       \n[19] codetools_0.2-20    cli_3.6.5           labelled_2.14.1    \n[22] rlang_1.1.6         parallelly_1.45.0   future.apply_1.20.0\n[25] withr_3.0.2         yaml_2.3.10         tools_4.5.0        \n[28] parallel_4.5.0      dplyr_1.1.4         mosaicCore_0.9.4.0 \n[31] forcats_1.0.0       globals_0.18.0      broom_1.0.8        \n[34] vctrs_0.6.5         R6_2.6.1            ggridges_0.5.6     \n[37] lifecycle_1.0.4     stringr_1.5.2       htmlwidgets_1.6.4  \n[40] MASS_7.3-65         pkgconfig_2.0.3     pillar_1.11.0      \n[43] gtable_0.3.6        glue_1.8.0          haven_2.5.4        \n[46] xfun_0.53           tibble_3.3.0        tidyselect_1.2.1   \n[49] knitr_1.50          farver_2.1.2        htmltools_0.5.8.1  \n[52] labeling_0.4.3      rmarkdown_2.29      compiler_4.5.0     \n[55] S7_0.2.0"
  },
  {
    "objectID": "posts/2025-10-06-simglm-propensity-modeling.html#fitting-a-covariate-adjusted-model.",
    "href": "posts/2025-10-06-simglm-propensity-modeling.html#fitting-a-covariate-adjusted-model.",
    "title": "simglm v0.9.23: Propensity Score Modeling",
    "section": "",
    "text": "The below code does the following steps:\n\nSimulates the data with the propensity model. That is, the treatment attribute is simulated as a function of age and SES so that those that are older and higher SES are more likely to be in the treatment group.\n\nSimulates the full model to generate scores for student achievement.\n\nFit propensity score model to the simulated data.\n\nFit the final model, with the estimated propensity scores included as a covariate, and store model estimates.\n\nRepeat steps 1–4 for 500 replications (illustrative only; use more in practice).\n\nTo fit the propensity score adjusted covariate model, propensity_type = 'covariate' is added to the propensity model simulation arguments.\n\nlibrary(simglm)\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'covariate'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nreplicate_simulation(sim_arguments) |&gt;\n  compute_statistics(sim_arguments, alternative_power = FALSE, \n                     type_1_error = FALSE)\n\n# A tibble: 6 × 7\n  term  avg_estimate  power param_estimate_sd avg_standard_error precision_ratio\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 (Int…      49.9    1                0.856              0.854             1.00 \n2 age         0.0964 0.574            0.0406             0.0401            1.01 \n3 moti…       0.400  1                0.00494            0.00502           0.985\n4 prop…       0.124  0.0320           1.28               1.25              1.02 \n5 ses         0.254  0.98             0.0595             0.0575            1.03 \n6 trt         1.18   0.998            0.256              0.262             0.975\n# ℹ 1 more variable: replications &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2025-10-06-simglm-propensity-modeling.html#fitting-a-ipw-adjusted-model.",
    "href": "posts/2025-10-06-simglm-propensity-modeling.html#fitting-a-ipw-adjusted-model.",
    "title": "simglm v0.9.23: Propensity Score Modeling",
    "section": "",
    "text": "The below code does the following steps:\n\nSimulates the data with the propensity model. That is, the treatment attribute is simulated as a function of age and SES so that those that are older and higher SES are more likely to be in the treatment group.\n\nSimulates the full model to generate scores for student achievement.\n\nFit the propensity score model to the simulated data.\n\nFit the final model, using inverse propensity score weighting, and store model estimates.\n\nRepeat steps 1–4 for 500 replications (illustrative only; use more in practice).\n\nThe only addition needed is to add, propensity_type = 'ipw' to the propensity model simulation arguments.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'ipw'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nreplicate_simulation(sim_arguments) |&gt;\n  compute_statistics(sim_arguments, alternative_power = FALSE, \n                     type_1_error = FALSE)\n\n# A tibble: 5 × 7\n  term   avg_estimate power param_estimate_sd avg_standard_error precision_ratio\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 (Inte…      50.0    1                0.306             0.216              1.42\n2 age          0.0983 0.846            0.0432            0.0252             1.72\n3 motiv…       0.400  1                0.0103            0.00501            2.05\n4 ses          0.256  1                0.0534            0.0204             2.62\n5 trt          1.23   0.962            0.371             0.263              1.41\n# ℹ 1 more variable: replications &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2025-10-06-simglm-propensity-modeling.html#fitting-a-sbw-adjusted-model.",
    "href": "posts/2025-10-06-simglm-propensity-modeling.html#fitting-a-sbw-adjusted-model.",
    "title": "simglm v0.9.23: Propensity Score Modeling",
    "section": "",
    "text": "The code below performs the following steps.\n\nSimulates the data with the propensity model. That is, the treatment attribute is simulated as a function of age and SES so that those that are older and higher SES are more likely to be in the treatment group.\nSimulates the full model to generate scores for student achievement.\nFit the propensity score model to the simulated data\nFit the final model, using stabilized propensity score weighting, and store model estimates.\nRepeat steps 1–4 for 500 replications (illustrative only; use more in practice).\n\nThe only addition needed is to add, propensity_type = 'sbw' to the propensity model simulation arguments.\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'sbw'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nreplicate_simulation(sim_arguments) |&gt;\n  compute_statistics(sim_arguments, alternative_power = FALSE, \n                     type_1_error = FALSE)\n\n# A tibble: 5 × 7\n  term   avg_estimate power param_estimate_sd avg_standard_error precision_ratio\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;\n1 (Inte…       50.0   1               0.309              0.216              1.43\n2 age           0.102 0.838           0.0443             0.0252             1.76\n3 motiv…        0.400 1               0.00905            0.00502            1.80\n4 ses           0.248 0.998           0.0540             0.0205             2.64\n5 trt           1.18  0.934           0.384              0.263              1.46\n# ℹ 1 more variable: replications &lt;dbl&gt;"
  },
  {
    "objectID": "posts/2025-10-06-simglm-propensity-modeling.html#comparing-model-estimates-with-and-without-propensity-scores.",
    "href": "posts/2025-10-06-simglm-propensity-modeling.html#comparing-model-estimates-with-and-without-propensity-scores.",
    "title": "simglm v0.9.23: Propensity Score Modeling",
    "section": "",
    "text": "As a final example, let’s see what happens when we generate data with propensity scores, but do not account for this within the model fitting. To omit the propensity scores being fitted, the propensity_model simulation arguments can be omitted. I’m fitting three different types of models to this:\n\nThe full model, including a propensity score analysis using stabilized propensity weights.\nThe full model without any propensity score analysis, but statistically adjusting for the observed covariates used in the propensity score model.\nA reduced, misspecified model that omits age and SES and does not include a propensity score model.\n\nThe last misspecified model should produce an average treatment effect that is severely biased as the treatment group was not random, instead, the treatment group specification was a function of age and SES.\n\n\nCode\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nno_propensity &lt;- replicate_simulation(sim_arguments) |&gt;\n    dplyr::bind_rows() |&gt;\n    dplyr::mutate(group = 'no_prop')\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt,\n                   model_function = 'lm'),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nmissspec &lt;- replicate_simulation(sim_arguments) |&gt;\n    dplyr::bind_rows() |&gt;\n    dplyr::mutate(group = 'miss_spec')\n\nsim_arguments &lt;- list(\n    formula = achievement ~ 1 + motivation + trt + age + ses,\n    fixed = list(\n        motivation = list(var_type = 'continuous',\n                           mean = 0, sd = 20)\n    ),\n    sample_size = 1000,\n    error = list(variance = 10),\n    reg_weights = c(50, 0.4, 1.2, 0.1, 0.25),\n    propensity = list(\n        formula = trt ~ 1 + age + ses,\n        fixed = list(age = list(var_type = 'ordinal', \n                         levels = -7:7),\n                     ses = list(var_type = 'continuous', \n                         mean = 0, sd = 5)),\n        sample_size = 1000,\n        error = list(variance = 5),\n        reg_weights = c(2, 0.3, -0.5),\n        outcome_type = 'binary'\n    ),\n    model_fit = list(formula = achievement ~ 1 + motivation + trt + age + ses,\n                   model_function = 'lm'),\n    propensity_model = list(\n        formula = trt ~ 1 + age + ses,\n        propensity_type = 'sbw'\n    ),\n    replications = 500,\n    extract_coefficients = TRUE\n)\nsbw_propensity &lt;- replicate_simulation(sim_arguments) |&gt;\n    dplyr::bind_rows() |&gt;\n    dplyr::mutate(group = 'sbw')\n\n\nLet’s visualize the differences across the three methods, with a vertical line at the specified population treatment effect. You can see that when nothing is done with the misspecified model, the estimated treatment effect is biased severely. The simple covariate adjusted effect without fitting the propensity score model produces an estimated effect similar to the specified population treatment effect, but may slightly underestimate variance. Finally, the propensity score model with the stabilized propensity score weighting produces an unbiased treatment effect and may produce a more accurate picture of variability in this treatment effect.\n\nsbw_propensity |&gt; \n    dplyr::bind_rows(no_propensity)|&gt; \n    dplyr::bind_rows(missspec) |&gt;\n    dplyr::filter(term == 'trt') |&gt; \n    ggformula::gf_density(~estimate, fill = ~ group) |&gt; \n    ggformula::gf_vline(xintercept = ~ 1.2)\n\n\n\n\n\n\n\n\nThese examples show how simglm can be used not only to simulate propensity score models but also to estimate power under different adjustment strategies. By simulating under known conditions, you can evaluate the robustness of your causal modeling choices before applying them to real data. Because these same simulations can also track significance rates across replications, you can easily extend them to estimate statistical power under different adjustment strategies.\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 26.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] C.UTF-8/C.UTF-8/C.UTF-8/C/C.UTF-8/C.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] future_1.58.0 simglm_0.9.23 ggplot2_4.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.6          generics_0.1.4      tidyr_1.3.1        \n [4] gtools_3.9.5        stringi_1.8.7       listenv_0.9.1      \n [7] hms_1.1.3           digest_0.6.37       magrittr_2.0.3     \n[10] evaluate_1.0.5      grid_4.5.0          RColorBrewer_1.1-3 \n[13] fastmap_1.2.0       ggformula_0.12.0    jsonlite_2.0.0     \n[16] backports_1.5.0     purrr_1.1.0         scales_1.4.0       \n[19] codetools_0.2-20    cli_3.6.5           labelled_2.14.1    \n[22] rlang_1.1.6         parallelly_1.45.0   future.apply_1.20.0\n[25] withr_3.0.2         yaml_2.3.10         tools_4.5.0        \n[28] parallel_4.5.0      dplyr_1.1.4         mosaicCore_0.9.4.0 \n[31] forcats_1.0.0       globals_0.18.0      broom_1.0.8        \n[34] vctrs_0.6.5         R6_2.6.1            ggridges_0.5.6     \n[37] lifecycle_1.0.4     stringr_1.5.2       htmlwidgets_1.6.4  \n[40] MASS_7.3-65         pkgconfig_2.0.3     pillar_1.11.0      \n[43] gtable_0.3.6        glue_1.8.0          haven_2.5.4        \n[46] xfun_0.53           tibble_3.3.0        tidyselect_1.2.1   \n[49] knitr_1.50          farver_2.1.2        htmltools_0.5.8.1  \n[52] labeling_0.4.3      rmarkdown_2.29      compiler_4.5.0     \n[55] S7_0.2.0"
  },
  {
    "objectID": "publications/mnmap2.html",
    "href": "publications/mnmap2.html",
    "title": "A multi-institutional study of the relationship between high school mathematics achievement and performance in introductory college statistics",
    "section": "",
    "text": "In this study we examined the effects of prior mathematics achievement and completion of a commercially developed, National Science Foundation-funded, or University of Chicago School Mathematics Project high school mathematics curriculum on achievement in students’ first college statistics course. Specifically, we examined the relationship between students’ high school mathematics achievement and high school mathematics curriculum on the difficulty level of students’ first college statistics course, and on the grade earned in that course. In general, students with greater prior mathematics achievement took more difficult statistics courses and earned higher grades in those courses. The high school mathematics curriculum a student completed was unrelated to statistics grades and course-taking."
  },
  {
    "objectID": "publications/mnmap2.html#abstract",
    "href": "publications/mnmap2.html#abstract",
    "title": "A multi-institutional study of the relationship between high school mathematics achievement and performance in introductory college statistics",
    "section": "",
    "text": "In this study we examined the effects of prior mathematics achievement and completion of a commercially developed, National Science Foundation-funded, or University of Chicago School Mathematics Project high school mathematics curriculum on achievement in students’ first college statistics course. Specifically, we examined the relationship between students’ high school mathematics achievement and high school mathematics curriculum on the difficulty level of students’ first college statistics course, and on the grade earned in that course. In general, students with greater prior mathematics achievement took more difficult statistics courses and earned higher grades in those courses. The high school mathematics curriculum a student completed was unrelated to statistics grades and course-taking."
  },
  {
    "objectID": "publications/mnmap2.html#citation",
    "href": "publications/mnmap2.html#citation",
    "title": "A multi-institutional study of the relationship between high school mathematics achievement and performance in introductory college statistics",
    "section": "Citation",
    "text": "Citation\nDupuis, Danielle, Medhanie, Amanuel, Harwell, Michael R., LeBeau, Brandon, Monson, Debra, Post, Thomas R. (2012). A multi-institutional study of the relationship between high school mathematics achievement and performance in introductory college statistics. **Statistics Education Research Journal, 11(4), 4 - 20."
  },
  {
    "objectID": "publications/mnmap2.html#links",
    "href": "publications/mnmap2.html#links",
    "title": "A multi-institutional study of the relationship between high school mathematics achievement and performance in introductory college statistics",
    "section": "Links",
    "text": "Links\nLink to Journal PDF\n\nPublication: Statistics Education Research Journal, 11(4), 4 - 20 Authors: Danielle Dupuis, Amanuel Medhanie, Michael R. Harwell, Brandon LeBeau, Debra Monson, Thomas R. Post Date: May 01, 2012"
  },
  {
    "objectID": "publications/dev-psych.html",
    "href": "publications/dev-psych.html",
    "title": "Development of Internalizing Problems From Adolescence to Emerging Adulthood: Accounting for Heterotypic Continuity With Vertical Scaling",
    "section": "",
    "text": "Manifestations of internalizing problems, such as specific symptoms of anxiety and depression, can change across development, even if individuals show strong continuity in rank-order levels of internalizing problems. This illustrates the concept of heterotypic continuity, and raises the question of whether common measures might be construct-valid for one age but not another. This study examines mean-level changes in internalizing problems across a long span of development at the same time as accounting for heterotypic continuity by using age-appropriate, changing measures. Internalizing problems from age 14–24 were studied longitudinally in a community sample (N = 585), using Achenbach’s Youth Self-Report (YSR) and Young Adult Self-Report (YASR). Heterotypic continuity was evaluated with an item response theory (IRT) approach to vertical scaling, linking different measures over time to be on the same scale, as well as with a Thurstone scaling approach. With vertical scaling, internalizing problems peaked in mid-to-late adolescence and showed a group-level decrease from adolescence to early adulthood, a change that would not have been seen with the approach of using only age-common items. Individuals’ trajectories were sometimes different than would have been seen with the common-items approach. Findings support the importance of considering heterotypic continuity when examining development and vertical scaling to account for heterotypic continuity with changing measures."
  },
  {
    "objectID": "publications/dev-psych.html#abstract",
    "href": "publications/dev-psych.html#abstract",
    "title": "Development of Internalizing Problems From Adolescence to Emerging Adulthood: Accounting for Heterotypic Continuity With Vertical Scaling",
    "section": "",
    "text": "Manifestations of internalizing problems, such as specific symptoms of anxiety and depression, can change across development, even if individuals show strong continuity in rank-order levels of internalizing problems. This illustrates the concept of heterotypic continuity, and raises the question of whether common measures might be construct-valid for one age but not another. This study examines mean-level changes in internalizing problems across a long span of development at the same time as accounting for heterotypic continuity by using age-appropriate, changing measures. Internalizing problems from age 14–24 were studied longitudinally in a community sample (N = 585), using Achenbach’s Youth Self-Report (YSR) and Young Adult Self-Report (YASR). Heterotypic continuity was evaluated with an item response theory (IRT) approach to vertical scaling, linking different measures over time to be on the same scale, as well as with a Thurstone scaling approach. With vertical scaling, internalizing problems peaked in mid-to-late adolescence and showed a group-level decrease from adolescence to early adulthood, a change that would not have been seen with the approach of using only age-common items. Individuals’ trajectories were sometimes different than would have been seen with the common-items approach. Findings support the importance of considering heterotypic continuity when examining development and vertical scaling to account for heterotypic continuity with changing measures."
  },
  {
    "objectID": "publications/dev-psych.html#citation",
    "href": "publications/dev-psych.html#citation",
    "title": "Development of Internalizing Problems From Adolescence to Emerging Adulthood: Accounting for Heterotypic Continuity With Vertical Scaling",
    "section": "Citation",
    "text": "Citation\nPeterson, Isaac T., Lindhiem, Oliver, LeBeau, Brandon, Bates, John E., Pettit, Gregory S., Lansford, Jennifer E., Dodge, Kenneth A. (2017). Development of Internalizing Problems From Adolescence to Emerging Adulthood: Accounting for Heterotypic Continuity With Vertical Scaling. **Developmental Psychology, 54 (3), 586."
  },
  {
    "objectID": "publications/dev-psych.html#links",
    "href": "publications/dev-psych.html#links",
    "title": "Development of Internalizing Problems From Adolescence to Emerging Adulthood: Accounting for Heterotypic Continuity With Vertical Scaling",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Developmental Psychology, 54 (3), 586 Authors: Isaac T. Peterson, Oliver Lindhiem, Brandon LeBeau, John E. Bates, Gregory S. Pettit, Jennifer E. Lansford, Kenneth A. Dodge Date: November 20, 2017 DOI: 10.1037/dev0000449"
  },
  {
    "objectID": "publications/mnmap4.html",
    "href": "publications/mnmap4.html",
    "title": "The Impact of Institutional Factors on the Relationship Between High School Mathematics Curricula and College Mathematics Course-Taking and Achievement",
    "section": "",
    "text": "Meta-analytic methods were used to examine the moderating effect of institutional factors on the relationship between high school mathematics curricula and college mathematics course-taking and achievement from a sample of 32 colleges. The findings suggest that the impact of curriculum on college mathematics outcomes is not generally moderated by institutional characteristics such as selectivity and educational profile, providing evidence that the relationships between curriculum and college mathematics outcomes generalize to a range of colleges. The results inform college policies and practices for advising students on mathematics course-taking including enrollment in developmental courses, and high school mathematics curriculum selection."
  },
  {
    "objectID": "publications/mnmap4.html#abstract",
    "href": "publications/mnmap4.html#abstract",
    "title": "The Impact of Institutional Factors on the Relationship Between High School Mathematics Curricula and College Mathematics Course-Taking and Achievement",
    "section": "",
    "text": "Meta-analytic methods were used to examine the moderating effect of institutional factors on the relationship between high school mathematics curricula and college mathematics course-taking and achievement from a sample of 32 colleges. The findings suggest that the impact of curriculum on college mathematics outcomes is not generally moderated by institutional characteristics such as selectivity and educational profile, providing evidence that the relationships between curriculum and college mathematics outcomes generalize to a range of colleges. The results inform college policies and practices for advising students on mathematics course-taking including enrollment in developmental courses, and high school mathematics curriculum selection."
  },
  {
    "objectID": "publications/mnmap4.html#citation",
    "href": "publications/mnmap4.html#citation",
    "title": "The Impact of Institutional Factors on the Relationship Between High School Mathematics Curricula and College Mathematics Course-Taking and Achievement",
    "section": "Citation",
    "text": "Citation\nHarwell, Michael R., Dupuis, Danielle, Post, Thomas R., Medhanie, Amanuel, LeBeau, Brandon (2013). The Impact of Institutional Factors on the Relationship Between High School Mathematics Curricula and College Mathematics Course-Taking and Achievement. **Educational Research Quarterly, 36.3, 22 - 46."
  },
  {
    "objectID": "publications/mnmap4.html#links",
    "href": "publications/mnmap4.html#links",
    "title": "The Impact of Institutional Factors on the Relationship Between High School Mathematics Curricula and College Mathematics Course-Taking and Achievement",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Educational Research Quarterly, 36.3, 22 - 46 Authors: Michael R. Harwell, Danielle Dupuis, Thomas R. Post, Amanuel Medhanie, Brandon LeBeau Date: May 01, 2013"
  },
  {
    "objectID": "publications/joss-pdfsearch.html",
    "href": "publications/joss-pdfsearch.html",
    "title": "pdfsearch: Search Tools for PDF Files",
    "section": "",
    "text": "PDF files are common formats for reports, journal articles, briefs, and many other documents. PDFs are lightweight, portable, and easily viewed across operating systems. Even though PDF files are ubiquitous, extracting and finding text within a PDF can be time consuming and not easily reproducible. The pdftools R package (Ooms 2017), which uses the poppler C++ library to extract text from PDF documents, aids in the ability to import text data from PDF files to manipulate in R. The pdfsearch package (LeBeau 2018) is an R package (R Core Team 2016) that extends the text extraction of pdftools to allow for keyword searching within a single PDF or a directory of PDF files. The pdfsearch package can aid users in manipulation of text data from PDF files in R and may also improve the reproducibility of the extraction and manipulation tasks. Users can search for keywords within PDF files where the location of the match and the raw text from the match are returned. This aspect of searching for keywords may be most useful for those conducting research syntheses or meta-analyses (Cooper 2017) that are more reproducible and less time consuming than current practice. Current research synthesis or meta-analysis practice involves the reading of each document to search for the presence of certain terms, phrases, or statistical effect size information to answer specific research questions. The improved workflow with the pdfsearch package would allow those conducting research syntheses, the ability to narrow down relevant portions of text based on the keyword matches returned by the package instead of looking at the entire text of the document. In addition, regular expressions could be written to search and extract statistical information needed to compute effect sizes automatically. As an example, the package is currently being used to explore the evolution of statistical software and quantitative methods used in published social science research (LeBeau and Aloe 2018). This process involves getting PDF files from published research articles and using pdfsearch to search for specific software and quantitative methods keywords within the research articles. The results of the keyword matches will be explored using research synthesis methods (Cooper 2017). The package vignette includes more information on this package. Included in the vignette are keyword searches within PDF documents and an exploration of the output from the package. The vignette also discusses limitations of the package. Below is example output of the package searching for the phrase “repeated measures” from Guo and Deng (2015)."
  },
  {
    "objectID": "publications/joss-pdfsearch.html#abstract",
    "href": "publications/joss-pdfsearch.html#abstract",
    "title": "pdfsearch: Search Tools for PDF Files",
    "section": "",
    "text": "PDF files are common formats for reports, journal articles, briefs, and many other documents. PDFs are lightweight, portable, and easily viewed across operating systems. Even though PDF files are ubiquitous, extracting and finding text within a PDF can be time consuming and not easily reproducible. The pdftools R package (Ooms 2017), which uses the poppler C++ library to extract text from PDF documents, aids in the ability to import text data from PDF files to manipulate in R. The pdfsearch package (LeBeau 2018) is an R package (R Core Team 2016) that extends the text extraction of pdftools to allow for keyword searching within a single PDF or a directory of PDF files. The pdfsearch package can aid users in manipulation of text data from PDF files in R and may also improve the reproducibility of the extraction and manipulation tasks. Users can search for keywords within PDF files where the location of the match and the raw text from the match are returned. This aspect of searching for keywords may be most useful for those conducting research syntheses or meta-analyses (Cooper 2017) that are more reproducible and less time consuming than current practice. Current research synthesis or meta-analysis practice involves the reading of each document to search for the presence of certain terms, phrases, or statistical effect size information to answer specific research questions. The improved workflow with the pdfsearch package would allow those conducting research syntheses, the ability to narrow down relevant portions of text based on the keyword matches returned by the package instead of looking at the entire text of the document. In addition, regular expressions could be written to search and extract statistical information needed to compute effect sizes automatically. As an example, the package is currently being used to explore the evolution of statistical software and quantitative methods used in published social science research (LeBeau and Aloe 2018). This process involves getting PDF files from published research articles and using pdfsearch to search for specific software and quantitative methods keywords within the research articles. The results of the keyword matches will be explored using research synthesis methods (Cooper 2017). The package vignette includes more information on this package. Included in the vignette are keyword searches within PDF documents and an exploration of the output from the package. The vignette also discusses limitations of the package. Below is example output of the package searching for the phrase “repeated measures” from Guo and Deng (2015)."
  },
  {
    "objectID": "publications/joss-pdfsearch.html#citation",
    "href": "publications/joss-pdfsearch.html#citation",
    "title": "pdfsearch: Search Tools for PDF Files",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2018). pdfsearch: Search Tools for PDF Files. **Journal of Open Source Software, 3 (27)."
  },
  {
    "objectID": "publications/joss-pdfsearch.html#links",
    "href": "publications/joss-pdfsearch.html#links",
    "title": "pdfsearch: Search Tools for PDF Files",
    "section": "Links",
    "text": "Links\nLink to JOSS PDF Code\n\nPublication: Journal of Open Source Software, 3 (27) Authors: Brandon LeBeau Date: July 09, 2018 DOI: 10.21105/joss.00668"
  },
  {
    "objectID": "publications/mnmap7.html#citation",
    "href": "publications/mnmap7.html#citation",
    "title": "The Relationship between High School Mathematics Curriculum and Mathematics Course-Taking and Achievement for Students Attending Community College",
    "section": "Citation",
    "text": "Citation\nDupuis, Danielle, Medhanie, Amanuel, Monson, Debra, LeBeau, Brandon, Harwell, Michael R., Post, Thomas R. (2015). The Relationship between High School Mathematics Curriculum and Mathematics Course-Taking and Achievement for Students Attending Community College. **MathAMATYC Educator, 6.3.\n\nPublication: MathAMATYC Educator, 6.3 Authors: Danielle Dupuis, Amanuel Medhanie, Debra Monson, Brandon LeBeau, Michael R. Harwell, Thomas R. Post Date: June 01, 2015"
  },
  {
    "objectID": "publications/mnmap6.html",
    "href": "publications/mnmap6.html",
    "title": "A Multisite Study of High School Mathematics Curricula and the Impact of Taking a Developmental Mathematics Course in College",
    "section": "",
    "text": "The relationship between high school mathematics curricula and the likelihood of students who enroll in a developmental (non-credit bearing) course in college taking additional mathematics courses was studied. The results showed that high school mathematics curriculum, years of high school mathematics completed, and A C T mathematics scores were related to developmental mathematics course-taking, but curriculum was not related to the subsequent mathematics course-taking o f students who began college with developmental mathematics. The results have important implications for educational researchers and policymakers at the college and high school levels."
  },
  {
    "objectID": "publications/mnmap6.html#abstract",
    "href": "publications/mnmap6.html#abstract",
    "title": "A Multisite Study of High School Mathematics Curricula and the Impact of Taking a Developmental Mathematics Course in College",
    "section": "",
    "text": "The relationship between high school mathematics curricula and the likelihood of students who enroll in a developmental (non-credit bearing) course in college taking additional mathematics courses was studied. The results showed that high school mathematics curriculum, years of high school mathematics completed, and A C T mathematics scores were related to developmental mathematics course-taking, but curriculum was not related to the subsequent mathematics course-taking o f students who began college with developmental mathematics. The results have important implications for educational researchers and policymakers at the college and high school levels."
  },
  {
    "objectID": "publications/mnmap6.html#citation",
    "href": "publications/mnmap6.html#citation",
    "title": "A Multisite Study of High School Mathematics Curricula and the Impact of Taking a Developmental Mathematics Course in College",
    "section": "Citation",
    "text": "Citation\nHarwell, Michael R., Dupuis, Danielle, Post, Thomas R., Medhanie, Amanuel, LeBeau, Brandon (2014). A Multisite Study of High School Mathematics Curricula and the Impact of Taking a Developmental Mathematics Course in College. **Educational Research Quarterly, 37.3, 3 - 22."
  },
  {
    "objectID": "publications/mnmap6.html#links",
    "href": "publications/mnmap6.html#links",
    "title": "A Multisite Study of High School Mathematics Curricula and the Impact of Taking a Developmental Mathematics Course in College",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Educational Research Quarterly, 37.3, 3 - 22 Authors: Michael R. Harwell, Danielle Dupuis, Thomas R. Post, Amanuel Medhanie, Brandon LeBeau Date: June 01, 2014"
  },
  {
    "objectID": "publications/software-pop.html",
    "href": "publications/software-pop.html",
    "title": "Evolution of Statistical Software and Quantitative Methods",
    "section": "",
    "text": "Statistical software is the enabling tool of quantitative research and the availability and use of the software can greatly shape which methods are used by researchers. Software that is more accessible is likely to have more users and the methods implemented within the software limits the methods accessible to researchers. Open source software, (e.g. R), has reduced these barriers by making cutting edge statistical methods available to researchers through add-on packages. This manuscript explores the evolution of statistical software within social science research using a research synthesis to establish the state of affairs."
  },
  {
    "objectID": "publications/software-pop.html#abstract",
    "href": "publications/software-pop.html#abstract",
    "title": "Evolution of Statistical Software and Quantitative Methods",
    "section": "",
    "text": "Statistical software is the enabling tool of quantitative research and the availability and use of the software can greatly shape which methods are used by researchers. Software that is more accessible is likely to have more users and the methods implemented within the software limits the methods accessible to researchers. Open source software, (e.g. R), has reduced these barriers by making cutting edge statistical methods available to researchers through add-on packages. This manuscript explores the evolution of statistical software within social science research using a research synthesis to establish the state of affairs."
  },
  {
    "objectID": "publications/software-pop.html#citation",
    "href": "publications/software-pop.html#citation",
    "title": "Evolution of Statistical Software and Quantitative Methods",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon, Aloe, Ariel M. (2020). Evolution of Statistical Software and Quantitative Methods. **."
  },
  {
    "objectID": "publications/software-pop.html#links",
    "href": "publications/software-pop.html#links",
    "title": "Evolution of Statistical Software and Quantitative Methods",
    "section": "Links",
    "text": "Links\nPDF\n\nPublication: Authors: Brandon LeBeau, Ariel M. Aloe Date: June 01, 2020"
  },
  {
    "objectID": "publications/stat-thinking-book.html",
    "href": "publications/stat-thinking-book.html",
    "title": "Statistical Reasoning through Computation and R",
    "section": "",
    "text": "Textbook used for Introduction to Statistical Methods. The goal of the text is to take a computational approach to statistics using graphical, bootstrap, and other computation methods."
  },
  {
    "objectID": "publications/stat-thinking-book.html#abstract",
    "href": "publications/stat-thinking-book.html#abstract",
    "title": "Statistical Reasoning through Computation and R",
    "section": "",
    "text": "Textbook used for Introduction to Statistical Methods. The goal of the text is to take a computational approach to statistics using graphical, bootstrap, and other computation methods."
  },
  {
    "objectID": "publications/stat-thinking-book.html#citation",
    "href": "publications/stat-thinking-book.html#citation",
    "title": "Statistical Reasoning through Computation and R",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon, Zieffler, Andrew S. (2021). Statistical Reasoning through Computation and R. **."
  },
  {
    "objectID": "publications/stat-thinking-book.html#links",
    "href": "publications/stat-thinking-book.html#links",
    "title": "Statistical Reasoning through Computation and R",
    "section": "Links",
    "text": "Links\nOpen Access Link\n\nPublication: Authors: Brandon LeBeau, Andrew S. Zieffler Date: October 07, 2021"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nOct 7, 2021\n\n\nStatistical Reasoning through Computation and R\n\n\nBrandon LeBeau, Andrew S. Zieffler\n\n\n\n\n\n\nApr 25, 2021\n\n\nReproducible Analyses in Educational Research\n\n\nBrandon LeBeau, Scott Ellison, Ariel M. Aloe\n\n\n\n\n\n\nJun 1, 2020\n\n\nCreating a Developmental Scale to Chart the Development of Psychopathology with Different Informants and Measures across Time\n\n\nIsaac T. Petersen, Brandon LeBeau\n\n\n\n\n\n\nJun 1, 2020\n\n\nCreating a developmental scale to account for heterotypic continuity in development: A simulation study\n\n\nIsaac T. Peterson, Brandon LeBeau, Daniel Choe\n\n\n\n\n\n\nJun 1, 2020\n\n\nThe Advanced Placement Program in Rural Schools: Equalizing Opportunity\n\n\nBrandon LeBeau, Susan G. Assouline, Ann Lupkowski-Shoplik, Duhita Mahatmya\n\n\n\n\n\n\nJun 1, 2020\n\n\nEvolution of Statistical Software and Quantitative Methods\n\n\nBrandon LeBeau, Ariel M. Aloe\n\n\n\n\n\n\nJun 1, 2020\n\n\nStudying a Moving Target in Development: The Challenge and Opportunity of Heterotypic Continuity\n\n\nIsaac T. Petersen, Daniel E. Choe, Brandon LeBeau\n\n\n\n\n\n\nJun 1, 2020\n\n\nDesign and Analysis of Monte Carlo Studies: Improving External Validity\n\n\nBrandon LeBeau\n\n\n\n\n\n\nJun 1, 2020\n\n\nDifferentiating Among Gifted Learners: A Comparison of Classical Test Theory and Item Response Theory on Above-Level Testing\n\n\nBrandon LeBeau, Susan G. Assouline, Duhita Mahatmya, Ann Lupkowski-Shoplik\n\n\n\n\n\n\nDec 31, 2019\n\n\nLanguage Ability in the Development of Externalizing Behavior Problems in Childhood\n\n\nIsaac T. Petersen, Brandon LeBeau\n\n\n\n\n\n\nJun 11, 2019\n\n\nPower Analysis by Simulation using R and simglm\n\n\nBrandon LeBeau\n\n\n\n\n\n\nMay 20, 2019\n\n\nReading Comprehension Assessment: The Effect of Reading the Items Aloud Before or After Reading the Passage\n\n\nDeborah K. Reed, Nathan Stevenson, Brandon LeBeau\n\n\n\n\n\n\nMay 14, 2019\n\n\nExamining High School Student Engagement and Critical Factors in Dropout Prevention\n\n\nNathan Stevenson, Jessica Swain-Bradway, Brandon LeBeau\n\n\n\n\n\n\nDec 1, 2018\n\n\nModel misspecification and assumption violations with the linear mixed model: A meta-analysis\n\n\nBrandon LeBeau, Yoon Ah Song, Wei Cheng Liu\n\n\n\n\n\n\nJul 9, 2018\n\n\npdfsearch: Search Tools for PDF Files\n\n\nBrandon LeBeau\n\n\n\n\n\n\nMay 8, 2018\n\n\nMisspecification of the random effect structure Implications for the linear mixed model\n\n\nBrandon LeBeau\n\n\n\n\n\n\nFeb 21, 2018\n\n\nhighlightHTML: CSS Formatting of R Markdown Documents\n\n\nBrandon LeBeau\n\n\n\n\n\n\nNov 20, 2017\n\n\nDevelopment of Internalizing Problems From Adolescence to Emerging Adulthood: Accounting for Heterotypic Continuity With Vertical Scaling\n\n\nIsaac T. Peterson, Oliver Lindhiem, Brandon LeBeau, John E. Bates, Gregory S. Pettit, Jennifer E. Lansford, Kenneth A. Dodge\n\n\n\n\n\n\nOct 9, 2017\n\n\nResearch Synthesis and Meta-Analysis of Monte Carlo Studies: The Best of Two Worlds\n\n\nBrandon LeBeau\n\n\n\n\n\n\nMay 18, 2017\n\n\nAbility and Prior Distribution Mismatch: An Exploration of Common-Item Linking Methods\n\n\nBrandon LeBeau\n\n\n\n\n\n\nMay 1, 2016\n\n\nImpact of Serial Correlation Misspecification with the Linear Mixed Model\n\n\nBrandon LeBeau\n\n\n\n\n\n\nJun 1, 2015\n\n\nThe Relationship between High School Mathematics Curriculum and Mathematics Course-Taking and Achievement for Students Attending Community College\n\n\nDanielle Dupuis, Amanuel Medhanie, Debra Monson, Brandon LeBeau, Michael R. Harwell, Thomas R. Post\n\n\n\n\n\n\nJun 1, 2014\n\n\nA Multisite Study of High School Mathematics Curricula and the Impact of Taking a Developmental Mathematics Course in College\n\n\nMichael R. Harwell, Danielle Dupuis, Thomas R. Post, Amanuel Medhanie, Brandon LeBeau\n\n\n\n\n\n\nJun 1, 2013\n\n\nA Multi-Institutional Study of High School Mathematics Curricula and College Mathematics Achievement and Course Taking\n\n\nMichael R. Harwell, Thomas R. Post, Amanuel Medhanie, Danielle Dupuis, Brandon LeBeau\n\n\n\n\n\n\nMay 1, 2013\n\n\nThe Impact of Institutional Factors on the Relationship Between High School Mathematics Curricula and College Mathematics Course-Taking and Achievement\n\n\nMichael R. Harwell, Danielle Dupuis, Thomas R. Post, Amanuel Medhanie, Brandon LeBeau\n\n\n\n\n\n\nAug 1, 2012\n\n\nOutcomes assessment of case-based writing exercises in a veterinary clinical pathology course\n\n\nLeslie Sharkey, Helen Michael, Brandon LeBeau, Bruce Center, Deb Wingert\n\n\n\n\n\n\nMay 1, 2012\n\n\nA multi-institutional study of the relationship between high school mathematics achievement and performance in introductory college statistics\n\n\nDanielle Dupuis, Amanuel Medhanie, Michael R. Harwell, Brandon LeBeau, Debra Monson, Thomas R. Post\n\n\n\n\n\n\nMar 27, 2012\n\n\nThe role of the accuplacer mathematics placement test on a student’s first college mathematics course\n\n\nAmanuel Medhanie, Danielle Dupuis, Brandon LeBeau, Michael R. Harwell, Thomas R. Post\n\n\n\n\n\n\nMar 19, 2012\n\n\nStudent and High-School Characteristics related to completing a science, technology, engineering or mathematics (STEM) major in college\n\n\nBrandon LeBeau, Michael R. Harwell, Debra Monson, Danielle Dupuis, Amanuel Medhanie, Thomas R. Post\n\n\n\n\n\n\nMar 1, 2010\n\n\nStudent eligibility for a free lunch as an SES measure in education research\n\n\nMichael R Harwell, Brandon LeBeau\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/jep-2019.html",
    "href": "publications/jep-2019.html",
    "title": "Language Ability in the Development of Externalizing Behavior Problems in Childhood",
    "section": "",
    "text": "Poorer language ability has been shown to predict the development of externalizing behavior problems such as aggression and conduct problems. However, the developmental process that links poorer language ability to externalizing problems is unclear. The present study examined (a) whether withinchild changes in language ability predict within-child changes in externalizing problems, (b) whether social skills are a potential mechanism that explains the association between language ability and externalizing problems, and (c) whether there are sex-related differences in the association between language ability and externalizing problems. The present study examined these questions in children (N = 1,364) followed annually from 4 to 10 years of age. Language ability was assessed by a measure of receptive language (i.e., vocabulary). Externalizing problems were rated by mothers and teachers. Social skills were rated by mothers, fathers, and teachers. Findings showed that within-child changes in language ability predicted within-child changes in externalizing problems, even controlling for the family’s income-to-needs ratio. We found that social skills partially mediated the association between poorer language ability and later externalizing problems, but this was limited to a between-person effect. There was not strong evidence of sex-related differences in the association. Findings suggest that language ability may play a role in the development of externalizing problems for boys and girls, and that social skills may be a mechanism that partially explains how poorer language ability leads to the development of externalizing problems. Or, alternatively, language ability, social skills, and externalizing problems may partially share common causes."
  },
  {
    "objectID": "publications/jep-2019.html#abstract",
    "href": "publications/jep-2019.html#abstract",
    "title": "Language Ability in the Development of Externalizing Behavior Problems in Childhood",
    "section": "",
    "text": "Poorer language ability has been shown to predict the development of externalizing behavior problems such as aggression and conduct problems. However, the developmental process that links poorer language ability to externalizing problems is unclear. The present study examined (a) whether withinchild changes in language ability predict within-child changes in externalizing problems, (b) whether social skills are a potential mechanism that explains the association between language ability and externalizing problems, and (c) whether there are sex-related differences in the association between language ability and externalizing problems. The present study examined these questions in children (N = 1,364) followed annually from 4 to 10 years of age. Language ability was assessed by a measure of receptive language (i.e., vocabulary). Externalizing problems were rated by mothers and teachers. Social skills were rated by mothers, fathers, and teachers. Findings showed that within-child changes in language ability predicted within-child changes in externalizing problems, even controlling for the family’s income-to-needs ratio. We found that social skills partially mediated the association between poorer language ability and later externalizing problems, but this was limited to a between-person effect. There was not strong evidence of sex-related differences in the association. Findings suggest that language ability may play a role in the development of externalizing problems for boys and girls, and that social skills may be a mechanism that partially explains how poorer language ability leads to the development of externalizing problems. Or, alternatively, language ability, social skills, and externalizing problems may partially share common causes."
  },
  {
    "objectID": "publications/jep-2019.html#citation",
    "href": "publications/jep-2019.html#citation",
    "title": "Language Ability in the Development of Externalizing Behavior Problems in Childhood",
    "section": "Citation",
    "text": "Citation\nPetersen, Isaac T., LeBeau, Brandon (2019). Language Ability in the Development of Externalizing Behavior Problems in Childhood. Journal of Educational Psychology."
  },
  {
    "objectID": "publications/jep-2019.html#links",
    "href": "publications/jep-2019.html#links",
    "title": "Language Ability in the Development of Externalizing Behavior Problems in Childhood",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Journal of Educational Psychology Authors: Isaac T. Petersen, Brandon LeBeau Date: December 31, 2019"
  },
  {
    "objectID": "publications/aei-19.html",
    "href": "publications/aei-19.html",
    "title": "Examining High School Student Engagement and Critical Factors in Dropout Prevention",
    "section": "",
    "text": "There is a well-known positive relation between student engagement and a host of school outcomes. The following article describes a study that explores a commonly used tool for measuring engagement in high schools. Specifically, this study examined internal validity, factor structure, and the relation between factors of engagement (e.g. social, emotional, and academic) and outcomes of school attendance and suspension rates. Data were collected for 7,718 students in 9th through 12th grade. Engagement was measured using the High School Survey of Student Engagement (HSSSE). Analyses raise concern over the degree to which the structure of HSSSE fits with established constructs of engagement. Implications and directions for future research are discussed."
  },
  {
    "objectID": "publications/aei-19.html#abstract",
    "href": "publications/aei-19.html#abstract",
    "title": "Examining High School Student Engagement and Critical Factors in Dropout Prevention",
    "section": "",
    "text": "There is a well-known positive relation between student engagement and a host of school outcomes. The following article describes a study that explores a commonly used tool for measuring engagement in high schools. Specifically, this study examined internal validity, factor structure, and the relation between factors of engagement (e.g. social, emotional, and academic) and outcomes of school attendance and suspension rates. Data were collected for 7,718 students in 9th through 12th grade. Engagement was measured using the High School Survey of Student Engagement (HSSSE). Analyses raise concern over the degree to which the structure of HSSSE fits with established constructs of engagement. Implications and directions for future research are discussed."
  },
  {
    "objectID": "publications/aei-19.html#citation",
    "href": "publications/aei-19.html#citation",
    "title": "Examining High School Student Engagement and Critical Factors in Dropout Prevention",
    "section": "Citation",
    "text": "Citation\nStevenson, Nathan, Swain-Bradway, Jessica, LeBeau, Brandon (2019). Examining High School Student Engagement and Critical Factors in Dropout Prevention. **Assessment for Effective Intervention, (in press)."
  },
  {
    "objectID": "publications/aei-19.html#links",
    "href": "publications/aei-19.html#links",
    "title": "Examining High School Student Engagement and Critical Factors in Dropout Prevention",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Assessment for Effective Intervention, (in press) Authors: Nathan Stevenson, Jessica Swain-Bradway, Brandon LeBeau Date: May 14, 2019 DOI: 10.1177/1534508419859655"
  },
  {
    "objectID": "publications/joss-highlighthtml.html",
    "href": "publications/joss-highlighthtml.html",
    "title": "highlightHTML: CSS Formatting of R Markdown Documents",
    "section": "",
    "text": "Markdown is a markup language with light formatting options and has become popular within the R community for creating dynamic, reproducible reports. The benefits of this approach to reproducible reports include a simple syntax, ability to focus on content instead of appearance, and the interaction of code with text. However, there are times when formatting is useful to highlight aspects of a table or text more generally within a report. The highlightHTML package (LeBeau 2017) is an R package (R Core Team 2016) that extends the basic formatting of markdown documents using Cascading Style Sheets (CSS) and HTML ids. As CSS is used to do the formatting, the limits in the styles that can be implemented are up to the amount of CSS knowledge the author has. The hightlightHTML package fits nicely into the workflow of a reproducible research report as the package can dynamically insert the ids into tables with R code. Lastly, compilation from rmarkdown or markdown to HTML can be done directly from the package which makes use of the render function from the rmarkdown package (Allaire et al. 2016). The package vignette includes more information on this package, including simple example files to show the features in more detail. Below is example output of one of these examples. Link to release can be found here: https://doi.org/10.5281/zenodo.265701"
  },
  {
    "objectID": "publications/joss-highlighthtml.html#abstract",
    "href": "publications/joss-highlighthtml.html#abstract",
    "title": "highlightHTML: CSS Formatting of R Markdown Documents",
    "section": "",
    "text": "Markdown is a markup language with light formatting options and has become popular within the R community for creating dynamic, reproducible reports. The benefits of this approach to reproducible reports include a simple syntax, ability to focus on content instead of appearance, and the interaction of code with text. However, there are times when formatting is useful to highlight aspects of a table or text more generally within a report. The highlightHTML package (LeBeau 2017) is an R package (R Core Team 2016) that extends the basic formatting of markdown documents using Cascading Style Sheets (CSS) and HTML ids. As CSS is used to do the formatting, the limits in the styles that can be implemented are up to the amount of CSS knowledge the author has. The hightlightHTML package fits nicely into the workflow of a reproducible research report as the package can dynamically insert the ids into tables with R code. Lastly, compilation from rmarkdown or markdown to HTML can be done directly from the package which makes use of the render function from the rmarkdown package (Allaire et al. 2016). The package vignette includes more information on this package, including simple example files to show the features in more detail. Below is example output of one of these examples. Link to release can be found here: https://doi.org/10.5281/zenodo.265701"
  },
  {
    "objectID": "publications/joss-highlighthtml.html#citation",
    "href": "publications/joss-highlighthtml.html#citation",
    "title": "highlightHTML: CSS Formatting of R Markdown Documents",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2018). highlightHTML: CSS Formatting of R Markdown Documents. **Journal of Open Source Software, 3 (21)."
  },
  {
    "objectID": "publications/joss-highlighthtml.html#links",
    "href": "publications/joss-highlighthtml.html#links",
    "title": "highlightHTML: CSS Formatting of R Markdown Documents",
    "section": "Links",
    "text": "Links\nLink to JOSS PDF Code\n\nPublication: Journal of Open Source Software, 3 (21) Authors: Brandon LeBeau Date: February 21, 2018 DOI: 10.21105/joss.00185"
  },
  {
    "objectID": "publications/miss_re.html",
    "href": "publications/miss_re.html",
    "title": "Misspecification of the random effect structure Implications for the linear mixed model",
    "section": "",
    "text": "The linear mixed model is a commonly used model for longitudinal or nested data due to its ability to account for the dependency of nested data. Researchers typically rely on the random effects to adequately account for the dependency due to correlated data, however serial correlation can also be used. If the random effect structure is misspecified (perhaps due to convergence problems), can the addition of serial correlation overcome this misspecification and allow for unbiased estimation and accurate inferences? This study explored this question with a simulation. Simulation results show that the fixed effects are unbiased, however inflation of the empirical type I error rate occurs when a random effect is missing from the model. Implications for applied researchers are discussed."
  },
  {
    "objectID": "publications/miss_re.html#abstract",
    "href": "publications/miss_re.html#abstract",
    "title": "Misspecification of the random effect structure Implications for the linear mixed model",
    "section": "",
    "text": "The linear mixed model is a commonly used model for longitudinal or nested data due to its ability to account for the dependency of nested data. Researchers typically rely on the random effects to adequately account for the dependency due to correlated data, however serial correlation can also be used. If the random effect structure is misspecified (perhaps due to convergence problems), can the addition of serial correlation overcome this misspecification and allow for unbiased estimation and accurate inferences? This study explored this question with a simulation. Simulation results show that the fixed effects are unbiased, however inflation of the empirical type I error rate occurs when a random effect is missing from the model. Implications for applied researchers are discussed."
  },
  {
    "objectID": "publications/miss_re.html#citation",
    "href": "publications/miss_re.html#citation",
    "title": "Misspecification of the random effect structure Implications for the linear mixed model",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon (2018). Misspecification of the random effect structure Implications for the linear mixed model. **."
  },
  {
    "objectID": "publications/miss_re.html#links",
    "href": "publications/miss_re.html#links",
    "title": "Misspecification of the random effect structure Implications for the linear mixed model",
    "section": "Links",
    "text": "Links\nPDF\n\nPublication: Authors: Brandon LeBeau Date: May 08, 2018"
  },
  {
    "objectID": "publications/vet-ed.html",
    "href": "publications/vet-ed.html",
    "title": "Outcomes assessment of case-based writing exercises in a veterinary clinical pathology course",
    "section": "",
    "text": "Our second-year core clinical pathology course uses free-response case-based learning exercises in an otherwise traditional lecture or laboratory course format to augment the development of skills in application of knowledge and critical thinking and clinical reasoning. We previously reported increased learner confidence accompanied by perceived improvements in understanding and ability to apply information, along with enhanced feelings of preparedness for examinations that students attributed to the case-based exercises. The current study prospectively follows a cohort of students to determine the ability of traditional multiple-choice versus free-response case-based assessments to predict future academic performance and to determine if the perceived value of the case-based exercises persists through the curriculum. Our data show that after holding multiple-choice scores constant, better performance on case-based free-response exercises led to higher GPA and better class rank in the second and third years and better class rank in the fourth year. Students in clinical rotations reported that the case-based approach was superior to traditional lecture or multiple-choice exam format for learning clinical reasoning, retaining factual information, organizing information, communicating medical information clearly to colleagues in clinical situations, and preparing high quality medical records. In summary, this longitudinal study shows that case-based free-response writing assignments are efficacious above and beyond standard measures in determining students’ GPAs and class rank and in students’ acquisition of knowledge, skills, and clinical reasoning. Students value these assignments and overwhelmingly find them an efficient use of their time, and these opinions are maintained even two years following the course."
  },
  {
    "objectID": "publications/vet-ed.html#abstract",
    "href": "publications/vet-ed.html#abstract",
    "title": "Outcomes assessment of case-based writing exercises in a veterinary clinical pathology course",
    "section": "",
    "text": "Our second-year core clinical pathology course uses free-response case-based learning exercises in an otherwise traditional lecture or laboratory course format to augment the development of skills in application of knowledge and critical thinking and clinical reasoning. We previously reported increased learner confidence accompanied by perceived improvements in understanding and ability to apply information, along with enhanced feelings of preparedness for examinations that students attributed to the case-based exercises. The current study prospectively follows a cohort of students to determine the ability of traditional multiple-choice versus free-response case-based assessments to predict future academic performance and to determine if the perceived value of the case-based exercises persists through the curriculum. Our data show that after holding multiple-choice scores constant, better performance on case-based free-response exercises led to higher GPA and better class rank in the second and third years and better class rank in the fourth year. Students in clinical rotations reported that the case-based approach was superior to traditional lecture or multiple-choice exam format for learning clinical reasoning, retaining factual information, organizing information, communicating medical information clearly to colleagues in clinical situations, and preparing high quality medical records. In summary, this longitudinal study shows that case-based free-response writing assignments are efficacious above and beyond standard measures in determining students’ GPAs and class rank and in students’ acquisition of knowledge, skills, and clinical reasoning. Students value these assignments and overwhelmingly find them an efficient use of their time, and these opinions are maintained even two years following the course."
  },
  {
    "objectID": "publications/vet-ed.html#citation",
    "href": "publications/vet-ed.html#citation",
    "title": "Outcomes assessment of case-based writing exercises in a veterinary clinical pathology course",
    "section": "Citation",
    "text": "Citation\nSharkey, Leslie, Michael, Helen, LeBeau, Brandon, Center, Bruce, Wingert, Deb (2012). Outcomes assessment of case-based writing exercises in a veterinary clinical pathology course. **Journal of Veterinary Medical Education, 39.4, 396 - 403."
  },
  {
    "objectID": "publications/vet-ed.html#links",
    "href": "publications/vet-ed.html#links",
    "title": "Outcomes assessment of case-based writing exercises in a veterinary clinical pathology course",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Journal of Veterinary Medical Education, 39.4, 396 - 403 Authors: Leslie Sharkey, Helen Michael, Brandon LeBeau, Bruce Center, Deb Wingert Date: August 01, 2012 DOI: 10.3138/jvme.0312.020R1"
  },
  {
    "objectID": "publications/rre-reproducible-2021.html",
    "href": "publications/rre-reproducible-2021.html",
    "title": "Reproducible Analyses in Educational Research",
    "section": "",
    "text": "A reproducible analysis is one in which an independent analyst, using the same data and the same statistical code, would obtain the exact same result as the previous analyst. Reproducible analyses utilize script based analyses and open data to aid in the reproduction of the analysis. A reproducible analysis does not ensure the same results are obtained if another sample of data are obtained, often referred to replicability. Reproduction and replication of studies are discussed as well as the overwhelming benefits of creating a reproducible analysis workflow. A tool is proposed to aid in the evaluation of studies to help describe which elements a study has a strong reproducible workflow and areas that could be improved. This tool is meant to serve as a discussion tool, not to rank studies or devalue studies that are unable to share data or statistical code. Finally, discussion surrounding reproducibility for qualitative studies are discussed along with unique challenges for adopting a reproducible analysis framework."
  },
  {
    "objectID": "publications/rre-reproducible-2021.html#abstract",
    "href": "publications/rre-reproducible-2021.html#abstract",
    "title": "Reproducible Analyses in Educational Research",
    "section": "",
    "text": "A reproducible analysis is one in which an independent analyst, using the same data and the same statistical code, would obtain the exact same result as the previous analyst. Reproducible analyses utilize script based analyses and open data to aid in the reproduction of the analysis. A reproducible analysis does not ensure the same results are obtained if another sample of data are obtained, often referred to replicability. Reproduction and replication of studies are discussed as well as the overwhelming benefits of creating a reproducible analysis workflow. A tool is proposed to aid in the evaluation of studies to help describe which elements a study has a strong reproducible workflow and areas that could be improved. This tool is meant to serve as a discussion tool, not to rank studies or devalue studies that are unable to share data or statistical code. Finally, discussion surrounding reproducibility for qualitative studies are discussed along with unique challenges for adopting a reproducible analysis framework."
  },
  {
    "objectID": "publications/rre-reproducible-2021.html#citation",
    "href": "publications/rre-reproducible-2021.html#citation",
    "title": "Reproducible Analyses in Educational Research",
    "section": "Citation",
    "text": "Citation\nLeBeau, Brandon, Ellison, Scott, Aloe, Ariel M. (2021). Reproducible Analyses in Educational Research. Review of Reseach in Education."
  },
  {
    "objectID": "publications/rre-reproducible-2021.html#links",
    "href": "publications/rre-reproducible-2021.html#links",
    "title": "Reproducible Analyses in Educational Research",
    "section": "Links",
    "text": "Links\nLink to Journal\n\nPublication: Review of Reseach in Education Authors: Brandon LeBeau, Scott Ellison, Ariel M. Aloe Date: April 25, 2021"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Ph.D. Quantitative Methods in Education\nUniversity of Minnesota, 2013"
  },
  {
    "objectID": "about/index.html#education",
    "href": "about/index.html#education",
    "title": "About",
    "section": "",
    "text": "Ph.D. Quantitative Methods in Education\nUniversity of Minnesota, 2013"
  },
  {
    "objectID": "about/index.html#contact-information",
    "href": "about/index.html#contact-information",
    "title": "About",
    "section": "Contact Information",
    "text": "Contact Information\n   brandon dot c dot lebeau@gmail.com\n   LinkedIn\n   GitHub\n   Publications"
  },
  {
    "objectID": "presentations/2023-03-30-ncme.html#adding-attributes",
    "href": "presentations/2023-03-30-ncme.html#adding-attributes",
    "title": "Linking with the Bayesian Item Response Theory Model",
    "section": "Adding Attributes",
    "text": "Adding Attributes\nAttributes can be added as item, person, or both item and person attributes.\nExample:\n\\[\nP(y = 1) = logistic(\\alpha_{i}(b_{p} + \\gamma_{i} + X_{jpi} \\beta_{j}))\n\\]\n\n\\(X_{jpi}\\) is a design matrix.\n\\(\\beta_{j}\\) are a set of \\(J\\) regression coefficients."
  },
  {
    "objectID": "presentations/2023-03-30-ncme.html#specific-example---3-time-points",
    "href": "presentations/2023-03-30-ncme.html#specific-example---3-time-points",
    "title": "Linking with the Bayesian Item Response Theory Model",
    "section": "Specific Example - 3 time points",
    "text": "Specific Example - 3 time points\nExample\n\\[\nP(y = 1) = logistic(\\alpha_{i}(b_{p} + \\gamma_{i} + \\beta_{1} time_{2} + \\beta_{2} time_{3}))\n\\]\n\n\\(time_{2}\\) is a dummy attribute, 1 = time 2, 0 = otherwise\n\n\\(time_{3}\\) is a dummy attribute, 1 = time 3, 0 = otherwise"
  }
]